{
  "id": "perceptron",
  "name": "Single Neuron / Perceptron",
  "category": "deep-learning",
  "description": {
    "short": "Watch a single neuron compute: inputs × weights + bias → activation → output.",
    "long": "The perceptron is the fundamental building block of neural networks. A single neuron takes multiple inputs, multiplies each by a learned weight, sums the results, adds a bias term, and passes the total through an activation function. This visualization walks through each computation step, showing how inputs are transformed into an output, and how changing weights and bias affects the decision."
  },
  "complexity": {
    "time": "O(n)",
    "space": "O(1)",
    "level": "beginner"
  },
  "visual": {
    "layout": "neuron-diagram",
    "theme": { "primary": "#8b5cf6", "secondary": "#c084fc" },
    "components": {}
  },
  "inputs": {
    "schema": {
      "inputs": { "type": "array", "items": { "type": "number" }, "description": "Input values (x₀, x₁, ...)", "minItems": 1, "maxItems": 5 },
      "weights": { "type": "array", "items": { "type": "number" }, "description": "Weight values (w₀, w₁, ...)" },
      "bias": { "type": "number", "description": "Bias term" },
      "activationFunction": { "type": "string", "enum": ["sigmoid", "relu", "tanh", "step"], "description": "Activation function" }
    },
    "defaults": {
      "inputs": [0.5, 0.8],
      "weights": [0.6, -0.3],
      "bias": 0.1,
      "activationFunction": "sigmoid"
    },
    "examples": [
      {
        "name": "AND gate (step function)",
        "values": { "inputs": [1, 1], "weights": [0.5, 0.5], "bias": -0.7, "activationFunction": "step" }
      },
      {
        "name": "OR gate (step function)",
        "values": { "inputs": [1, 0], "weights": [0.5, 0.5], "bias": -0.2, "activationFunction": "step" }
      },
      {
        "name": "Three inputs (sigmoid)",
        "values": { "inputs": [0.3, 0.7, 0.5], "weights": [0.4, -0.2, 0.8], "bias": -0.1, "activationFunction": "sigmoid" }
      }
    ]
  },
  "code": {
    "implementations": {
      "pseudocode": "function perceptron(inputs, weights, bias, activation_fn):\n  z = 0\n  for i in range(len(inputs)):\n    z += inputs[i] * weights[i]    // weighted sum\n  z += bias                         // add bias\n  output = activation_fn(z)          // apply activation\n  return output",
      "python": "import numpy as np\n\ndef perceptron(x, w, b, activation='sigmoid'):\n    z = np.dot(x, w) + b\n    if activation == 'sigmoid':\n        return 1 / (1 + np.exp(-z))\n    elif activation == 'relu':\n        return max(0, z)\n    elif activation == 'step':\n        return 1 if z >= 0 else 0",
      "javascript": "function perceptron(inputs, weights, bias, activation) {\n  const z = inputs.reduce((sum, x, i) => sum + x * weights[i], 0) + bias;\n  if (activation === 'sigmoid') return 1 / (1 + Math.exp(-z));\n  if (activation === 'relu') return Math.max(0, z);\n  if (activation === 'step') return z >= 0 ? 1 : 0;\n}"
    },
    "defaultLanguage": "pseudocode"
  },
  "education": {
    "keyConcepts": [
      { "title": "Weighted Sum", "description": "Each input is multiplied by its weight. The weight controls how much influence that input has." },
      { "title": "Bias Term", "description": "The bias shifts the activation threshold. It allows the neuron to fire even when all inputs are zero." },
      { "title": "Activation Function", "description": "Without activation, a neuron is just a linear function. Activation functions introduce non-linearity for learning complex patterns." },
      { "title": "The Perceptron's Limitation", "description": "A single perceptron can only learn linearly separable patterns. It can learn AND and OR but NOT XOR." }
    ],
    "pitfalls": [
      { "title": "Weight Initialization Matters", "description": "If all weights start at zero, all neurons compute the same thing." },
      { "title": "Sigmoid Saturation", "description": "When |z| is large, sigmoid derivative is nearly 0 — the vanishing gradient problem." }
    ],
    "quiz": [
      {
        "question": "A neuron has inputs [1, 0], weights [0.5, 0.5], and bias -0.7. Using a step activation function, what is the output?",
        "options": ["0", "1", "0.5", "-0.2"],
        "correctIndex": 0,
        "explanation": "z = 1×0.5 + 0×0.5 + (-0.7) = -0.2. Since z < 0, step(-0.2) = 0."
      },
      {
        "question": "Why can't a single perceptron learn XOR?",
        "options": ["XOR requires more than 2 inputs", "XOR is not a linear function — no single line can separate the outputs", "The step function can't handle XOR", "XOR requires negative weights"],
        "correctIndex": 1,
        "explanation": "XOR outputs 1 for (0,1) and (1,0) but 0 for (0,0) and (1,1). No single straight line can separate these two classes."
      }
    ],
    "resources": [
      { "title": "Rosenblatt 1958 — The Perceptron", "url": "https://psycnet.apa.org/doi/10.1037/h0042519", "type": "paper" },
      { "title": "3Blue1Brown — But what is a neural network?", "url": "https://www.youtube.com/watch?v=aircAruvnKk", "type": "video" }
    ]
  },
  "seo": {
    "keywords": ["perceptron", "single neuron", "neural network basics", "activation function", "weighted sum"],
    "ogDescription": "Interactive visualization of a single neuron (perceptron). Watch inputs get multiplied by weights, summed with bias, and transformed by an activation function."
  },
  "prerequisites": [],
  "related": ["feedforward-network", "backpropagation"],
  "author": "eigenvue",
  "version": "1.0.0"
}

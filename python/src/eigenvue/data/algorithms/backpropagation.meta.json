{
  "id": "backpropagation",
  "name": "Backpropagation",
  "category": "deep-learning",
  "description": {
    "short": "How neural networks learn: computing gradients through the chain rule.",
    "long": "Backpropagation (backward propagation of errors) is the algorithm that makes neural network training possible. After a forward pass computes the network's prediction, backpropagation computes the gradient of the loss function with respect to every weight in the network by applying the chain rule of calculus layer by layer, from output back to input. These gradients tell each weight how to adjust to reduce the loss. Combined with an optimizer like gradient descent, backpropagation enables the network to learn from data. Introduced by Rumelhart, Hinton, and Williams in 1986, backpropagation remains the foundation of all deep learning training."
  },
  "complexity": {
    "time": "O(Σ n_l × n_{l+1})",
    "space": "O(Σ n_l × n_{l+1})",
    "level": "intermediate"
  },
  "visual": {
    "layout": "layer-network",
    "theme": {
      "primary": "#ef4444",
      "secondary": "#dc2626"
    },
    "components": {
      "showWeights": true,
      "showGradients": true,
      "showActivations": true,
      "showBiases": true,
      "gradientColor": "#ef4444",
      "forwardColor": "#22c55e",
      "backwardColor": "#ef4444"
    }
  },
  "inputs": {
    "schema": {
      "layerSizes": {
        "type": "array",
        "items": {
          "type": "number",
          "minimum": 1,
          "maximum": 4
        },
        "minItems": 2,
        "maxItems": 4,
        "description": "Number of neurons in each layer."
      },
      "inputValues": {
        "type": "array",
        "items": {
          "type": "number",
          "minimum": -2,
          "maximum": 2
        },
        "description": "Input values for the forward pass."
      },
      "targetValues": {
        "type": "array",
        "items": {
          "type": "number",
          "minimum": 0,
          "maximum": 1
        },
        "description": "Target output values for computing the loss."
      },
      "learningRate": {
        "type": "number",
        "minimum": 0.001,
        "maximum": 1,
        "description": "Learning rate for weight updates."
      }
    },
    "defaults": {
      "layerSizes": [
        2,
        2,
        1
      ],
      "inputValues": [
        0.5,
        0.8
      ],
      "targetValues": [
        1
      ],
      "learningRate": 0.1
    },
    "examples": [
      {
        "name": "Default (2-2-1 network)",
        "values": {
          "layerSizes": [
            2,
            2,
            1
          ],
          "inputValues": [
            0.5,
            0.8
          ],
          "targetValues": [
            1
          ],
          "learningRate": 0.1
        }
      },
      {
        "name": "Higher learning rate",
        "values": {
          "layerSizes": [
            2,
            2,
            1
          ],
          "inputValues": [
            0.5,
            0.8
          ],
          "targetValues": [
            1
          ],
          "learningRate": 0.5
        }
      }
    ]
  },
  "code": {
    "implementations": {
      "pseudocode": "function backpropagation(network, input, target, learningRate):\n  // Forward pass\n  activations = forwardPass(network, input)\n  loss = computeLoss(activations[-1], target)\n\n  // Backward pass — compute gradients\n  delta = lossDeriv(activations[-1], target)\n  for layer in reversed(range(num_layers - 1)):\n    weightGrad = delta × activations[layer].T\n    biasGrad = delta\n    delta = (weights[layer].T × delta) * activDeriv(z[layer])\n\n    // Update weights\n    weights[layer] -= learningRate * weightGrad\n    biases[layer] -= learningRate * biasGrad"
    },
    "defaultLanguage": "pseudocode"
  },
  "education": {
    "keyConcepts": [
      {
        "title": "Chain Rule of Calculus",
        "description": "Backpropagation is an efficient application of the chain rule. To find how a weight in an early layer affects the loss, we multiply the local gradients along the path from the loss back to that weight: ∂Loss/∂w = ∂Loss/∂a × ∂a/∂z × ∂z/∂w."
      },
      {
        "title": "Gradient Descent",
        "description": "Gradients indicate the direction of steepest increase. To minimize the loss, we move weights in the opposite direction of the gradient: w_new = w_old - η × ∂Loss/∂w, where η is the learning rate."
      },
      {
        "title": "Forward Then Backward",
        "description": "Training requires two passes: a forward pass to compute predictions and loss, then a backward pass to compute gradients. Both passes have the same computational cost — O(number of weights)."
      }
    ],
    "pitfalls": [
      {
        "title": "Learning rate too large",
        "description": "A learning rate that is too large causes weight updates to overshoot the minimum, potentially diverging. Too small means painfully slow convergence."
      },
      {
        "title": "Vanishing gradients in deep networks",
        "description": "When using sigmoid or tanh activations, gradients shrink exponentially as they propagate backward through many layers, making early layers almost impossible to train."
      }
    ],
    "quiz": [
      {
        "question": "What mathematical tool does backpropagation use to compute gradients?",
        "options": [
          "Integration by parts",
          "The chain rule of calculus",
          "Taylor series expansion",
          "Fourier transform"
        ],
        "correctIndex": 1,
        "explanation": "Backpropagation applies the chain rule to compute the gradient of the loss with respect to each weight by multiplying local gradients along the computation path."
      }
    ],
    "resources": [
      {
        "title": "Learning Representations by Back-Propagating Errors (Rumelhart et al., 1986)",
        "url": "https://www.nature.com/articles/323533a0",
        "type": "paper"
      },
      {
        "title": "Backpropagation — 3Blue1Brown",
        "url": "https://www.3blue1brown.com/lessons/backpropagation",
        "type": "video"
      }
    ]
  },
  "seo": {
    "keywords": [
      "backpropagation visualization",
      "neural network training step by step",
      "gradient computation animation",
      "chain rule neural networks",
      "how backpropagation works"
    ],
    "ogDescription": "Interactive visualization of backpropagation. Watch gradients flow backward through a neural network as weights update to minimize loss."
  },
  "prerequisites": [
    "feedforward-network"
  ],
  "related": [
    "feedforward-network",
    "gradient-descent"
  ],
  "author": "eigenvue",
  "version": "1.0.0"
}

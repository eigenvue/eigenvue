[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.17.2","content-config-digest","7cec5e9af53249ca","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://eigenvue.web.app\",\"compressHTML\":true,\"base\":\"/docs\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"where\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":false,\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null,null,null],\"rehypePlugins\":[[null,{\"experimentalHeadingIdCompat\":false}],null,[null,{\"themes\":[{\"name\":\"Night Owl No Italics\",\"type\":\"dark\",\"colors\":{\"focusBorder\":\"#122d42\",\"foreground\":\"#d6deeb\",\"disabledForeground\":\"#cccccc80\",\"descriptionForeground\":\"#d6deebb3\",\"errorForeground\":\"#ef5350\",\"icon.foreground\":\"#c5c5c5\",\"contrastActiveBorder\":null,\"contrastBorder\":\"#122d42\",\"textBlockQuote.background\":\"#7f7f7f1a\",\"textBlockQuote.border\":\"#007acc80\",\"textCodeBlock.background\":\"#4f4f4f\",\"textLink.activeForeground\":\"#3794ff\",\"textLink.foreground\":\"#3794ff\",\"textPreformat.foreground\":\"#d7ba7d\",\"textSeparator.foreground\":\"#ffffff2e\",\"editor.background\":\"#23262f\",\"editor.foreground\":\"#d6deeb\",\"editorLineNumber.foreground\":\"#4b6479\",\"editorLineNumber.activeForeground\":\"#c5e4fd\",\"editorActiveLineNumber.foreground\":\"#c6c6c6\",\"editor.selectionBackground\":\"#1d3b53\",\"editor.inactiveSelectionBackground\":\"#7e57c25a\",\"editor.selectionHighlightBackground\":\"#5f7e9779\",\"editorError.foreground\":\"#ef5350\",\"editorWarning.foreground\":\"#b39554\",\"editorInfo.foreground\":\"#3794ff\",\"editorHint.foreground\":\"#eeeeeeb2\",\"problemsErrorIcon.foreground\":\"#ef5350\",\"problemsWarningIcon.foreground\":\"#b39554\",\"problemsInfoIcon.foreground\":\"#3794ff\",\"editor.findMatchBackground\":\"#5f7e9779\",\"editor.findMatchHighlightBackground\":\"#1085bb5d\",\"editor.findRangeHighlightBackground\":\"#3a3d4166\",\"editorLink.activeForeground\":\"#4e94ce\",\"editorLightBulb.foreground\":\"#ffcc00\",\"editorLightBulbAutoFix.foreground\":\"#75beff\",\"diffEditor.insertedTextBackground\":\"#99b76d23\",\"diffEditor.insertedTextBorder\":\"#c5e47833\",\"diffEditor.removedTextBackground\":\"#ef535033\",\"diffEditor.removedTextBorder\":\"#ef53504d\",\"diffEditor.insertedLineBackground\":\"#9bb95533\",\"diffEditor.removedLineBackground\":\"#ff000033\",\"editorStickyScroll.background\":\"#011627\",\"editorStickyScrollHover.background\":\"#2a2d2e\",\"editorInlayHint.background\":\"#5f7e97cc\",\"editorInlayHint.foreground\":\"#ffffff\",\"editorInlayHint.typeBackground\":\"#5f7e97cc\",\"editorInlayHint.typeForeground\":\"#ffffff\",\"editorInlayHint.parameterBackground\":\"#5f7e97cc\",\"editorInlayHint.parameterForeground\":\"#ffffff\",\"editorPane.background\":\"#011627\",\"editorGroup.emptyBackground\":\"#011627\",\"editorGroup.focusedEmptyBorder\":null,\"editorGroupHeader.tabsBackground\":\"var(--sl-color-black)\",\"editorGroupHeader.tabsBorder\":\"color-mix(in srgb, var(--sl-color-gray-5), transparent 25%)\",\"editorGroupHeader.noTabsBackground\":\"#011627\",\"editorGroupHeader.border\":null,\"editorGroup.border\":\"#011627\",\"editorGroup.dropBackground\":\"#7e57c273\",\"editorGroup.dropIntoPromptForeground\":\"#d6deeb\",\"editorGroup.dropIntoPromptBackground\":\"#021320\",\"editorGroup.dropIntoPromptBorder\":null,\"sideBySideEditor.horizontalBorder\":\"#011627\",\"sideBySideEditor.verticalBorder\":\"#011627\",\"scrollbar.shadow\":\"#010b14\",\"scrollbarSlider.background\":\"#ffffff17\",\"scrollbarSlider.hoverBackground\":\"#ffffff40\",\"scrollbarSlider.activeBackground\":\"#084d8180\",\"panel.background\":\"#011627\",\"panel.border\":\"#5f7e97\",\"panelTitle.activeBorder\":\"#5f7e97\",\"panelTitle.activeForeground\":\"#ffffffcc\",\"panelTitle.inactiveForeground\":\"#d6deeb80\",\"panelSectionHeader.background\":\"#80808051\",\"terminal.background\":\"#011627\",\"widget.shadow\":\"#011627\",\"editorWidget.background\":\"#021320\",\"editorWidget.foreground\":\"#d6deeb\",\"editorWidget.border\":\"#5f7e97\",\"quickInput.background\":\"#021320\",\"quickInput.foreground\":\"#d6deeb\",\"quickInputTitle.background\":\"#ffffff1a\",\"pickerGroup.foreground\":\"#d1aaff\",\"pickerGroup.border\":\"#011627\",\"editor.hoverHighlightBackground\":\"#7e57c25a\",\"editorHoverWidget.background\":\"#011627\",\"editorHoverWidget.foreground\":\"#d6deeb\",\"editorHoverWidget.border\":\"#5f7e97\",\"editorHoverWidget.statusBarBackground\":\"#011a2f\",\"titleBar.activeBackground\":\"var(--sl-color-black)\",\"titleBar.activeForeground\":\"var(--sl-color-text)\",\"titleBar.inactiveBackground\":\"#010e1a\",\"titleBar.inactiveForeground\":\"#eeefff99\",\"titleBar.border\":\"color-mix(in srgb, var(--sl-color-gray-5), transparent 25%)\",\"toolbar.hoverBackground\":\"#5a5d5e50\",\"toolbar.activeBackground\":\"#63666750\",\"tab.activeBackground\":\"#0b2942\",\"tab.unfocusedActiveBackground\":\"#0b2942\",\"tab.inactiveBackground\":\"#01111d\",\"tab.unfocusedInactiveBackground\":\"#01111d\",\"tab.activeForeground\":\"var(--sl-color-text)\",\"tab.inactiveForeground\":\"#5f7e97\",\"tab.unfocusedActiveForeground\":\"#5f7e97\",\"tab.unfocusedInactiveForeground\":\"#5f7e97\",\"tab.hoverBackground\":null,\"tab.unfocusedHoverBackground\":null,\"tab.hoverForeground\":null,\"tab.unfocusedHoverForeground\":null,\"tab.border\":\"#272b3b\",\"tab.lastPinnedBorder\":\"#585858\",\"tab.activeBorder\":\"transparent\",\"tab.unfocusedActiveBorder\":\"#262a39\",\"tab.activeBorderTop\":\"var(--sl-color-accent-high)\",\"tab.unfocusedActiveBorderTop\":null,\"tab.hoverBorder\":null,\"tab.unfocusedHoverBorder\":null,\"tab.activeModifiedBorder\":\"#3399cc\",\"tab.inactiveModifiedBorder\":\"#3399cc80\",\"tab.unfocusedActiveModifiedBorder\":\"#3399cc80\",\"tab.unfocusedInactiveModifiedBorder\":\"#3399cc40\",\"badge.background\":\"#5f7e97\",\"badge.foreground\":\"#ffffff\",\"button.background\":\"#7e57c2cc\",\"button.foreground\":\"#ffffffcc\",\"button.border\":\"#122d42\",\"button.separator\":\"#ffffff52\",\"button.hoverBackground\":\"#7e57c2\",\"button.secondaryBackground\":\"#3a3d41\",\"button.secondaryForeground\":\"#ffffff\",\"button.secondaryHoverBackground\":\"#46494e\",\"dropdown.background\":\"#011627\",\"dropdown.foreground\":\"#ffffffcc\",\"dropdown.border\":\"#5f7e97\",\"list.activeSelectionBackground\":\"#234d708c\",\"list.activeSelectionForeground\":\"#ffffff\",\"tree.indentGuidesStroke\":\"#585858\",\"input.background\":\"#0b253a\",\"input.foreground\":\"#ffffffcc\",\"input.placeholderForeground\":\"#5f7e97\",\"inputOption.activeBorder\":\"#ffffffcc\",\"inputOption.hoverBackground\":\"#5a5d5e80\",\"inputOption.activeBackground\":\"#122d4266\",\"inputOption.activeForeground\":\"#ffffff\",\"inputValidation.infoBackground\":\"#00589ef2\",\"inputValidation.infoBorder\":\"#64b5f6\",\"inputValidation.warningBackground\":\"#675700f2\",\"inputValidation.warningBorder\":\"#ffca28\",\"inputValidation.errorBackground\":\"#ab0300f2\",\"inputValidation.errorBorder\":\"#ef5350\",\"keybindingLabel.background\":\"#8080802b\",\"keybindingLabel.foreground\":\"#cccccc\",\"keybindingLabel.border\":\"#33333399\",\"keybindingLabel.bottomBorder\":\"#44444499\",\"menu.foreground\":\"#ffffffcc\",\"menu.background\":\"#011627\",\"menu.selectionForeground\":\"#ffffff\",\"menu.selectionBackground\":\"#234d708c\",\"menu.separatorBackground\":\"#606060\",\"editor.snippetTabstopHighlightBackground\":\"#7c7c74c\",\"editor.snippetFinalTabstopHighlightBorder\":\"#525252\",\"terminal.ansiBlack\":\"#011627\",\"terminal.ansiRed\":\"#ef5350\",\"terminal.ansiGreen\":\"#22da6e\",\"terminal.ansiYellow\":\"#c5e478\",\"terminal.ansiBlue\":\"#82aaff\",\"terminal.ansiMagenta\":\"#c792ea\",\"terminal.ansiCyan\":\"#21c7a8\",\"terminal.ansiWhite\":\"#ffffff\",\"terminal.ansiBrightBlack\":\"#575656\",\"terminal.ansiBrightRed\":\"#ef5350\",\"terminal.ansiBrightGreen\":\"#22da6e\",\"terminal.ansiBrightYellow\":\"#ffeb95\",\"terminal.ansiBrightBlue\":\"#82aaff\",\"terminal.ansiBrightMagenta\":\"#c792ea\",\"terminal.ansiBrightCyan\":\"#7fdbca\",\"terminal.ansiBrightWhite\":\"#ffffff\",\"selection.background\":\"#4373c2\",\"input.border\":\"#5f7e97\",\"punctuation.definition.generic.begin.html\":\"#ef5350f2\",\"progress.background\":\"#7e57c2\",\"breadcrumb.foreground\":\"#a599e9\",\"breadcrumb.focusForeground\":\"#ffffff\",\"breadcrumb.activeSelectionForeground\":\"#ffffff\",\"breadcrumbPicker.background\":\"#001122\",\"list.invalidItemForeground\":\"#975f94\",\"list.dropBackground\":\"#011627\",\"list.focusBackground\":\"#010d18\",\"list.focusForeground\":\"#ffffff\",\"list.highlightForeground\":\"#ffffff\",\"list.hoverBackground\":\"#011627\",\"list.hoverForeground\":\"#ffffff\",\"list.inactiveSelectionBackground\":\"#0e293f\",\"list.inactiveSelectionForeground\":\"#5f7e97\",\"activityBar.background\":\"#011627\",\"activityBar.dropBackground\":\"#5f7e97\",\"activityBar.foreground\":\"#5f7e97\",\"activityBar.border\":\"#011627\",\"activityBarBadge.background\":\"#44596b\",\"activityBarBadge.foreground\":\"#ffffff\",\"sideBar.background\":\"#011627\",\"sideBar.foreground\":\"#89a4bb\",\"sideBar.border\":\"#011627\",\"sideBarTitle.foreground\":\"#5f7e97\",\"sideBarSectionHeader.background\":\"#011627\",\"sideBarSectionHeader.foreground\":\"#5f7e97\",\"editorCursor.foreground\":\"#80a4c2\",\"editor.wordHighlightBackground\":\"#f6bbe533\",\"editor.wordHighlightStrongBackground\":\"#e2a2f433\",\"editor.lineHighlightBackground\":\"#0003\",\"editor.rangeHighlightBackground\":\"#7e57c25a\",\"editorIndentGuide.background\":\"#5e81ce52\",\"editorIndentGuide.activeBackground\":\"#7e97ac\",\"editorRuler.foreground\":\"#5e81ce52\",\"editorCodeLens.foreground\":\"#5e82ceb4\",\"editorBracketMatch.background\":\"#5f7e974d\",\"editorOverviewRuler.currentContentForeground\":\"#7e57c2\",\"editorOverviewRuler.incomingContentForeground\":\"#7e57c2\",\"editorOverviewRuler.commonContentForeground\":\"#7e57c2\",\"editorGutter.background\":\"#011627\",\"editorGutter.modifiedBackground\":\"#e2b93d\",\"editorGutter.addedBackground\":\"#9ccc65\",\"editorGutter.deletedBackground\":\"#ef5350\",\"editorSuggestWidget.background\":\"#2c3043\",\"editorSuggestWidget.border\":\"#2b2f40\",\"editorSuggestWidget.foreground\":\"#d6deeb\",\"editorSuggestWidget.highlightForeground\":\"#ffffff\",\"editorSuggestWidget.selectedBackground\":\"#5f7e97\",\"debugExceptionWidget.background\":\"#011627\",\"debugExceptionWidget.border\":\"#5f7e97\",\"editorMarkerNavigation.background\":\"#0b2942\",\"editorMarkerNavigationError.background\":\"#ef5350\",\"editorMarkerNavigationWarning.background\":\"#ffca28\",\"peekView.border\":\"#5f7e97\",\"peekViewEditor.background\":\"#011627\",\"peekViewEditor.matchHighlightBackground\":\"#7e57c25a\",\"peekViewResult.background\":\"#011627\",\"peekViewResult.fileForeground\":\"#5f7e97\",\"peekViewResult.lineForeground\":\"#5f7e97\",\"peekViewResult.matchHighlightBackground\":\"#ffffffcc\",\"peekViewResult.selectionBackground\":\"#2e3250\",\"peekViewResult.selectionForeground\":\"#5f7e97\",\"peekViewTitle.background\":\"#011627\",\"peekViewTitleDescription.foreground\":\"#697098\",\"peekViewTitleLabel.foreground\":\"#5f7e97\",\"merge.currentHeaderBackground\":\"#5f7e97\",\"merge.incomingHeaderBackground\":\"#7e57c25a\",\"statusBar.background\":\"#011627\",\"statusBar.foreground\":\"#5f7e97\",\"statusBar.border\":\"#262a39\",\"statusBar.debuggingBackground\":\"#202431\",\"statusBar.debuggingBorder\":\"#1f2330\",\"statusBar.noFolderBackground\":\"#011627\",\"statusBar.noFolderBorder\":\"#25293a\",\"statusBarItem.activeBackground\":\"#202431\",\"statusBarItem.hoverBackground\":\"#202431\",\"statusBarItem.prominentBackground\":\"#202431\",\"statusBarItem.prominentHoverBackground\":\"#202431\",\"notifications.background\":\"#01111d\",\"notifications.border\":\"#262a39\",\"notificationCenter.border\":\"#262a39\",\"notificationToast.border\":\"#262a39\",\"notifications.foreground\":\"#ffffffcc\",\"notificationLink.foreground\":\"#80cbc4\",\"extensionButton.prominentForeground\":\"#ffffffcc\",\"extensionButton.prominentBackground\":\"#7e57c2cc\",\"extensionButton.prominentHoverBackground\":\"#7e57c2\",\"terminal.selectionBackground\":\"#1b90dd4d\",\"terminalCursor.background\":\"#234d70\",\"debugToolBar.background\":\"#011627\",\"welcomePage.buttonBackground\":\"#011627\",\"welcomePage.buttonHoverBackground\":\"#011627\",\"walkThrough.embeddedEditorBackground\":\"#011627\",\"gitDecoration.modifiedResourceForeground\":\"#a2bffc\",\"gitDecoration.deletedResourceForeground\":\"#ef535090\",\"gitDecoration.untrackedResourceForeground\":\"#c5e478ff\",\"gitDecoration.ignoredResourceForeground\":\"#395a75\",\"gitDecoration.conflictingResourceForeground\":\"#ffeb95cc\",\"source.elm\":\"#5f7e97\",\"string.quoted.single.js\":\"#ffffff\",\"meta.objectliteral.js\":\"#82aaff\"},\"fg\":\"#d6deeb\",\"bg\":\"#23262f\",\"semanticHighlighting\":false,\"settings\":[{\"name\":\"Changed\",\"scope\":[\"markup.changed\",\"meta.diff.header.git\",\"meta.diff.header.from-file\",\"meta.diff.header.to-file\"],\"settings\":{\"foreground\":\"#a2bffc\"}},{\"name\":\"Deleted\",\"scope\":[\"markup.deleted.diff\"],\"settings\":{\"foreground\":\"#f27775fe\"}},{\"name\":\"Inserted\",\"scope\":[\"markup.inserted.diff\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Global settings\",\"settings\":{\"background\":\"#011627\",\"foreground\":\"#d6deeb\"}},{\"name\":\"Comment\",\"scope\":[\"comment\"],\"settings\":{\"foreground\":\"#919f9f\",\"fontStyle\":\"\"}},{\"name\":\"String\",\"scope\":[\"string\"],\"settings\":{\"foreground\":\"#ecc48d\"}},{\"name\":\"String Quoted\",\"scope\":[\"string.quoted\",\"variable.other.readwrite.js\"],\"settings\":{\"foreground\":\"#ecc48d\"}},{\"name\":\"Support Constant Math\",\"scope\":[\"support.constant.math\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Number\",\"scope\":[\"constant.numeric\",\"constant.character.numeric\"],\"settings\":{\"foreground\":\"#f78c6c\",\"fontStyle\":\"\"}},{\"name\":\"Built-in constant\",\"scope\":[\"constant.language\",\"punctuation.definition.constant\",\"variable.other.constant\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"User-defined constant\",\"scope\":[\"constant.character\",\"constant.other\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Constant Character Escape\",\"scope\":[\"constant.character.escape\"],\"settings\":{\"foreground\":\"#f78c6c\"}},{\"name\":\"RegExp String\",\"scope\":[\"string.regexp\",\"string.regexp keyword.other\"],\"settings\":{\"foreground\":\"#5ca7e4\"}},{\"name\":\"Comma in functions\",\"scope\":[\"meta.function punctuation.separator.comma\"],\"settings\":{\"foreground\":\"#889fb2\"}},{\"name\":\"Variable\",\"scope\":[\"variable\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Keyword\",\"scope\":[\"punctuation.accessor\",\"keyword\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Storage\",\"scope\":[\"storage\",\"meta.var.expr\",\"meta.class meta.method.declaration meta.var.expr storage.type.js\",\"storage.type.property.js\",\"storage.type.property.ts\",\"storage.type.property.tsx\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Storage type\",\"scope\":[\"storage.type\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Storage type\",\"scope\":[\"storage.type.function.arrow.js\"],\"settings\":{\"fontStyle\":\"\"}},{\"name\":\"Class name\",\"scope\":[\"entity.name.class\",\"meta.class entity.name.type.class\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"Inherited class\",\"scope\":[\"entity.other.inherited-class\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Function name\",\"scope\":[\"entity.name.function\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Meta Tag\",\"scope\":[\"punctuation.definition.tag\",\"meta.tag\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"HTML Tag names\",\"scope\":[\"entity.name.tag\",\"meta.tag.other.html\",\"meta.tag.other.js\",\"meta.tag.other.tsx\",\"entity.name.tag.tsx\",\"entity.name.tag.js\",\"entity.name.tag\",\"meta.tag.js\",\"meta.tag.tsx\",\"meta.tag.html\"],\"settings\":{\"foreground\":\"#caece6\",\"fontStyle\":\"\"}},{\"name\":\"Tag attribute\",\"scope\":[\"entity.other.attribute-name\"],\"settings\":{\"fontStyle\":\"\",\"foreground\":\"#c5e478\"}},{\"name\":\"Entity Name Tag Custom\",\"scope\":[\"entity.name.tag.custom\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Library (function & constant)\",\"scope\":[\"support.function\",\"support.constant\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Support Constant Property Value meta\",\"scope\":[\"support.constant.meta.property-value\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Library class/type\",\"scope\":[\"support.type\",\"support.class\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Support Variable DOM\",\"scope\":[\"support.variable.dom\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Invalid\",\"scope\":[\"invalid\"],\"settings\":{\"background\":\"#ff2c83\",\"foreground\":\"#ffffff\"}},{\"name\":\"Invalid deprecated\",\"scope\":[\"invalid.deprecated\"],\"settings\":{\"foreground\":\"#ffffff\",\"background\":\"#d3423e\"}},{\"name\":\"Keyword Operator\",\"scope\":[\"keyword.operator\"],\"settings\":{\"foreground\":\"#7fdbca\",\"fontStyle\":\"\"}},{\"name\":\"Keyword Operator Relational\",\"scope\":[\"keyword.operator.relational\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Keyword Operator Assignment\",\"scope\":[\"keyword.operator.assignment\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Keyword Operator Arithmetic\",\"scope\":[\"keyword.operator.arithmetic\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Keyword Operator Bitwise\",\"scope\":[\"keyword.operator.bitwise\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Keyword Operator Increment\",\"scope\":[\"keyword.operator.increment\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Keyword Operator Ternary\",\"scope\":[\"keyword.operator.ternary\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Double-Slashed Comment\",\"scope\":[\"comment.line.double-slash\"],\"settings\":{\"foreground\":\"#919f9f\"}},{\"name\":\"Object\",\"scope\":[\"object\"],\"settings\":{\"foreground\":\"#cdebf7\"}},{\"name\":\"Null\",\"scope\":[\"constant.language.null\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"Meta Brace\",\"scope\":[\"meta.brace\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"Meta Delimiter Period\",\"scope\":[\"meta.delimiter.period\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Punctuation Definition String\",\"scope\":[\"punctuation.definition.string\"],\"settings\":{\"foreground\":\"#d9f5dd\"}},{\"name\":\"Punctuation Definition String Markdown\",\"scope\":[\"punctuation.definition.string.begin.markdown\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"Boolean\",\"scope\":[\"constant.language.boolean\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"Object Comma\",\"scope\":[\"object.comma\"],\"settings\":{\"foreground\":\"#ffffff\"}},{\"name\":\"Variable Parameter Function\",\"scope\":[\"variable.parameter.function\"],\"settings\":{\"foreground\":\"#7fdbca\",\"fontStyle\":\"\"}},{\"name\":\"Support Type Property Name & entity name tags\",\"scope\":[\"support.type.vendor.property-name\",\"support.constant.vendor.property-value\",\"support.type.property-name\",\"meta.property-list entity.name.tag\"],\"settings\":{\"foreground\":\"#80cbc4\",\"fontStyle\":\"\"}},{\"name\":\"Entity Name tag reference in stylesheets\",\"scope\":[\"meta.property-list entity.name.tag.reference\"],\"settings\":{\"foreground\":\"#57eaf1\"}},{\"name\":\"Constant Other Color RGB Value Punctuation Definition Constant\",\"scope\":[\"constant.other.color.rgb-value punctuation.definition.constant\"],\"settings\":{\"foreground\":\"#f78c6c\"}},{\"name\":\"Constant Other Color\",\"scope\":[\"constant.other.color\"],\"settings\":{\"foreground\":\"#ffeb95\"}},{\"name\":\"Keyword Other Unit\",\"scope\":[\"keyword.other.unit\"],\"settings\":{\"foreground\":\"#ffeb95\"}},{\"name\":\"Meta Selector\",\"scope\":[\"meta.selector\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Entity Other Attribute Name Id\",\"scope\":[\"entity.other.attribute-name.id\"],\"settings\":{\"foreground\":\"#fad430\"}},{\"name\":\"Meta Property Name\",\"scope\":[\"meta.property-name\"],\"settings\":{\"foreground\":\"#80cbc4\"}},{\"name\":\"Doctypes\",\"scope\":[\"entity.name.tag.doctype\",\"meta.tag.sgml.doctype\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Punctuation Definition Parameters\",\"scope\":[\"punctuation.definition.parameters\"],\"settings\":{\"foreground\":\"#d9f5dd\"}},{\"name\":\"Keyword Control Operator\",\"scope\":[\"keyword.control.operator\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Keyword Operator Logical\",\"scope\":[\"keyword.operator.logical\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Variable Instances\",\"scope\":[\"variable.instance\",\"variable.other.instance\",\"variable.readwrite.instance\",\"variable.other.readwrite.instance\",\"variable.other.property\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Variable Property Other object property\",\"scope\":[\"variable.other.object.property\"],\"settings\":{\"foreground\":\"#faf39f\",\"fontStyle\":\"\"}},{\"name\":\"Variable Property Other object\",\"scope\":[\"variable.other.object.js\"],\"settings\":{\"fontStyle\":\"\"}},{\"name\":\"Entity Name Function\",\"scope\":[\"entity.name.function\"],\"settings\":{\"foreground\":\"#82aaff\",\"fontStyle\":\"\"}},{\"name\":\"Keyword Operator Comparison, returns, imports, and Keyword Operator Ruby\",\"scope\":[\"keyword.control.conditional.js\",\"keyword.operator.comparison\",\"keyword.control.flow.js\",\"keyword.control.flow.ts\",\"keyword.control.flow.tsx\",\"keyword.control.ruby\",\"keyword.control.def.ruby\",\"keyword.control.loop.js\",\"keyword.control.loop.ts\",\"keyword.control.import.js\",\"keyword.control.import.ts\",\"keyword.control.import.tsx\",\"keyword.control.from.js\",\"keyword.control.from.ts\",\"keyword.control.from.tsx\",\"keyword.control.conditional.js\",\"keyword.control.conditional.ts\",\"keyword.control.switch.js\",\"keyword.control.switch.ts\",\"keyword.operator.instanceof.js\",\"keyword.operator.expression.instanceof.ts\",\"keyword.operator.expression.instanceof.tsx\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Support Constant, `new` keyword, Special Method Keyword, `debugger`, other keywords\",\"scope\":[\"support.constant\",\"keyword.other.special-method\",\"keyword.other.new\",\"keyword.other.debugger\",\"keyword.control\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Support Function\",\"scope\":[\"support.function\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Invalid Broken\",\"scope\":[\"invalid.broken\"],\"settings\":{\"foreground\":\"#989da0\",\"background\":\"#F78C6C\"}},{\"name\":\"Invalid Unimplemented\",\"scope\":[\"invalid.unimplemented\"],\"settings\":{\"background\":\"#8BD649\",\"foreground\":\"#ffffff\"}},{\"name\":\"Invalid Illegal\",\"scope\":[\"invalid.illegal\"],\"settings\":{\"foreground\":\"#ffffff\",\"background\":\"#ec5f67\"}},{\"name\":\"Language Variable\",\"scope\":[\"variable.language\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Support Variable Property\",\"scope\":[\"support.variable.property\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Variable Function\",\"scope\":[\"variable.function\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Variable Interpolation\",\"scope\":[\"variable.interpolation\"],\"settings\":{\"foreground\":\"#ef787f\"}},{\"name\":\"Meta Function Call\",\"scope\":[\"meta.function-call\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Punctuation Section Embedded\",\"scope\":[\"punctuation.section.embedded\"],\"settings\":{\"foreground\":\"#e2817f\"}},{\"name\":\"Punctuation Tweaks\",\"scope\":[\"punctuation.terminator.expression\",\"punctuation.definition.arguments\",\"punctuation.definition.array\",\"punctuation.section.array\",\"meta.array\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"More Punctuation Tweaks\",\"scope\":[\"punctuation.definition.list.begin\",\"punctuation.definition.list.end\",\"punctuation.separator.arguments\",\"punctuation.definition.list\"],\"settings\":{\"foreground\":\"#d9f5dd\"}},{\"name\":\"Template Strings\",\"scope\":[\"string.template meta.template.expression\"],\"settings\":{\"foreground\":\"#e2817f\"}},{\"name\":\"Backticks(``) in Template Strings\",\"scope\":[\"string.template punctuation.definition.string\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"Italics\",\"scope\":[\"italic\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"italic\"}},{\"name\":\"Bold\",\"scope\":[\"bold\"],\"settings\":{\"foreground\":\"#c5e478\",\"fontStyle\":\"bold\"}},{\"name\":\"Quote\",\"scope\":[\"quote\"],\"settings\":{\"foreground\":\"#969bb7\",\"fontStyle\":\"\"}},{\"name\":\"Raw Code\",\"scope\":[\"raw\"],\"settings\":{\"foreground\":\"#80cbc4\"}},{\"name\":\"CoffeeScript Variable Assignment\",\"scope\":[\"variable.assignment.coffee\"],\"settings\":{\"foreground\":\"#31e1eb\"}},{\"name\":\"CoffeeScript Parameter Function\",\"scope\":[\"variable.parameter.function.coffee\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"CoffeeScript Assignments\",\"scope\":[\"variable.assignment.coffee\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"C# Readwrite Variables\",\"scope\":[\"variable.other.readwrite.cs\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"C# Classes & Storage types\",\"scope\":[\"entity.name.type.class.cs\",\"storage.type.cs\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"C# Namespaces\",\"scope\":[\"entity.name.type.namespace.cs\"],\"settings\":{\"foreground\":\"#b2ccd6\"}},{\"name\":\"C# Unquoted String Zone\",\"scope\":[\"string.unquoted.preprocessor.message.cs\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"C# Region\",\"scope\":[\"punctuation.separator.hash.cs\",\"keyword.preprocessor.region.cs\",\"keyword.preprocessor.endregion.cs\"],\"settings\":{\"foreground\":\"#ffcb8b\",\"fontStyle\":\"bold\"}},{\"name\":\"C# Other Variables\",\"scope\":[\"variable.other.object.cs\"],\"settings\":{\"foreground\":\"#b2ccd6\"}},{\"name\":\"C# Enum\",\"scope\":[\"entity.name.type.enum.cs\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Dart String\",\"scope\":[\"string.interpolated.single.dart\",\"string.interpolated.double.dart\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"Dart Class\",\"scope\":[\"support.class.dart\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"Tag names in Stylesheets\",\"scope\":[\"entity.name.tag.css\",\"entity.name.tag.less\",\"entity.name.tag.custom.css\",\"support.constant.property-value.css\"],\"settings\":{\"foreground\":\"#ff6d6d\",\"fontStyle\":\"\"}},{\"name\":\"Wildcard(*) selector in Stylesheets\",\"scope\":[\"entity.name.tag.wildcard.css\",\"entity.name.tag.wildcard.less\",\"entity.name.tag.wildcard.scss\",\"entity.name.tag.wildcard.sass\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"CSS Keyword Other Unit\",\"scope\":[\"keyword.other.unit.css\"],\"settings\":{\"foreground\":\"#ffeb95\"}},{\"name\":\"Attribute Name for CSS\",\"scope\":[\"meta.attribute-selector.css entity.other.attribute-name.attribute\",\"variable.other.readwrite.js\"],\"settings\":{\"foreground\":\"#f78c6c\"}},{\"name\":\"Elixir Classes\",\"scope\":[\"source.elixir support.type.elixir\",\"source.elixir meta.module.elixir entity.name.class.elixir\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Elixir Functions\",\"scope\":[\"source.elixir entity.name.function\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Elixir Constants\",\"scope\":[\"source.elixir constant.other.symbol.elixir\",\"source.elixir constant.other.keywords.elixir\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Elixir String Punctuations\",\"scope\":[\"source.elixir punctuation.definition.string\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Elixir\",\"scope\":[\"source.elixir variable.other.readwrite.module.elixir\",\"source.elixir variable.other.readwrite.module.elixir punctuation.definition.variable.elixir\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Elixir Binary Punctuations\",\"scope\":[\"source.elixir .punctuation.binary.elixir\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"Closure Constant Keyword\",\"scope\":[\"constant.keyword.clojure\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Go Function Calls\",\"scope\":[\"source.go meta.function-call.go\"],\"settings\":{\"foreground\":\"#dddddd\"}},{\"name\":\"Go Keywords\",\"scope\":[\"source.go keyword.package.go\",\"source.go keyword.import.go\",\"source.go keyword.function.go\",\"source.go keyword.type.go\",\"source.go keyword.struct.go\",\"source.go keyword.interface.go\",\"source.go keyword.const.go\",\"source.go keyword.var.go\",\"source.go keyword.map.go\",\"source.go keyword.channel.go\",\"source.go keyword.control.go\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"Go Constants e.g. nil, string format (%s, %d, etc.)\",\"scope\":[\"source.go constant.language.go\",\"source.go constant.other.placeholder.go\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"C++ Functions\",\"scope\":[\"entity.name.function.preprocessor.cpp\",\"entity.scope.name.cpp\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"C++ Meta Namespace\",\"scope\":[\"meta.namespace-block.cpp\"],\"settings\":{\"foreground\":\"#e0dec6\"}},{\"name\":\"C++ Language Primitive Storage\",\"scope\":[\"storage.type.language.primitive.cpp\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"C++ Preprocessor Macro\",\"scope\":[\"meta.preprocessor.macro.cpp\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"C++ Variable Parameter\",\"scope\":[\"variable.parameter\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"Powershell Variables\",\"scope\":[\"variable.other.readwrite.powershell\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Powershell Function\",\"scope\":[\"support.function.powershell\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"ID Attribute Name in HTML\",\"scope\":[\"entity.other.attribute-name.id.html\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"HTML Punctuation Definition Tag\",\"scope\":[\"punctuation.definition.tag.html\"],\"settings\":{\"foreground\":\"#6ae9f0\"}},{\"name\":\"HTML Doctype\",\"scope\":[\"meta.tag.sgml.doctype.html\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"\"}},{\"name\":\"JavaScript Classes\",\"scope\":[\"meta.class entity.name.type.class.js\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"JavaScript Method Declaration e.g. `constructor`\",\"scope\":[\"meta.method.declaration storage.type.js\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"JavaScript Terminator\",\"scope\":[\"terminator.js\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"JavaScript Meta Punctuation Definition\",\"scope\":[\"meta.js punctuation.definition.js\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"Entity Names in Code Documentations\",\"scope\":[\"entity.name.type.instance.jsdoc\",\"entity.name.type.instance.phpdoc\"],\"settings\":{\"foreground\":\"#889fb2\"}},{\"name\":\"Other Variables in Code Documentations\",\"scope\":[\"variable.other.jsdoc\",\"variable.other.phpdoc\"],\"settings\":{\"foreground\":\"#78ccf0\"}},{\"name\":\"JavaScript module imports and exports\",\"scope\":[\"variable.other.meta.import.js\",\"meta.import.js variable.other\",\"variable.other.meta.export.js\",\"meta.export.js variable.other\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"JavaScript Variable Parameter Function\",\"scope\":[\"variable.parameter.function.js\"],\"settings\":{\"foreground\":\"#8b96ea\"}},{\"name\":\"JavaScript[React] Variable Other Object\",\"scope\":[\"variable.other.object.js\",\"variable.other.object.jsx\",\"variable.object.property.js\",\"variable.object.property.jsx\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"JavaScript Variables\",\"scope\":[\"variable.js\",\"variable.other.js\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"JavaScript Entity Name Type\",\"scope\":[\"entity.name.type.js\",\"entity.name.type.module.js\"],\"settings\":{\"foreground\":\"#ffcb8b\",\"fontStyle\":\"\"}},{\"name\":\"JavaScript Support Classes\",\"scope\":[\"support.class.js\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"JSON Property Names\",\"scope\":[\"support.type.property-name.json\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"JSON Support Constants\",\"scope\":[\"support.constant.json\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"JSON Property values (string)\",\"scope\":[\"meta.structure.dictionary.value.json string.quoted.double\"],\"settings\":{\"foreground\":\"#c789d6\"}},{\"name\":\"Strings in JSON values\",\"scope\":[\"string.quoted.double.json punctuation.definition.string.json\"],\"settings\":{\"foreground\":\"#80cbc4\"}},{\"name\":\"Specific JSON Property values like null\",\"scope\":[\"meta.structure.dictionary.json meta.structure.dictionary.value constant.language\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"JavaScript Other Variable\",\"scope\":[\"variable.other.object.js\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Ruby Variables\",\"scope\":[\"variable.other.ruby\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"Ruby Class\",\"scope\":[\"entity.name.type.class.ruby\"],\"settings\":{\"foreground\":\"#ecc48d\"}},{\"name\":\"Ruby Hashkeys\",\"scope\":[\"constant.language.symbol.hashkey.ruby\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"LESS Tag names\",\"scope\":[\"entity.name.tag.less\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"LESS Keyword Other Unit\",\"scope\":[\"keyword.other.unit.css\"],\"settings\":{\"foreground\":\"#ffeb95\"}},{\"name\":\"Attribute Name for LESS\",\"scope\":[\"meta.attribute-selector.less entity.other.attribute-name.attribute\"],\"settings\":{\"foreground\":\"#f78c6c\"}},{\"name\":\"Markdown Headings\",\"scope\":[\"markup.heading.markdown\",\"markup.heading.setext.1.markdown\",\"markup.heading.setext.2.markdown\"],\"settings\":{\"foreground\":\"#82b1ff\"}},{\"name\":\"Markdown Italics\",\"scope\":[\"markup.italic.markdown\"],\"settings\":{\"foreground\":\"#c792ea\",\"fontStyle\":\"italic\"}},{\"name\":\"Markdown Bold\",\"scope\":[\"markup.bold.markdown\"],\"settings\":{\"foreground\":\"#c5e478\",\"fontStyle\":\"bold\"}},{\"name\":\"Markdown Quote + others\",\"scope\":[\"markup.quote.markdown\"],\"settings\":{\"foreground\":\"#969bb7\",\"fontStyle\":\"\"}},{\"name\":\"Markdown Raw Code + others\",\"scope\":[\"markup.inline.raw.markdown\"],\"settings\":{\"foreground\":\"#80cbc4\"}},{\"name\":\"Markdown Links\",\"scope\":[\"markup.underline.link.markdown\",\"markup.underline.link.image.markdown\"],\"settings\":{\"foreground\":\"#ff869a\",\"fontStyle\":\"underline\"}},{\"name\":\"Markdown Link Title and Description\",\"scope\":[\"string.other.link.title.markdown\",\"string.other.link.description.markdown\"],\"settings\":{\"foreground\":\"#d6deeb\",\"fontStyle\":\"underline\"}},{\"name\":\"Markdown Punctuation\",\"scope\":[\"punctuation.definition.string.markdown\",\"punctuation.definition.string.begin.markdown\",\"punctuation.definition.string.end.markdown\",\"meta.link.inline.markdown punctuation.definition.string\"],\"settings\":{\"foreground\":\"#82b1ff\"}},{\"name\":\"Markdown MetaData Punctuation\",\"scope\":[\"punctuation.definition.metadata.markdown\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"Markdown List Punctuation\",\"scope\":[\"beginning.punctuation.definition.list.markdown\"],\"settings\":{\"foreground\":\"#82b1ff\"}},{\"name\":\"Markdown Inline Raw String\",\"scope\":[\"markup.inline.raw.string.markdown\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"PHP Variables\",\"scope\":[\"variable.other.php\"],\"settings\":{\"foreground\":\"#bec5d4\"}},{\"name\":\"Support Classes in PHP\",\"scope\":[\"support.class.php\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"Punctuations in PHP function calls\",\"scope\":[\"meta.function-call.php punctuation\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"PHP Global Variables\",\"scope\":[\"variable.other.global.php\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Declaration Punctuation in PHP Global Variables\",\"scope\":[\"variable.other.global.php punctuation.definition.variable\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Language Constants in Python\",\"scope\":[\"constant.language.python\"],\"settings\":{\"foreground\":\"#ff6a83\"}},{\"name\":\"Python Function Parameter and Arguments\",\"scope\":[\"variable.parameter.function.python\",\"meta.function-call.arguments.python\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Python Function Call\",\"scope\":[\"meta.function-call.python\",\"meta.function-call.generic.python\"],\"settings\":{\"foreground\":\"#b2ccd6\"}},{\"name\":\"Punctuations in Python\",\"scope\":[\"punctuation.python\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"Decorator Functions in Python\",\"scope\":[\"entity.name.function.decorator.python\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Python Language Variable\",\"scope\":[\"source.python variable.language.special\"],\"settings\":{\"foreground\":\"#8eace3\"}},{\"name\":\"Python import control keyword\",\"scope\":[\"keyword.control\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"SCSS Variable\",\"scope\":[\"variable.scss\",\"variable.sass\",\"variable.parameter.url.scss\",\"variable.parameter.url.sass\"],\"settings\":{\"foreground\":\"#c5e478\"}},{\"name\":\"Variables in SASS At-Rules\",\"scope\":[\"source.css.scss meta.at-rule variable\",\"source.css.sass meta.at-rule variable\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"Variables in SASS At-Rules\",\"scope\":[\"source.css.scss meta.at-rule variable\",\"source.css.sass meta.at-rule variable\"],\"settings\":{\"foreground\":\"#bec5d4\"}},{\"name\":\"Attribute Name for SASS\",\"scope\":[\"meta.attribute-selector.scss entity.other.attribute-name.attribute\",\"meta.attribute-selector.sass entity.other.attribute-name.attribute\"],\"settings\":{\"foreground\":\"#f78c6c\"}},{\"name\":\"Tag names in SASS\",\"scope\":[\"entity.name.tag.scss\",\"entity.name.tag.sass\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"SASS Keyword Other Unit\",\"scope\":[\"keyword.other.unit.scss\",\"keyword.other.unit.sass\"],\"settings\":{\"foreground\":\"#ffeb95\"}},{\"name\":\"TypeScript[React] Variables and Object Properties\",\"scope\":[\"variable.other.readwrite.alias.ts\",\"variable.other.readwrite.alias.tsx\",\"variable.other.readwrite.ts\",\"variable.other.readwrite.tsx\",\"variable.other.object.ts\",\"variable.other.object.tsx\",\"variable.object.property.ts\",\"variable.object.property.tsx\",\"variable.other.ts\",\"variable.other.tsx\",\"variable.tsx\",\"variable.ts\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"TypeScript[React] Entity Name Types\",\"scope\":[\"entity.name.type.ts\",\"entity.name.type.tsx\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"TypeScript[React] Node Classes\",\"scope\":[\"support.class.node.ts\",\"support.class.node.tsx\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"TypeScript[React] Entity Name Types as Parameters\",\"scope\":[\"meta.type.parameters.ts entity.name.type\",\"meta.type.parameters.tsx entity.name.type\"],\"settings\":{\"foreground\":\"#889fb2\"}},{\"name\":\"TypeScript[React] Import/Export Punctuations\",\"scope\":[\"meta.import.ts punctuation.definition.block\",\"meta.import.tsx punctuation.definition.block\",\"meta.export.ts punctuation.definition.block\",\"meta.export.tsx punctuation.definition.block\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"TypeScript[React] Punctuation Decorators\",\"scope\":[\"meta.decorator punctuation.decorator.ts\",\"meta.decorator punctuation.decorator.tsx\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"TypeScript[React] Punctuation Decorators\",\"scope\":[\"meta.tag.js meta.jsx.children.tsx\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"YAML Entity Name Tags\",\"scope\":[\"entity.name.tag.yaml\"],\"settings\":{\"foreground\":\"#7fdbca\"}},{\"name\":\"JavaScript Variable Other ReadWrite\",\"scope\":[\"variable.other.readwrite.js\",\"variable.parameter\"],\"settings\":{\"foreground\":\"#d7dbe0\"}},{\"name\":\"Support Class Component\",\"scope\":[\"support.class.component.js\",\"support.class.component.tsx\"],\"settings\":{\"foreground\":\"#f78c6c\",\"fontStyle\":\"\"}},{\"name\":\"Text nested in React tags\",\"scope\":[\"meta.jsx.children\",\"meta.jsx.children.js\",\"meta.jsx.children.tsx\"],\"settings\":{\"foreground\":\"#d6deeb\"}},{\"name\":\"TypeScript Classes\",\"scope\":[\"meta.class entity.name.type.class.tsx\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"TypeScript Entity Name Type\",\"scope\":[\"entity.name.type.tsx\",\"entity.name.type.module.tsx\"],\"settings\":{\"foreground\":\"#ffcb8b\"}},{\"name\":\"TypeScript Class Variable Keyword\",\"scope\":[\"meta.class.ts meta.var.expr.ts storage.type.ts\",\"meta.class.tsx meta.var.expr.tsx storage.type.tsx\"],\"settings\":{\"foreground\":\"#c792ea\"}},{\"name\":\"TypeScript Method Declaration e.g. `constructor`\",\"scope\":[\"meta.method.declaration storage.type.ts\",\"meta.method.declaration storage.type.tsx\"],\"settings\":{\"foreground\":\"#82aaff\"}},{\"name\":\"normalize font style of certain components\",\"scope\":[\"meta.property-list.css meta.property-value.css variable.other.less\",\"meta.property-list.scss variable.scss\",\"meta.property-list.sass variable.sass\",\"meta.brace\",\"keyword.operator.operator\",\"keyword.operator.or.regexp\",\"keyword.operator.expression.in\",\"keyword.operator.relational\",\"keyword.operator.assignment\",\"keyword.operator.comparison\",\"keyword.operator.type\",\"keyword.operator\",\"keyword\",\"punctuation.definition.string\",\"punctuation\",\"variable.other.readwrite.js\",\"storage.type\",\"source.css\",\"string.quoted\"],\"settings\":{\"fontStyle\":\"\"}}],\"styleOverrides\":{\"frames\":{\"editorBackground\":\"var(--sl-color-gray-6)\",\"terminalBackground\":\"var(--sl-color-gray-6)\",\"editorActiveTabBackground\":\"var(--sl-color-gray-6)\",\"terminalTitlebarDotsForeground\":\"color-mix(in srgb, var(--sl-color-gray-5), transparent 25%)\",\"terminalTitlebarDotsOpacity\":\"0.75\",\"inlineButtonForeground\":\"var(--sl-color-text)\",\"frameBoxShadowCssValue\":\"none\"},\"textMarkers\":{\"markBackground\":\"#ffffff17\",\"markBorderColor\":\"#ffffff40\"}}},{\"name\":\"Night Owl Light\",\"type\":\"light\",\"colors\":{\"focusBorder\":\"#93a1a1\",\"foreground\":\"#403f53\",\"disabledForeground\":\"#61616180\",\"descriptionForeground\":\"#403f53\",\"errorForeground\":\"#403f53\",\"icon.foreground\":\"#424242\",\"contrastActiveBorder\":null,\"contrastBorder\":null,\"textBlockQuote.background\":\"#7f7f7f1a\",\"textBlockQuote.border\":\"#007acc80\",\"textCodeBlock.background\":\"#dcdcdc66\",\"textLink.activeForeground\":\"#006ab1\",\"textLink.foreground\":\"#006ab1\",\"textPreformat.foreground\":\"#a31515\",\"textSeparator.foreground\":\"#0000002e\",\"editor.background\":\"#f6f7f9\",\"editor.foreground\":\"#403f53\",\"editorLineNumber.foreground\":\"#90a7b2\",\"editorLineNumber.activeForeground\":\"#403f53\",\"editorActiveLineNumber.foreground\":\"#0b216f\",\"editor.selectionBackground\":\"#e0e0e0\",\"editor.inactiveSelectionBackground\":\"#e0e0e080\",\"editor.selectionHighlightBackground\":\"#339cec33\",\"editorError.foreground\":\"#e64d49\",\"editorWarning.foreground\":\"#daaa01\",\"editorInfo.foreground\":\"#1a85ff\",\"editorHint.foreground\":\"#6c6c6c\",\"problemsErrorIcon.foreground\":\"#e64d49\",\"problemsWarningIcon.foreground\":\"#daaa01\",\"problemsInfoIcon.foreground\":\"#1a85ff\",\"editor.findMatchBackground\":\"#93a1a16c\",\"editor.findMatchHighlightBackground\":\"#93a1a16c\",\"editor.findRangeHighlightBackground\":\"#7497a633\",\"editorLink.activeForeground\":\"#0000ff\",\"editorLightBulb.foreground\":\"#ddb100\",\"editorLightBulbAutoFix.foreground\":\"#007acc\",\"diffEditor.insertedTextBackground\":\"#9ccc2c40\",\"diffEditor.insertedTextBorder\":null,\"diffEditor.removedTextBackground\":\"#ff000033\",\"diffEditor.removedTextBorder\":null,\"diffEditor.insertedLineBackground\":\"#9bb95533\",\"diffEditor.removedLineBackground\":\"#ff000033\",\"editorStickyScroll.background\":\"#fbfbfb\",\"editorStickyScrollHover.background\":\"#f0f0f0\",\"editorInlayHint.background\":\"#2aa29899\",\"editorInlayHint.foreground\":\"#f0f0f0\",\"editorInlayHint.typeBackground\":\"#2aa29899\",\"editorInlayHint.typeForeground\":\"#f0f0f0\",\"editorInlayHint.parameterBackground\":\"#2aa29899\",\"editorInlayHint.parameterForeground\":\"#f0f0f0\",\"editorPane.background\":\"#fbfbfb\",\"editorGroup.emptyBackground\":null,\"editorGroup.focusedEmptyBorder\":null,\"editorGroupHeader.tabsBackground\":\"var(--sl-color-gray-6)\",\"editorGroupHeader.tabsBorder\":\"color-mix(in srgb, var(--sl-color-gray-5), transparent 25%)\",\"editorGroupHeader.noTabsBackground\":\"#f0f0f0\",\"editorGroupHeader.border\":null,\"editorGroup.border\":\"#f0f0f0\",\"editorGroup.dropBackground\":\"#2677cb2d\",\"editorGroup.dropIntoPromptForeground\":\"#403f53\",\"editorGroup.dropIntoPromptBackground\":\"#f0f0f0\",\"editorGroup.dropIntoPromptBorder\":null,\"sideBySideEditor.horizontalBorder\":\"#f0f0f0\",\"sideBySideEditor.verticalBorder\":\"#f0f0f0\",\"scrollbar.shadow\":\"#cccccc\",\"scrollbarSlider.background\":\"#0000001a\",\"scrollbarSlider.hoverBackground\":\"#00000055\",\"scrollbarSlider.activeBackground\":\"#00000099\",\"panel.background\":\"#f0f0f0\",\"panel.border\":\"#d9d9d9\",\"panelTitle.activeBorder\":\"#424242\",\"panelTitle.activeForeground\":\"#424242\",\"panelTitle.inactiveForeground\":\"#424242bf\",\"panelSectionHeader.background\":\"#80808051\",\"terminal.background\":\"#f6f6f6\",\"widget.shadow\":\"#d9d9d9\",\"editorWidget.background\":\"#f0f0f0\",\"editorWidget.foreground\":\"#403f53\",\"editorWidget.border\":\"#d9d9d9\",\"quickInput.background\":\"#f0f0f0\",\"quickInput.foreground\":\"#403f53\",\"quickInputTitle.background\":\"#0000000f\",\"pickerGroup.foreground\":\"#403f53\",\"pickerGroup.border\":\"#d9d9d9\",\"editor.hoverHighlightBackground\":\"#339cec33\",\"editorHoverWidget.background\":\"#f0f0f0\",\"editorHoverWidget.foreground\":\"#403f53\",\"editorHoverWidget.border\":\"#d9d9d9\",\"editorHoverWidget.statusBarBackground\":\"#e4e4e4\",\"titleBar.activeBackground\":\"var(--sl-color-gray-6)\",\"titleBar.activeForeground\":\"var(--sl-color-text)\",\"titleBar.inactiveBackground\":\"#f0f0f099\",\"titleBar.inactiveForeground\":\"#33333399\",\"titleBar.border\":\"color-mix(in srgb, var(--sl-color-gray-5), transparent 25%)\",\"toolbar.hoverBackground\":\"#b8b8b850\",\"toolbar.activeBackground\":\"#a6a6a650\",\"tab.activeBackground\":\"#f6f6f6\",\"tab.unfocusedActiveBackground\":\"#f6f6f6\",\"tab.inactiveBackground\":\"#f0f0f0\",\"tab.unfocusedInactiveBackground\":\"#f0f0f0\",\"tab.activeForeground\":\"var(--sl-color-text)\",\"tab.inactiveForeground\":\"#403f53\",\"tab.unfocusedActiveForeground\":\"#403f53b3\",\"tab.unfocusedInactiveForeground\":\"#403f5380\",\"tab.hoverBackground\":null,\"tab.unfocusedHoverBackground\":null,\"tab.hoverForeground\":null,\"tab.unfocusedHoverForeground\":null,\"tab.border\":\"#f0f0f0\",\"tab.lastPinnedBorder\":\"#a9a9a9\",\"tab.activeBorder\":\"transparent\",\"tab.unfocusedActiveBorder\":null,\"tab.activeBorderTop\":\"var(--sl-color-accent)\",\"tab.unfocusedActiveBorderTop\":null,\"tab.hoverBorder\":null,\"tab.unfocusedHoverBorder\":null,\"tab.activeModifiedBorder\":\"#2aa298\",\"tab.inactiveModifiedBorder\":\"#93a1a1\",\"tab.unfocusedActiveModifiedBorder\":\"#93a1a1\",\"tab.unfocusedInactiveModifiedBorder\":\"#93a1a1\",\"badge.background\":\"#2aa298\",\"badge.foreground\":\"#f0f0f0\",\"button.background\":\"#2aa298\",\"button.foreground\":\"#f0f0f0\",\"button.border\":null,\"button.separator\":\"#f0f0f066\",\"button.hoverBackground\":\"#22827a\",\"button.secondaryBackground\":\"#5f6a79\",\"button.secondaryForeground\":\"#ffffff\",\"button.secondaryHoverBackground\":\"#4c5561\",\"dropdown.background\":\"#f0f0f0\",\"dropdown.foreground\":\"#403f53\",\"dropdown.border\":\"#d9d9d9\",\"list.activeSelectionBackground\":\"#d3e8f8\",\"list.activeSelectionForeground\":\"#403f53\",\"tree.indentGuidesStroke\":\"#a9a9a9\",\"input.background\":\"#f0f0f0\",\"input.foreground\":\"#403f53\",\"input.placeholderForeground\":\"#93a1a1\",\"inputOption.activeBorder\":\"#2aa298\",\"inputOption.hoverBackground\":\"#b8b8b850\",\"inputOption.activeBackground\":\"#93a1a133\",\"inputOption.activeForeground\":\"#000000\",\"inputValidation.infoBackground\":\"#f0f0f0\",\"inputValidation.infoBorder\":\"#d0d0d0\",\"inputValidation.warningBackground\":\"#daaa01\",\"inputValidation.warningBorder\":\"#e0af02\",\"inputValidation.errorBackground\":\"#f76e6e\",\"inputValidation.errorBorder\":\"#de3d3b\",\"keybindingLabel.background\":\"#dddddd66\",\"keybindingLabel.foreground\":\"#555555\",\"keybindingLabel.border\":\"#cccccc66\",\"keybindingLabel.bottomBorder\":\"#bbbbbb66\",\"menu.foreground\":\"#403f53\",\"menu.background\":\"#f0f0f0\",\"menu.selectionForeground\":\"#403f53\",\"menu.selectionBackground\":\"#d3e8f8\",\"menu.separatorBackground\":\"#d4d4d4\",\"editor.snippetTabstopHighlightBackground\":\"#0a326433\",\"editor.snippetFinalTabstopHighlightBorder\":\"#0a326480\",\"terminal.ansiBlack\":\"#403f53\",\"terminal.ansiRed\":\"#de3d3b\",\"terminal.ansiGreen\":\"#08916a\",\"terminal.ansiYellow\":\"#e0af02\",\"terminal.ansiBlue\":\"#288ed7\",\"terminal.ansiMagenta\":\"#d6438a\",\"terminal.ansiCyan\":\"#2aa298\",\"terminal.ansiWhite\":\"#f0f0f0\",\"terminal.ansiBrightBlack\":\"#403f53\",\"terminal.ansiBrightRed\":\"#de3d3b\",\"terminal.ansiBrightGreen\":\"#08916a\",\"terminal.ansiBrightYellow\":\"#daaa01\",\"terminal.ansiBrightBlue\":\"#288ed7\",\"terminal.ansiBrightMagenta\":\"#d6438a\",\"terminal.ansiBrightCyan\":\"#2aa298\",\"terminal.ansiBrightWhite\":\"#f0f0f0\",\"selection.background\":\"#7a8181ad\",\"notifications.background\":\"#f0f0f0\",\"notifications.foreground\":\"#403f53\",\"notificationLink.foreground\":\"#994cc3\",\"notifications.border\":\"#cccccc\",\"notificationCenter.border\":\"#cccccc\",\"notificationToast.border\":\"#cccccc\",\"notificationCenterHeader.foreground\":\"#403f53\",\"notificationCenterHeader.background\":\"#f0f0f0\",\"input.border\":\"#d9d9d9\",\"progressBar.background\":\"#2aa298\",\"list.inactiveSelectionBackground\":\"#e0e7ea\",\"list.inactiveSelectionForeground\":\"#403f53\",\"list.focusBackground\":\"#d3e8f8\",\"list.hoverBackground\":\"#d3e8f8\",\"list.focusForeground\":\"#403f53\",\"list.hoverForeground\":\"#403f53\",\"list.highlightForeground\":\"#403f53\",\"list.errorForeground\":\"#e64d49\",\"list.warningForeground\":\"#daaa01\",\"activityBar.background\":\"#f0f0f0\",\"activityBar.foreground\":\"#403f53\",\"activityBar.dropBackground\":\"#d0d0d0\",\"activityBarBadge.background\":\"#403f53\",\"activityBarBadge.foreground\":\"#f0f0f0\",\"activityBar.border\":\"#f0f0f0\",\"sideBar.background\":\"#f0f0f0\",\"sideBar.foreground\":\"#403f53\",\"sideBarTitle.foreground\":\"#403f53\",\"sideBar.border\":\"#f0f0f0\",\"editorGroup.background\":\"#f6f6f6\",\"editorCursor.foreground\":\"#90a7b2\",\"editor.wordHighlightBackground\":\"#339cec33\",\"editor.wordHighlightStrongBackground\":\"#007dd659\",\"editor.lineHighlightBackground\":\"#f0f0f0\",\"editor.rangeHighlightBackground\":\"#7497a633\",\"editorWhitespace.foreground\":\"#d9d9d9\",\"editorIndentGuide.background\":\"#d9d9d9\",\"editorCodeLens.foreground\":\"#403f53\",\"editorBracketMatch.background\":\"#d3e8f8\",\"editorBracketMatch.border\":\"#2aa298\",\"editorError.border\":\"#fbfbfb\",\"editorWarning.border\":\"#daaa01\",\"editorGutter.addedBackground\":\"#49d0c5\",\"editorGutter.modifiedBackground\":\"#6fbef6\",\"editorGutter.deletedBackground\":\"#f76e6e\",\"editorRuler.foreground\":\"#d9d9d9\",\"editorOverviewRuler.errorForeground\":\"#e64d49\",\"editorOverviewRuler.warningForeground\":\"#daaa01\",\"editorSuggestWidget.background\":\"#f0f0f0\",\"editorSuggestWidget.foreground\":\"#403f53\",\"editorSuggestWidget.highlightForeground\":\"#403f53\",\"editorSuggestWidget.selectedBackground\":\"#d3e8f8\",\"editorSuggestWidget.border\":\"#d9d9d9\",\"debugExceptionWidget.background\":\"#f0f0f0\",\"debugExceptionWidget.border\":\"#d9d9d9\",\"editorMarkerNavigation.background\":\"#d0d0d0\",\"editorMarkerNavigationError.background\":\"#f76e6e\",\"editorMarkerNavigationWarning.background\":\"#daaa01\",\"debugToolBar.background\":\"#f0f0f0\",\"extensionButton.prominentBackground\":\"#2aa298\",\"extensionButton.prominentForeground\":\"#f0f0f0\",\"statusBar.background\":\"#f0f0f0\",\"statusBar.border\":\"#f0f0f0\",\"statusBar.debuggingBackground\":\"#f0f0f0\",\"statusBar.debuggingForeground\":\"#403f53\",\"statusBar.foreground\":\"#403f53\",\"statusBar.noFolderBackground\":\"#f0f0f0\",\"statusBar.noFolderForeground\":\"#403f53\",\"peekView.border\":\"#d9d9d9\",\"peekViewEditor.background\":\"#f6f6f6\",\"peekViewEditorGutter.background\":\"#f6f6f6\",\"peekViewEditor.matchHighlightBackground\":\"#49d0c5\",\"peekViewResult.background\":\"#f0f0f0\",\"peekViewResult.fileForeground\":\"#403f53\",\"peekViewResult.lineForeground\":\"#403f53\",\"peekViewResult.matchHighlightBackground\":\"#49d0c5\",\"peekViewResult.selectionBackground\":\"#e0e7ea\",\"peekViewResult.selectionForeground\":\"#403f53\",\"peekViewTitle.background\":\"#f0f0f0\",\"peekViewTitleLabel.foreground\":\"#403f53\",\"peekViewTitleDescription.foreground\":\"#403f53\",\"terminal.foreground\":\"#403f53\"},\"fg\":\"#403f53\",\"bg\":\"#f6f7f9\",\"semanticHighlighting\":false,\"settings\":[{\"name\":\"Changed\",\"scope\":[\"markup.changed\",\"meta.diff.header.git\",\"meta.diff.header.from-file\",\"meta.diff.header.to-file\"],\"settings\":{\"foreground\":\"#556484\"}},{\"name\":\"Deleted\",\"scope\":[\"markup.deleted.diff\"],\"settings\":{\"foreground\":\"#ae3c3afd\"}},{\"name\":\"Inserted\",\"scope\":[\"markup.inserted.diff\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Global settings\",\"settings\":{\"background\":\"#011627\",\"foreground\":\"#403f53\"}},{\"name\":\"Comment\",\"scope\":[\"comment\"],\"settings\":{\"foreground\":\"#5f636f\"}},{\"name\":\"String\",\"scope\":[\"string\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"String Quoted\",\"scope\":[\"string.quoted\",\"variable.other.readwrite.js\"],\"settings\":{\"foreground\":\"#984e4d\"}},{\"name\":\"Support Constant Math\",\"scope\":[\"support.constant.math\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Number\",\"scope\":[\"constant.numeric\",\"constant.character.numeric\"],\"settings\":{\"foreground\":\"#aa0982\",\"fontStyle\":\"\"}},{\"name\":\"Built-in constant\",\"scope\":[\"constant.language\",\"punctuation.definition.constant\",\"variable.other.constant\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"User-defined constant\",\"scope\":[\"constant.character\",\"constant.other\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Constant Character Escape\",\"scope\":[\"constant.character.escape\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"RegExp String\",\"scope\":[\"string.regexp\",\"string.regexp keyword.other\"],\"settings\":{\"foreground\":\"#3a688f\"}},{\"name\":\"Comma in functions\",\"scope\":[\"meta.function punctuation.separator.comma\"],\"settings\":{\"foreground\":\"#4d667b\"}},{\"name\":\"Variable\",\"scope\":[\"variable\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Keyword\",\"scope\":[\"punctuation.accessor\",\"keyword\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Storage\",\"scope\":[\"storage\",\"meta.var.expr\",\"meta.class meta.method.declaration meta.var.expr storage.type.js\",\"storage.type.property.js\",\"storage.type.property.ts\",\"storage.type.property.tsx\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Storage type\",\"scope\":[\"storage.type\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Storage type\",\"scope\":[\"storage.type.function.arrow.js\"],\"settings\":{\"fontStyle\":\"\"}},{\"name\":\"Class name\",\"scope\":[\"entity.name.class\",\"meta.class entity.name.type.class\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Inherited class\",\"scope\":[\"entity.other.inherited-class\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Function name\",\"scope\":[\"entity.name.function\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Meta Tag\",\"scope\":[\"punctuation.definition.tag\",\"meta.tag\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"HTML Tag names\",\"scope\":[\"entity.name.tag\",\"meta.tag.other.html\",\"meta.tag.other.js\",\"meta.tag.other.tsx\",\"entity.name.tag.tsx\",\"entity.name.tag.js\",\"entity.name.tag\",\"meta.tag.js\",\"meta.tag.tsx\",\"meta.tag.html\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Tag attribute\",\"scope\":[\"entity.other.attribute-name\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Entity Name Tag Custom\",\"scope\":[\"entity.name.tag.custom\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Library (function & constant)\",\"scope\":[\"support.function\",\"support.constant\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Support Constant Property Value meta\",\"scope\":[\"support.constant.meta.property-value\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Library class/type\",\"scope\":[\"support.type\",\"support.class\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Support Variable DOM\",\"scope\":[\"support.variable.dom\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Invalid\",\"scope\":[\"invalid\"],\"settings\":{\"foreground\":\"#bb2060\"}},{\"name\":\"Invalid deprecated\",\"scope\":[\"invalid.deprecated\"],\"settings\":{\"foreground\":\"#b23834\"}},{\"name\":\"Keyword Operator\",\"scope\":[\"keyword.operator\"],\"settings\":{\"foreground\":\"#096e72\",\"fontStyle\":\"\"}},{\"name\":\"Keyword Operator Relational\",\"scope\":[\"keyword.operator.relational\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Keyword Operator Assignment\",\"scope\":[\"keyword.operator.assignment\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Keyword Operator Arithmetic\",\"scope\":[\"keyword.operator.arithmetic\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Keyword Operator Bitwise\",\"scope\":[\"keyword.operator.bitwise\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Keyword Operator Increment\",\"scope\":[\"keyword.operator.increment\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Keyword Operator Ternary\",\"scope\":[\"keyword.operator.ternary\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Double-Slashed Comment\",\"scope\":[\"comment.line.double-slash\"],\"settings\":{\"foreground\":\"#5d6376\"}},{\"name\":\"Object\",\"scope\":[\"object\"],\"settings\":{\"foreground\":\"#58656a\"}},{\"name\":\"Null\",\"scope\":[\"constant.language.null\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"Meta Brace\",\"scope\":[\"meta.brace\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"Meta Delimiter Period\",\"scope\":[\"meta.delimiter.period\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Punctuation Definition String\",\"scope\":[\"punctuation.definition.string\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Punctuation Definition String Markdown\",\"scope\":[\"punctuation.definition.string.begin.markdown\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"Boolean\",\"scope\":[\"constant.language.boolean\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"Object Comma\",\"scope\":[\"object.comma\"],\"settings\":{\"foreground\":\"#646464\"}},{\"name\":\"Variable Parameter Function\",\"scope\":[\"variable.parameter.function\"],\"settings\":{\"foreground\":\"#096e72\",\"fontStyle\":\"\"}},{\"name\":\"Support Type Property Name & entity name tags\",\"scope\":[\"support.type.vendor.property-name\",\"support.constant.vendor.property-value\",\"support.type.property-name\",\"meta.property-list entity.name.tag\"],\"settings\":{\"foreground\":\"#096e72\",\"fontStyle\":\"\"}},{\"name\":\"Entity Name tag reference in stylesheets\",\"scope\":[\"meta.property-list entity.name.tag.reference\"],\"settings\":{\"foreground\":\"#286d70\"}},{\"name\":\"Constant Other Color RGB Value Punctuation Definition Constant\",\"scope\":[\"constant.other.color.rgb-value punctuation.definition.constant\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Constant Other Color\",\"scope\":[\"constant.other.color\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Keyword Other Unit\",\"scope\":[\"keyword.other.unit\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Meta Selector\",\"scope\":[\"meta.selector\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Entity Other Attribute Name Id\",\"scope\":[\"entity.other.attribute-name.id\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Meta Property Name\",\"scope\":[\"meta.property-name\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Doctypes\",\"scope\":[\"entity.name.tag.doctype\",\"meta.tag.sgml.doctype\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Punctuation Definition Parameters\",\"scope\":[\"punctuation.definition.parameters\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Keyword Control Operator\",\"scope\":[\"keyword.control.operator\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Keyword Operator Logical\",\"scope\":[\"keyword.operator.logical\"],\"settings\":{\"foreground\":\"#8844ae\",\"fontStyle\":\"\"}},{\"name\":\"Variable Instances\",\"scope\":[\"variable.instance\",\"variable.other.instance\",\"variable.readwrite.instance\",\"variable.other.readwrite.instance\",\"variable.other.property\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Variable Property Other object property\",\"scope\":[\"variable.other.object.property\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Variable Property Other object\",\"scope\":[\"variable.other.object.js\"],\"settings\":{\"fontStyle\":\"\"}},{\"name\":\"Entity Name Function\",\"scope\":[\"entity.name.function\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Keyword Operator Comparison, imports, returns and Keyword Operator Ruby\",\"scope\":[\"keyword.operator.comparison\",\"keyword.control.flow.js\",\"keyword.control.flow.ts\",\"keyword.control.flow.tsx\",\"keyword.control.ruby\",\"keyword.control.module.ruby\",\"keyword.control.class.ruby\",\"keyword.control.def.ruby\",\"keyword.control.loop.js\",\"keyword.control.loop.ts\",\"keyword.control.import.js\",\"keyword.control.import.ts\",\"keyword.control.import.tsx\",\"keyword.control.from.js\",\"keyword.control.from.ts\",\"keyword.control.from.tsx\",\"keyword.operator.instanceof.js\",\"keyword.operator.expression.instanceof.ts\",\"keyword.operator.expression.instanceof.tsx\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Keyword Control Conditional\",\"scope\":[\"keyword.control.conditional.js\",\"keyword.control.conditional.ts\",\"keyword.control.switch.js\",\"keyword.control.switch.ts\"],\"settings\":{\"foreground\":\"#8844ae\",\"fontStyle\":\"\"}},{\"name\":\"Support Constant, `new` keyword, Special Method Keyword, `debugger`, other keywords\",\"scope\":[\"support.constant\",\"keyword.other.special-method\",\"keyword.other.new\",\"keyword.other.debugger\",\"keyword.control\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Support Function\",\"scope\":[\"support.function\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Invalid Broken\",\"scope\":[\"invalid.broken\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Invalid Unimplemented\",\"scope\":[\"invalid.unimplemented\"],\"settings\":{\"foreground\":\"#486e26\"}},{\"name\":\"Invalid Illegal\",\"scope\":[\"invalid.illegal\"],\"settings\":{\"foreground\":\"#984e4d\"}},{\"name\":\"Language Variable\",\"scope\":[\"variable.language\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Support Variable Property\",\"scope\":[\"support.variable.property\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Variable Function\",\"scope\":[\"variable.function\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Variable Interpolation\",\"scope\":[\"variable.interpolation\"],\"settings\":{\"foreground\":\"#a64348\"}},{\"name\":\"Meta Function Call\",\"scope\":[\"meta.function-call\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Punctuation Section Embedded\",\"scope\":[\"punctuation.section.embedded\"],\"settings\":{\"foreground\":\"#b23834\"}},{\"name\":\"Punctuation Tweaks\",\"scope\":[\"punctuation.terminator.expression\",\"punctuation.definition.arguments\",\"punctuation.definition.array\",\"punctuation.section.array\",\"meta.array\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"More Punctuation Tweaks\",\"scope\":[\"punctuation.definition.list.begin\",\"punctuation.definition.list.end\",\"punctuation.separator.arguments\",\"punctuation.definition.list\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Template Strings\",\"scope\":[\"string.template meta.template.expression\"],\"settings\":{\"foreground\":\"#b23834\"}},{\"name\":\"Backticks(``) in Template Strings\",\"scope\":[\"string.template punctuation.definition.string\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"Italics\",\"scope\":[\"italic\"],\"settings\":{\"foreground\":\"#8844ae\",\"fontStyle\":\"italic\"}},{\"name\":\"Bold\",\"scope\":[\"bold\"],\"settings\":{\"foreground\":\"#3b61b0\",\"fontStyle\":\"bold\"}},{\"name\":\"Quote\",\"scope\":[\"quote\"],\"settings\":{\"foreground\":\"#5c6285\"}},{\"name\":\"Raw Code\",\"scope\":[\"raw\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"CoffeeScript Variable Assignment\",\"scope\":[\"variable.assignment.coffee\"],\"settings\":{\"foreground\":\"#186e73\"}},{\"name\":\"CoffeeScript Parameter Function\",\"scope\":[\"variable.parameter.function.coffee\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"CoffeeScript Assignments\",\"scope\":[\"variable.assignment.coffee\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"C# Readwrite Variables\",\"scope\":[\"variable.other.readwrite.cs\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"C# Classes & Storage types\",\"scope\":[\"entity.name.type.class.cs\",\"storage.type.cs\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"C# Namespaces\",\"scope\":[\"entity.name.type.namespace.cs\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Tag names in Stylesheets\",\"scope\":[\"entity.name.tag.css\",\"entity.name.tag.less\",\"entity.name.tag.custom.css\",\"support.constant.property-value.css\"],\"settings\":{\"foreground\":\"#984e4d\",\"fontStyle\":\"\"}},{\"name\":\"Wildcard(*) selector in Stylesheets\",\"scope\":[\"entity.name.tag.wildcard.css\",\"entity.name.tag.wildcard.less\",\"entity.name.tag.wildcard.scss\",\"entity.name.tag.wildcard.sass\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"CSS Keyword Other Unit\",\"scope\":[\"keyword.other.unit.css\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Attribute Name for CSS\",\"scope\":[\"meta.attribute-selector.css entity.other.attribute-name.attribute\",\"variable.other.readwrite.js\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Elixir Classes\",\"scope\":[\"source.elixir support.type.elixir\",\"source.elixir meta.module.elixir entity.name.class.elixir\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Elixir Functions\",\"scope\":[\"source.elixir entity.name.function\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Elixir Constants\",\"scope\":[\"source.elixir constant.other.symbol.elixir\",\"source.elixir constant.other.keywords.elixir\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Elixir String Punctuations\",\"scope\":[\"source.elixir punctuation.definition.string\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Elixir\",\"scope\":[\"source.elixir variable.other.readwrite.module.elixir\",\"source.elixir variable.other.readwrite.module.elixir punctuation.definition.variable.elixir\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Elixir Binary Punctuations\",\"scope\":[\"source.elixir .punctuation.binary.elixir\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Closure Constant Keyword\",\"scope\":[\"constant.keyword.clojure\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Go Function Calls\",\"scope\":[\"source.go meta.function-call.go\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Go Keywords\",\"scope\":[\"source.go keyword.package.go\",\"source.go keyword.import.go\",\"source.go keyword.function.go\",\"source.go keyword.type.go\",\"source.go keyword.struct.go\",\"source.go keyword.interface.go\",\"source.go keyword.const.go\",\"source.go keyword.var.go\",\"source.go keyword.map.go\",\"source.go keyword.channel.go\",\"source.go keyword.control.go\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"Go Constants e.g. nil, string format (%s, %d, etc.)\",\"scope\":[\"source.go constant.language.go\",\"source.go constant.other.placeholder.go\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"C++ Functions\",\"scope\":[\"entity.name.function.preprocessor.cpp\",\"entity.scope.name.cpp\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"C++ Meta Namespace\",\"scope\":[\"meta.namespace-block.cpp\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"C++ Language Primitive Storage\",\"scope\":[\"storage.type.language.primitive.cpp\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"C++ Preprocessor Macro\",\"scope\":[\"meta.preprocessor.macro.cpp\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"C++ Variable Parameter\",\"scope\":[\"variable.parameter\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Powershell Variables\",\"scope\":[\"variable.other.readwrite.powershell\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Powershell Function\",\"scope\":[\"support.function.powershell\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"ID Attribute Name in HTML\",\"scope\":[\"entity.other.attribute-name.id.html\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"HTML Punctuation Definition Tag\",\"scope\":[\"punctuation.definition.tag.html\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"HTML Doctype\",\"scope\":[\"meta.tag.sgml.doctype.html\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"JavaScript Classes\",\"scope\":[\"meta.class entity.name.type.class.js\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"JavaScript Method Declaration e.g. `constructor`\",\"scope\":[\"meta.method.declaration storage.type.js\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"JavaScript Terminator\",\"scope\":[\"terminator.js\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"JavaScript Meta Punctuation Definition\",\"scope\":[\"meta.js punctuation.definition.js\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"Entity Names in Code Documentations\",\"scope\":[\"entity.name.type.instance.jsdoc\",\"entity.name.type.instance.phpdoc\"],\"settings\":{\"foreground\":\"#4d667b\"}},{\"name\":\"Other Variables in Code Documentations\",\"scope\":[\"variable.other.jsdoc\",\"variable.other.phpdoc\"],\"settings\":{\"foreground\":\"#3e697c\"}},{\"name\":\"JavaScript module imports and exports\",\"scope\":[\"variable.other.meta.import.js\",\"meta.import.js variable.other\",\"variable.other.meta.export.js\",\"meta.export.js variable.other\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"JavaScript Variable Parameter Function\",\"scope\":[\"variable.parameter.function.js\"],\"settings\":{\"foreground\":\"#555ea2\"}},{\"name\":\"JavaScript[React] Variable Other Object\",\"scope\":[\"variable.other.object.js\",\"variable.other.object.jsx\",\"variable.object.property.js\",\"variable.object.property.jsx\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"JavaScript Variables\",\"scope\":[\"variable.js\",\"variable.other.js\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"JavaScript Entity Name Type\",\"scope\":[\"entity.name.type.js\",\"entity.name.type.module.js\"],\"settings\":{\"foreground\":\"#111111\",\"fontStyle\":\"\"}},{\"name\":\"JavaScript Support Classes\",\"scope\":[\"support.class.js\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"JSON Property Names\",\"scope\":[\"support.type.property-name.json\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"JSON Support Constants\",\"scope\":[\"support.constant.json\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"JSON Property values (string)\",\"scope\":[\"meta.structure.dictionary.value.json string.quoted.double\"],\"settings\":{\"foreground\":\"#7c5686\"}},{\"name\":\"Strings in JSON values\",\"scope\":[\"string.quoted.double.json punctuation.definition.string.json\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Specific JSON Property values like null\",\"scope\":[\"meta.structure.dictionary.json meta.structure.dictionary.value constant.language\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"JavaScript Other Variable\",\"scope\":[\"variable.other.object.js\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Ruby Variables\",\"scope\":[\"variable.other.ruby\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"Ruby Class\",\"scope\":[\"entity.name.type.class.ruby\"],\"settings\":{\"foreground\":\"#984e4d\"}},{\"name\":\"Ruby Hashkeys\",\"scope\":[\"constant.language.symbol.hashkey.ruby\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Ruby Symbols\",\"scope\":[\"constant.language.symbol.ruby\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"LESS Tag names\",\"scope\":[\"entity.name.tag.less\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"LESS Keyword Other Unit\",\"scope\":[\"keyword.other.unit.css\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Attribute Name for LESS\",\"scope\":[\"meta.attribute-selector.less entity.other.attribute-name.attribute\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Markdown Headings\",\"scope\":[\"markup.heading.markdown\",\"markup.heading.setext.1.markdown\",\"markup.heading.setext.2.markdown\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Markdown Italics\",\"scope\":[\"markup.italic.markdown\"],\"settings\":{\"foreground\":\"#8844ae\",\"fontStyle\":\"italic\"}},{\"name\":\"Markdown Bold\",\"scope\":[\"markup.bold.markdown\"],\"settings\":{\"foreground\":\"#3b61b0\",\"fontStyle\":\"bold\"}},{\"name\":\"Markdown Quote + others\",\"scope\":[\"markup.quote.markdown\"],\"settings\":{\"foreground\":\"#5c6285\"}},{\"name\":\"Markdown Raw Code + others\",\"scope\":[\"markup.inline.raw.markdown\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Markdown Links\",\"scope\":[\"markup.underline.link.markdown\",\"markup.underline.link.image.markdown\"],\"settings\":{\"foreground\":\"#954f5a\",\"fontStyle\":\"underline\"}},{\"name\":\"Markdown Link Title and Description\",\"scope\":[\"string.other.link.title.markdown\",\"string.other.link.description.markdown\"],\"settings\":{\"foreground\":\"#403f53\",\"fontStyle\":\"underline\"}},{\"name\":\"Markdown Punctuation\",\"scope\":[\"punctuation.definition.string.markdown\",\"punctuation.definition.string.begin.markdown\",\"punctuation.definition.string.end.markdown\",\"meta.link.inline.markdown punctuation.definition.string\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Markdown MetaData Punctuation\",\"scope\":[\"punctuation.definition.metadata.markdown\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Markdown List Punctuation\",\"scope\":[\"beginning.punctuation.definition.list.markdown\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Markdown Inline Raw String\",\"scope\":[\"markup.inline.raw.string.markdown\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"PHP Variables\",\"scope\":[\"variable.other.php\",\"variable.other.property.php\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Support Classes in PHP\",\"scope\":[\"support.class.php\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Punctuations in PHP function calls\",\"scope\":[\"meta.function-call.php punctuation\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"PHP Global Variables\",\"scope\":[\"variable.other.global.php\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Declaration Punctuation in PHP Global Variables\",\"scope\":[\"variable.other.global.php punctuation.definition.variable\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Language Constants in Python\",\"scope\":[\"constant.language.python\"],\"settings\":{\"foreground\":\"#a24848\"}},{\"name\":\"Python Function Parameter and Arguments\",\"scope\":[\"variable.parameter.function.python\",\"meta.function-call.arguments.python\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Python Function Call\",\"scope\":[\"meta.function-call.python\",\"meta.function-call.generic.python\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"Punctuations in Python\",\"scope\":[\"punctuation.python\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"Decorator Functions in Python\",\"scope\":[\"entity.name.function.decorator.python\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Python Language Variable\",\"scope\":[\"source.python variable.language.special\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Python import control keyword\",\"scope\":[\"keyword.control\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"SCSS Variable\",\"scope\":[\"variable.scss\",\"variable.sass\",\"variable.parameter.url.scss\",\"variable.parameter.url.sass\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Variables in SASS At-Rules\",\"scope\":[\"source.css.scss meta.at-rule variable\",\"source.css.sass meta.at-rule variable\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"Variables in SASS At-Rules\",\"scope\":[\"source.css.scss meta.at-rule variable\",\"source.css.sass meta.at-rule variable\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"Attribute Name for SASS\",\"scope\":[\"meta.attribute-selector.scss entity.other.attribute-name.attribute\",\"meta.attribute-selector.sass entity.other.attribute-name.attribute\"],\"settings\":{\"foreground\":\"#aa0982\"}},{\"name\":\"Tag names in SASS\",\"scope\":[\"entity.name.tag.scss\",\"entity.name.tag.sass\"],\"settings\":{\"foreground\":\"#096e72\"}},{\"name\":\"SASS Keyword Other Unit\",\"scope\":[\"keyword.other.unit.scss\",\"keyword.other.unit.sass\"],\"settings\":{\"foreground\":\"#8844ae\"}},{\"name\":\"TypeScript[React] Variables and Object Properties\",\"scope\":[\"variable.other.readwrite.alias.ts\",\"variable.other.readwrite.alias.tsx\",\"variable.other.readwrite.ts\",\"variable.other.readwrite.tsx\",\"variable.other.object.ts\",\"variable.other.object.tsx\",\"variable.object.property.ts\",\"variable.object.property.tsx\",\"variable.other.ts\",\"variable.other.tsx\",\"variable.tsx\",\"variable.ts\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"TypeScript[React] Entity Name Types\",\"scope\":[\"entity.name.type.ts\",\"entity.name.type.tsx\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"TypeScript[React] Node Classes\",\"scope\":[\"support.class.node.ts\",\"support.class.node.tsx\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"TypeScript[React] Entity Name Types as Parameters\",\"scope\":[\"meta.type.parameters.ts entity.name.type\",\"meta.type.parameters.tsx entity.name.type\"],\"settings\":{\"foreground\":\"#4d667b\"}},{\"name\":\"TypeScript[React] Import/Export Punctuations\",\"scope\":[\"meta.import.ts punctuation.definition.block\",\"meta.import.tsx punctuation.definition.block\",\"meta.export.ts punctuation.definition.block\",\"meta.export.tsx punctuation.definition.block\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"TypeScript[React] Punctuation Decorators\",\"scope\":[\"meta.decorator punctuation.decorator.ts\",\"meta.decorator punctuation.decorator.tsx\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"TypeScript[React] Punctuation Decorators\",\"scope\":[\"meta.tag.js meta.jsx.children.tsx\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"YAML Entity Name Tags\",\"scope\":[\"entity.name.tag.yaml\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"JavaScript Variable Other ReadWrite\",\"scope\":[\"variable.other.readwrite.js\",\"variable.parameter\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"Support Class Component\",\"scope\":[\"support.class.component.js\",\"support.class.component.tsx\"],\"settings\":{\"foreground\":\"#aa0982\",\"fontStyle\":\"\"}},{\"name\":\"Text nested in React tags\",\"scope\":[\"meta.jsx.children\",\"meta.jsx.children.js\",\"meta.jsx.children.tsx\"],\"settings\":{\"foreground\":\"#403f53\"}},{\"name\":\"TypeScript Classes\",\"scope\":[\"meta.class entity.name.type.class.tsx\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"TypeScript Entity Name Type\",\"scope\":[\"entity.name.type.tsx\",\"entity.name.type.module.tsx\"],\"settings\":{\"foreground\":\"#111111\"}},{\"name\":\"TypeScript Class Variable Keyword\",\"scope\":[\"meta.class.ts meta.var.expr.ts storage.type.ts\",\"meta.class.tsx meta.var.expr.tsx storage.type.tsx\"],\"settings\":{\"foreground\":\"#76578b\"}},{\"name\":\"TypeScript Method Declaration e.g. `constructor`\",\"scope\":[\"meta.method.declaration storage.type.ts\",\"meta.method.declaration storage.type.tsx\"],\"settings\":{\"foreground\":\"#3b61b0\"}},{\"name\":\"normalize font style of certain components\",\"scope\":[\"meta.property-list.css meta.property-value.css variable.other.less\",\"meta.property-list.scss variable.scss\",\"meta.property-list.sass variable.sass\",\"meta.brace\",\"keyword.operator.operator\",\"keyword.operator.or.regexp\",\"keyword.operator.expression.in\",\"keyword.operator.relational\",\"keyword.operator.assignment\",\"keyword.operator.comparison\",\"keyword.operator.type\",\"keyword.operator\",\"keyword\",\"punctuation.definition.string\",\"punctuation\",\"variable.other.readwrite.js\",\"storage.type\",\"source.css\",\"string.quoted\"],\"settings\":{\"fontStyle\":\"\"}}],\"styleOverrides\":{\"frames\":{\"editorBackground\":\"var(--sl-color-gray-7)\",\"terminalBackground\":\"var(--sl-color-gray-7)\",\"editorActiveTabBackground\":\"var(--sl-color-gray-7)\",\"terminalTitlebarDotsForeground\":\"color-mix(in srgb, var(--sl-color-gray-5), transparent 25%)\",\"terminalTitlebarDotsOpacity\":\"0.75\",\"inlineButtonForeground\":\"var(--sl-color-text)\",\"frameBoxShadowCssValue\":\"none\"},\"textMarkers\":{\"markBackground\":\"#0000001a\",\"markBorderColor\":\"#00000055\"}}}],\"defaultLocale\":\"en\",\"cascadeLayer\":\"starlight.components\",\"styleOverrides\":{\"borderRadius\":\"0px\",\"borderWidth\":\"1px\",\"codePaddingBlock\":\"0.75rem\",\"codePaddingInline\":\"1rem\",\"codeFontFamily\":\"var(--__sl-font-mono)\",\"codeFontSize\":\"var(--sl-text-code)\",\"codeLineHeight\":\"var(--sl-line-height)\",\"uiFontFamily\":\"var(--__sl-font)\",\"textMarkers\":{\"lineDiffIndicatorMarginLeft\":\"0.25rem\",\"defaultChroma\":\"45\",\"backgroundOpacity\":\"60%\"}},\"plugins\":[{\"name\":\"Starlight Plugin\",\"hooks\":{}},{\"name\":\"astro-expressive-code\",\"hooks\":{}}]}]],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false},\"prefetch\":{\"prefetchAll\":true},\"i18n\":{\"defaultLocale\":\"en\",\"locales\":[\"en\"],\"routing\":{\"prefixDefaultLocale\":false,\"redirectToDefaultLocale\":false,\"fallbackType\":\"redirect\"}}}","docs",["Map",11,12,44,45,56,57,67,68,78,79,89,90,100,101,111,112,122,123,133,134,144,145,155,156,166,167,177,178,188,189,199,200,210,211,221,222,232,233,243,244,254,255,265,266,276,277,287,288,298,299,309,310,320,321,331,332,342,343,353,354,364,365,375,376,386,387,397,398,408,409,419,420,430,431,441,442,452,453,463,464,474,475,485,486,496,497,507,508],"index",{"id":11,"data":13,"body":41,"filePath":42,"digest":43,"deferredRender":16},{"title":14,"description":15,"editUrl":16,"head":17,"template":18,"hero":19,"sidebar":38,"pagefind":16,"draft":39},"Eigenvue Documentation","Learn to use, contribute to, and understand Eigenvue  the visual learning platform for algorithms, AI architectures, and quantum computing.",true,[],"splash",{"title":14,"tagline":20,"actions":21},"The visual learning platform for algorithms, AI architectures, and quantum computing.",[22,29,33],{"text":23,"link":24,"variant":25,"icon":26},"Get Started (Web)","/docs/getting-started/web-app/","primary",{"type":27,"name":28},"icon","right-arrow",{"text":30,"link":31,"variant":32},"Install Python Package","/docs/getting-started/python-package/","minimal",{"text":34,"link":35,"variant":32,"icon":36},"View on GitHub","https://github.com/ashutoshm1771/eigenvue",{"type":27,"name":37},"external",{"hidden":39,"attrs":40},false,{},"import { Card, CardGrid } from \"@astrojs/starlight/components\";\n\n## Choose Your Path\n\n\u003CCardGrid>\n  \u003CCard title=\"I want to learn algorithms\" icon=\"magnifier\">\n    Browse the [algorithm catalog](/docs/algorithms/) and watch algorithms execute\n    step-by-step with synchronized code highlighting and explanations.\n  \u003C/Card>\n  \u003CCard title=\"I want to use the Python package\" icon=\"laptop\">\n    Install with `pip install eigenvue` and visualize algorithms in Jupyter\n    notebooks and scripts.\n    [Get started ](/docs/getting-started/python-package/)\n  \u003C/Card>\n  \u003CCard title=\"I want to contribute\" icon=\"pencil\">\n    Add new algorithms, improve visualizations, or fix bugs. Our contributor\n    guide walks you through every step.\n    [Contribute ](/docs/contributing/development-setup/)\n  \u003C/Card>\n  \u003CCard title=\"I'm reviewing this project\" icon=\"document\">\n    JOSS reviewers and researchers: read the [project overview](/docs/research/project-overview/),\n    [tool comparison](/docs/research/comparison/), and [citation info](/docs/research/citation/).\n  \u003C/Card>\n\u003C/CardGrid>\n\n## What Is Eigenvue?\n\nEigenvue is an **open-source platform** for visualizing algorithms across four domains:\nclassical algorithms, deep learning, generative AI, and quantum computing. It is\ndistributed as two products:\n\n- **Web Application** at [eigenvue.web.app](https://eigenvue.web.app)  interactive visualizations in the browser, shareable via deep links.\n- **Python Package** via `pip install eigenvue`  visualizations in Jupyter notebooks, scripts, and research workflows.\n\nBoth products share the same algorithm definitions and produce identical visualizations.\nThe platform currently includes **17 algorithms** with step-by-step execution,\nsynchronized code highlighting, plain-language explanations, and educational content.\n\n## Algorithm Domains\n\n| Domain | Algorithms | Accent |\n|--------|-----------|--------|\n| **Classical** | Binary Search, Bubble Sort, QuickSort, Merge Sort, BFS, DFS, Dijkstra | Blue |\n| **Deep Learning** | Perceptron, Feedforward Network, Backpropagation, Convolution, Gradient Descent | Purple |\n| **Generative AI** | Tokenization (BPE), Token Embeddings, Self-Attention, Multi-Head Attention, Transformer Block | Pink |\n| **Quantum** | *Coming in Phase 14* | Cyan |\n\n## Quick Links\n\n- [Step Format Specification](/docs/api-reference/step-format/)  the universal data contract\n- [Generator API (TypeScript)](/docs/api-reference/typescript-generator-api/)  how algorithms produce steps\n- [Python Public API](/docs/api-reference/python-api/)  the Python package public API\n- [Architecture Overview](/docs/architecture/overview/)  how the system fits together","src/content/docs/index.mdx","82cbdcc9603eb56b","algorithms/backpropagation",{"id":44,"data":46,"body":53,"filePath":54,"digest":55,"deferredRender":16},{"title":47,"description":48,"editUrl":16,"head":49,"template":50,"sidebar":51,"pagefind":16,"draft":39},"Backpropagation","How neural networks learn: computing gradients through the chain rule.",[],"doc",{"hidden":39,"attrs":52},{},"{/* This file is auto-generated by scripts/generate-algorithm-docs.py. */}\r\n{/* Do not edit manually  changes will be overwritten. */}\r\n{/* To update, modify the algorithm's meta.json and re-run the script. */}\r\n\r\n**Category:** Deep Learning  \r\n**Difficulty:** Intermediate  \r\n**Time Complexity:** `O( n_l  n_{l+1})`  \r\n**Space Complexity:** `O( n_l  n_{l+1})`\r\n\r\n## Overview\r\n\r\nBackpropagation (backward propagation of errors) is the algorithm that makes neural network training possible. After a forward pass computes the network's prediction, backpropagation computes the gradient of the loss function with respect to every weight in the network by applying the chain rule of calculus layer by layer, from output back to input. These gradients tell each weight how to adjust to reduce the loss. Combined with an optimizer like gradient descent, backpropagation enables the network to learn from data. Introduced by Rumelhart, Hinton, and Williams in 1986, backpropagation remains the foundation of all deep learning training.\r\n\r\n## Try It\r\n\r\n- **Web:** [Open in Eigenvue ](https://eigenvue.web.app/algo/backpropagation)\r\n- **Python:**\r\n  ```python\r\n  import eigenvue\r\n  eigenvue.show(\"backpropagation\")\r\n  ```\r\n\r\n## Default Inputs\r\n\r\n```json\r\n{\r\n  \"layerSizes\": [\r\n    2,\r\n    2,\r\n    1\r\n  ],\r\n  \"inputValues\": [\r\n    0.5,\r\n    0.8\r\n  ],\r\n  \"targets\": [\r\n    1\r\n  ],\r\n  \"activationFunction\": \"sigmoid\",\r\n  \"lossFunction\": \"mse\",\r\n  \"learningRate\": 0.1,\r\n  \"seed\": \"backprop-default\"\r\n}\r\n```\r\n\r\n## Input Examples\r\n\r\n### Default (2-2-1 network)\r\n\r\n```json\r\n{\r\n  \"layerSizes\": [\r\n    2,\r\n    2,\r\n    1\r\n  ],\r\n  \"inputValues\": [\r\n    0.5,\r\n    0.8\r\n  ],\r\n  \"targets\": [\r\n    1\r\n  ],\r\n  \"activationFunction\": \"sigmoid\",\r\n  \"lossFunction\": \"mse\",\r\n  \"learningRate\": 0.1,\r\n  \"seed\": \"backprop-default\"\r\n}\r\n```\r\n\r\n### Higher learning rate\r\n\r\n```json\r\n{\r\n  \"layerSizes\": [\r\n    2,\r\n    2,\r\n    1\r\n  ],\r\n  \"inputValues\": [\r\n    0.5,\r\n    0.8\r\n  ],\r\n  \"targets\": [\r\n    1\r\n  ],\r\n  \"activationFunction\": \"sigmoid\",\r\n  \"lossFunction\": \"mse\",\r\n  \"learningRate\": 0.5,\r\n  \"seed\": \"backprop-default\"\r\n}\r\n```\r\n\r\n## Code\r\n\r\n### Pseudocode\r\n\r\n```\r\nfunction backpropagation(network, input, target, learningRate):\r\n  // Forward pass\r\n  activations = forwardPass(network, input)\r\n  loss = computeLoss(activations[-1], target)\r\n\r\n  // Backward pass  compute gradients\r\n  delta = lossDeriv(activations[-1], target)\r\n  for layer in reversed(range(num_layers - 1)):\r\n    weightGrad = delta  activations[layer].T\r\n    biasGrad = delta\r\n    delta = (weights[layer].T  delta) * activDeriv(z[layer])\r\n\r\n    // Update weights\r\n    weights[layer] -= learningRate * weightGrad\r\n    biases[layer] -= learningRate * biasGrad\r\n```\r\n\r\n## Key Concepts\r\n\r\n### Chain Rule of Calculus\r\n\r\nBackpropagation is an efficient application of the chain rule. To find how a weight in an early layer affects the loss, we multiply the local gradients along the path from the loss back to that weight: Loss/w = Loss/a  a/z  z/w.\r\n\r\n### Gradient Descent\r\n\r\nGradients indicate the direction of steepest increase. To minimize the loss, we move weights in the opposite direction of the gradient: w_new = w_old -   Loss/w, where  is the learning rate.\r\n\r\n### Forward Then Backward\r\n\r\nTraining requires two passes: a forward pass to compute predictions and loss, then a backward pass to compute gradients. Both passes have the same computational cost  O(number of weights).\r\n\r\n## Common Pitfalls\r\n\r\n- **Learning rate too large:** A learning rate that is too large causes weight updates to overshoot the minimum, potentially diverging. Too small means painfully slow convergence.\r\n- **Vanishing gradients in deep networks:** When using sigmoid or tanh activations, gradients shrink exponentially as they propagate backward through many layers, making early layers almost impossible to train.\r\n\r\n## Quiz\r\n\r\n**Q1: What mathematical tool does backpropagation use to compute gradients?**\r\n\r\n- A) Integration by parts\r\n- B) The chain rule of calculus\r\n- C) Taylor series expansion\r\n- D) Fourier transform\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) The chain rule of calculus\r\n\r\nBackpropagation applies the chain rule to compute the gradient of the loss with respect to each weight by multiplying local gradients along the computation path.\r\n\r\n\u003C/details>\r\n\r\n## Further Reading\r\n\r\n- [Learning Representations by Back-Propagating Errors (Rumelhart et al., 1986)](https://www.nature.com/articles/323533a0) (paper)\r\n- [Backpropagation  3Blue1Brown](https://www.3blue1brown.com/lessons/backpropagation) (video)\r\n\r\n## Related Algorithms\r\n\r\n- [Feedforward Neural Network](/docs/algorithms/feedforward-network/)\r\n- [Gradient Descent](/docs/algorithms/gradient-descent/)\r\n\r\n## Prerequisites\r\n\r\n- [Feedforward Neural Network](/docs/algorithms/feedforward-network/)","src/content/docs/algorithms/backpropagation.mdx","309e95def50103f5","algorithms/bfs",{"id":56,"data":58,"body":64,"filePath":65,"digest":66,"deferredRender":16},{"title":59,"description":60,"editUrl":16,"head":61,"template":50,"sidebar":62,"pagefind":16,"draft":39},"Breadth-First Search","Explore a graph level by level using a queue.",[],{"hidden":39,"attrs":63},{},"{/* This file is auto-generated by scripts/generate-algorithm-docs.py. */}\r\n{/* Do not edit manually  changes will be overwritten. */}\r\n{/* To update, modify the algorithm's meta.json and re-run the script. */}\r\n\r\n**Category:** Classical  \r\n**Difficulty:** Intermediate  \r\n**Time Complexity:** `O(V + E)`  \r\n**Space Complexity:** `O(V)`\r\n\r\n## Overview\r\n\r\nBFS explores all neighbors of the current node before moving to the next level. It uses a FIFO queue and guarantees the shortest path (minimum edges) in unweighted graphs.\r\n\r\n## Try It\r\n\r\n- **Web:** [Open in Eigenvue ](https://eigenvue.web.app/algo/bfs)\r\n- **Python:**\r\n  ```python\r\n  import eigenvue\r\n  eigenvue.show(\"bfs\")\r\n  ```\r\n\r\n## Default Inputs\r\n\r\n```json\r\n{\r\n  \"adjacencyList\": {\r\n    \"A\": [\r\n      \"B\",\r\n      \"C\"\r\n    ],\r\n    \"B\": [\r\n      \"A\",\r\n      \"D\",\r\n      \"E\"\r\n    ],\r\n    \"C\": [\r\n      \"A\",\r\n      \"F\"\r\n    ],\r\n    \"D\": [\r\n      \"B\"\r\n    ],\r\n    \"E\": [\r\n      \"B\",\r\n      \"F\"\r\n    ],\r\n    \"F\": [\r\n      \"C\",\r\n      \"E\"\r\n    ]\r\n  },\r\n  \"positions\": {\r\n    \"A\": {\r\n      \"x\": 0.1,\r\n      \"y\": 0.5\r\n    },\r\n    \"B\": {\r\n      \"x\": 0.35,\r\n      \"y\": 0.2\r\n    },\r\n    \"C\": {\r\n      \"x\": 0.35,\r\n      \"y\": 0.8\r\n    },\r\n    \"D\": {\r\n      \"x\": 0.65,\r\n      \"y\": 0.1\r\n    },\r\n    \"E\": {\r\n      \"x\": 0.65,\r\n      \"y\": 0.5\r\n    },\r\n    \"F\": {\r\n      \"x\": 0.9,\r\n      \"y\": 0.8\r\n    }\r\n  },\r\n  \"startNode\": \"A\",\r\n  \"targetNode\": \"F\"\r\n}\r\n```\r\n\r\n## Input Examples\r\n\r\n### 6-node graph (find F)\r\n\r\n```json\r\n{\r\n  \"adjacencyList\": {\r\n    \"A\": [\r\n      \"B\",\r\n      \"C\"\r\n    ],\r\n    \"B\": [\r\n      \"A\",\r\n      \"D\",\r\n      \"E\"\r\n    ],\r\n    \"C\": [\r\n      \"A\",\r\n      \"F\"\r\n    ],\r\n    \"D\": [\r\n      \"B\"\r\n    ],\r\n    \"E\": [\r\n      \"B\",\r\n      \"F\"\r\n    ],\r\n    \"F\": [\r\n      \"C\",\r\n      \"E\"\r\n    ]\r\n  },\r\n  \"positions\": {\r\n    \"A\": {\r\n      \"x\": 0.1,\r\n      \"y\": 0.5\r\n    },\r\n    \"B\": {\r\n      \"x\": 0.35,\r\n      \"y\": 0.2\r\n    },\r\n    \"C\": {\r\n      \"x\": 0.35,\r\n      \"y\": 0.8\r\n    },\r\n    \"D\": {\r\n      \"x\": 0.65,\r\n      \"y\": 0.1\r\n    },\r\n    \"E\": {\r\n      \"x\": 0.65,\r\n      \"y\": 0.5\r\n    },\r\n    \"F\": {\r\n      \"x\": 0.9,\r\n      \"y\": 0.8\r\n    }\r\n  },\r\n  \"startNode\": \"A\",\r\n  \"targetNode\": \"F\"\r\n}\r\n```\r\n\r\n### Explore all (no target)\r\n\r\n```json\r\n{\r\n  \"adjacencyList\": {\r\n    \"A\": [\r\n      \"B\",\r\n      \"C\"\r\n    ],\r\n    \"B\": [\r\n      \"A\",\r\n      \"D\"\r\n    ],\r\n    \"C\": [\r\n      \"A\"\r\n    ],\r\n    \"D\": [\r\n      \"B\"\r\n    ]\r\n  },\r\n  \"positions\": {\r\n    \"A\": {\r\n      \"x\": 0.2,\r\n      \"y\": 0.5\r\n    },\r\n    \"B\": {\r\n      \"x\": 0.5,\r\n      \"y\": 0.2\r\n    },\r\n    \"C\": {\r\n      \"x\": 0.5,\r\n      \"y\": 0.8\r\n    },\r\n    \"D\": {\r\n      \"x\": 0.8,\r\n      \"y\": 0.5\r\n    }\r\n  },\r\n  \"startNode\": \"A\"\r\n}\r\n```\r\n\r\n## Code\r\n\r\n### Pseudocode\r\n\r\n```\r\nfunction BFS(graph, start, target?):\r\n  visited = {start}\r\n  queue = [start]\r\n  predecessor[start] = null\r\n  while queue is not empty:\r\n    current = queue.dequeue()\r\n    for each neighbor of current:\r\n      if neighbor not in visited:\r\n        visited.add(neighbor)\r\n        predecessor[neighbor] = current\r\n        queue.enqueue(neighbor)\r\n        if neighbor == target:\r\n          return reconstructPath(predecessor, target)\r\n  return null  // target not found\r\n```\r\n\r\n### Python\r\n\r\n```python\r\nfrom collections import deque\r\n\r\ndef bfs(graph, start, target=None):\r\n    visited = {start}\r\n    queue = deque([start])\r\n    pred = {start: None}\r\n    while queue:\r\n        current = queue.popleft()\r\n        for neighbor in graph[current]:\r\n            if neighbor not in visited:\r\n                visited.add(neighbor)\r\n                pred[neighbor] = current\r\n                queue.append(neighbor)\r\n                if neighbor == target:\r\n                    return reconstruct(pred, target)\r\n    return None\r\n```\r\n\r\n## Key Concepts\r\n\r\n### Level-by-Level Exploration\r\n\r\nBFS visits all nodes at distance d before any at distance d+1. This is guaranteed by the FIFO queue.\r\n\r\n### Shortest Path (Unweighted)\r\n\r\nThe first time BFS reaches a node, it found the shortest path (fewest edges). This does NOT apply to weighted graphs  use Dijkstra for those.\r\n\r\n### Queue (FIFO)\r\n\r\nThe queue ensures first-in-first-out order, which is what makes BFS explore level by level rather than depth-first.\r\n\r\n## Common Pitfalls\r\n\r\n- **Not for weighted graphs:** BFS finds the shortest path only in UNWEIGHTED graphs. For weighted graphs, use Dijkstra's algorithm.\r\n- **Memory usage:** BFS can use a lot of memory because it stores all nodes at the current level in the queue. For very wide graphs, this can be a problem.\r\n\r\n## Quiz\r\n\r\n**Q1: What data structure does BFS use?**\r\n\r\n- A) Stack\r\n- B) Queue\r\n- C) Priority Queue\r\n- D) Hash Set\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) Queue\r\n\r\nBFS uses a FIFO queue. Nodes are added to the back and removed from the front, ensuring level-by-level exploration.\r\n\r\n\u003C/details>\r\n\r\n**Q2: Does BFS guarantee the shortest path?**\r\n\r\n- A) Always\r\n- B) Only in unweighted graphs\r\n- C) Only in weighted graphs\r\n- D) Never\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) Only in unweighted graphs\r\n\r\nBFS guarantees shortest path only in unweighted graphs (where all edges have equal cost). For weighted graphs, use Dijkstra's algorithm.\r\n\r\n\u003C/details>\r\n\r\n## Further Reading\r\n\r\n- [Wikipedia: BFS](https://en.wikipedia.org/wiki/Breadth-first_search) (article)\r\n\r\n## Related Algorithms\r\n\r\n- [Depth-First Search](/docs/algorithms/dfs/)\r\n- [Dijkstra's Shortest Path](/docs/algorithms/dijkstra/)","src/content/docs/algorithms/bfs.mdx","d9f4e2f5871e2e5c","algorithms/binary-search",{"id":67,"data":69,"body":75,"filePath":76,"digest":77,"deferredRender":16},{"title":70,"description":71,"editUrl":16,"head":72,"template":50,"sidebar":73,"pagefind":16,"draft":39},"Binary Search","Efficiently find a target in a sorted array by halving the search space.",[],{"hidden":39,"attrs":74},{},"{/* This file is auto-generated by scripts/generate-algorithm-docs.py. */}\r\n{/* Do not edit manually  changes will be overwritten. */}\r\n{/* To update, modify the algorithm's meta.json and re-run the script. */}\r\n\r\n**Category:** Classical  \r\n**Difficulty:** Beginner  \r\n**Time Complexity:** `O(log n)`  \r\n**Space Complexity:** `O(1)`\r\n\r\n## Overview\r\n\r\nBinary search is a divide-and-conquer algorithm that finds a target value in a sorted array. At each step, it compares the target with the middle element and eliminates half of the remaining search space. This gives it O(log n) time complexity  dramatically faster than linear search for large arrays. Binary search is one of the most fundamental algorithms in computer science and appears everywhere from database indexing to debugging (git bisect).\r\n\r\n## Try It\r\n\r\n- **Web:** [Open in Eigenvue ](https://eigenvue.web.app/algo/binary-search)\r\n- **Python:**\r\n  ```python\r\n  import eigenvue\r\n  eigenvue.show(\"binary-search\")\r\n  ```\r\n\r\n## Default Inputs\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    1,\r\n    3,\r\n    5,\r\n    7,\r\n    9,\r\n    11,\r\n    13,\r\n    15,\r\n    17,\r\n    19\r\n  ],\r\n  \"target\": 13\r\n}\r\n```\r\n\r\n## Input Examples\r\n\r\n### Default (target found)\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    1,\r\n    3,\r\n    5,\r\n    7,\r\n    9,\r\n    11,\r\n    13,\r\n    15,\r\n    17,\r\n    19\r\n  ],\r\n  \"target\": 13\r\n}\r\n```\r\n\r\n### Target not found\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    2,\r\n    4,\r\n    6,\r\n    8,\r\n    10,\r\n    12,\r\n    14\r\n  ],\r\n  \"target\": 5\r\n}\r\n```\r\n\r\n### Single element (found)\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    42\r\n  ],\r\n  \"target\": 42\r\n}\r\n```\r\n\r\n### Single element (not found)\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    42\r\n  ],\r\n  \"target\": 99\r\n}\r\n```\r\n\r\n### Target is first element\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    1,\r\n    3,\r\n    5,\r\n    7,\r\n    9,\r\n    11,\r\n    13\r\n  ],\r\n  \"target\": 1\r\n}\r\n```\r\n\r\n### Target is last element\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    1,\r\n    3,\r\n    5,\r\n    7,\r\n    9,\r\n    11,\r\n    13\r\n  ],\r\n  \"target\": 13\r\n}\r\n```\r\n\r\n### Large array\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    2,\r\n    5,\r\n    8,\r\n    11,\r\n    14,\r\n    17,\r\n    20,\r\n    23,\r\n    26,\r\n    29,\r\n    32,\r\n    35,\r\n    38,\r\n    41,\r\n    44,\r\n    47\r\n  ],\r\n  \"target\": 29\r\n}\r\n```\r\n\r\n## Code\r\n\r\n### Pseudocode\r\n\r\n```\r\nfunction binarySearch(array, target):\r\n  left = 0\r\n  right = length(array) - 1\r\n\r\n  while left \u003C= right:\r\n    mid = floor((left + right) / 2)\r\n\r\n    if array[mid] == target:\r\n      return mid          // Found!\r\n\r\n    if array[mid] \u003C target:\r\n      left = mid + 1      // Search right half\r\n    else:\r\n      right = mid - 1     // Search left half\r\n\r\n  return -1               // Not found\r\n```\r\n\r\n### Python\r\n\r\n```python\r\ndef binary_search(array: list[int], target: int) -> int:\r\n    left = 0\r\n    right = len(array) - 1\r\n\r\n    while left \u003C= right:\r\n        mid = (left + right) // 2\r\n\r\n        if array[mid] == target:\r\n            return mid\r\n\r\n        if array[mid] \u003C target:\r\n            left = mid + 1\r\n        else:\r\n            right = mid - 1\r\n\r\n    return -1\r\n```\r\n\r\n### JavaScript\r\n\r\n```javascript\r\nfunction binarySearch(array, target) {\r\n  let left = 0;\r\n  let right = array.length - 1;\r\n\r\n  while (left \u003C= right) {\r\n    const mid = Math.floor((left + right) / 2);\r\n\r\n    if (array[mid] === target) {\r\n      return mid;\r\n    }\r\n\r\n    if (array[mid] \u003C target) {\r\n      left = mid + 1;\r\n    } else {\r\n      right = mid - 1;\r\n    }\r\n  }\r\n\r\n  return -1;\r\n}\r\n```\r\n\r\n## Key Concepts\r\n\r\n### Divide and Conquer\r\n\r\nBinary search works by repeatedly dividing the search interval in half. If the target is less than the middle element, narrow to the left half. If greater, narrow to the right half.\r\n\r\n### Sorted Input Requirement\r\n\r\nBinary search only works on sorted arrays. If the input is unsorted, the algorithm may miss the target entirely. Always verify the array is sorted before applying binary search.\r\n\r\n### Logarithmic Time Complexity\r\n\r\nEach comparison eliminates half the remaining elements. For an array of n elements, this means at most log(n) comparisons. For 1 million elements, that's at most 20 comparisons  compared to 1 million for linear search.\r\n\r\n### Integer Overflow in Mid Calculation\r\n\r\nThe classic formula mid = (left + right) / 2 can overflow for very large arrays. A safer alternative is mid = left + (right - left) / 2. In JavaScript with 64-bit floats this isn't an issue for arrays up to 2, but it matters in languages with 32-bit integers like Java or C.\r\n\r\n## Common Pitfalls\r\n\r\n- **Off-by-one in boundaries:** The most common binary search bug is using &lt; instead of &lt;= in the while condition, or forgetting the +1 / -1 when updating left and right. These cause the algorithm to miss elements or loop infinitely.\r\n- **Unsorted input:** Binary search silently produces wrong results on unsorted arrays. It won't crash  it'll just return an incorrect answer. Always validate that input is sorted.\r\n- **Empty array:** An empty array should return -1 immediately. The loop condition left &lt;= right handles this correctly (0 &lt;= -1 is false), but it's worth testing explicitly.\r\n\r\n## Quiz\r\n\r\n**Q1: What is the maximum number of comparisons binary search needs for an array of 1024 elements?**\r\n\r\n- A) 10\r\n- B) 11\r\n- C) 32\r\n- D) 1024\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) 11\r\n\r\nlog(1024) = 10, but we need one extra comparison for the final check, giving us at most 11 comparisons. Each comparison halves the space: 1024  512  256  ...  1.\r\n\r\n\u003C/details>\r\n\r\n**Q2: What happens if you use binary search on an unsorted array?**\r\n\r\n- A) It throws an error\r\n- B) It always returns -1\r\n- C) It may return a wrong index or miss the target\r\n- D) It sorts the array first\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** C) It may return a wrong index or miss the target\r\n\r\nBinary search assumes the array is sorted. On an unsorted array, the comparisons are meaningless  the algorithm may eliminate the half that actually contains the target, returning an incorrect result with no warning.\r\n\r\n\u003C/details>\r\n\r\n**Q3: What is the space complexity of binary search?**\r\n\r\n- A) O(1)\r\n- B) O(log n)\r\n- C) O(n)\r\n- D) O(n log n)\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** A) O(1)\r\n\r\nIterative binary search uses only a constant amount of extra memory (the left, right, and mid pointers), regardless of input size. The recursive version uses O(log n) stack space.\r\n\r\n\u003C/details>\r\n\r\n## Further Reading\r\n\r\n- [Binary Search  Wikipedia](https://en.wikipedia.org/wiki/Binary_search_algorithm) (reference)\r\n- [Nearly All Binary Searches Are Broken](https://research.google/blog/extra-extra-read-all-about-it-nearly-all-binary-searches-and-mergesorts-are-broken/) (article)\r\n\r\n## Related Algorithms\r\n\r\n- [Bubble Sort](/docs/algorithms/bubble-sort/)\r\n- [QuickSort](/docs/algorithms/quicksort/)\r\n- [Merge Sort](/docs/algorithms/merge-sort/)","src/content/docs/algorithms/binary-search.mdx","bd46f85bcc791a74","algorithms/bubble-sort",{"id":78,"data":80,"body":86,"filePath":87,"digest":88,"deferredRender":16},{"title":81,"description":82,"editUrl":16,"head":83,"template":50,"sidebar":84,"pagefind":16,"draft":39},"Bubble Sort","Repeatedly swap adjacent out-of-order elements until the array is sorted.",[],{"hidden":39,"attrs":85},{},"{/* This file is auto-generated by scripts/generate-algorithm-docs.py. */}\r\n{/* Do not edit manually  changes will be overwritten. */}\r\n{/* To update, modify the algorithm's meta.json and re-run the script. */}\r\n\r\n**Category:** Classical  \r\n**Difficulty:** Beginner  \r\n**Time Complexity:** `O(n)`  \r\n**Space Complexity:** `O(1)`\r\n\r\n## Overview\r\n\r\nBubble Sort is a simple comparison-based sorting algorithm. It repeatedly steps through the array, compares adjacent elements, and swaps them if they are in the wrong order. After each full pass, the largest unsorted element 'bubbles up' to its correct position. The algorithm terminates early if a pass completes with no swaps, indicating the array is already sorted. While its O(n) worst-case performance makes it impractical for large datasets, Bubble Sort is an excellent educational algorithm for understanding sorting fundamentals, invariants, and optimization techniques like early termination.\r\n\r\n## Try It\r\n\r\n- **Web:** [Open in Eigenvue ](https://eigenvue.web.app/algo/bubble-sort)\r\n- **Python:**\r\n  ```python\r\n  import eigenvue\r\n  eigenvue.show(\"bubble-sort\")\r\n  ```\r\n\r\n## Default Inputs\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    64,\r\n    34,\r\n    25,\r\n    12,\r\n    22,\r\n    11,\r\n    90\r\n  ]\r\n}\r\n```\r\n\r\n## Input Examples\r\n\r\n### Default (unsorted)\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    64,\r\n    34,\r\n    25,\r\n    12,\r\n    22,\r\n    11,\r\n    90\r\n  ]\r\n}\r\n```\r\n\r\n### Already sorted\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    1,\r\n    2,\r\n    3,\r\n    4,\r\n    5\r\n  ]\r\n}\r\n```\r\n\r\n### Reverse sorted (worst case)\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    5,\r\n    4,\r\n    3,\r\n    2,\r\n    1\r\n  ]\r\n}\r\n```\r\n\r\n### Single element\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    42\r\n  ]\r\n}\r\n```\r\n\r\n### Empty array\r\n\r\n```json\r\n{\r\n  \"array\": []\r\n}\r\n```\r\n\r\n### With duplicates\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    5,\r\n    3,\r\n    8,\r\n    3,\r\n    2,\r\n    5,\r\n    1\r\n  ]\r\n}\r\n```\r\n\r\n### Nearly sorted\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    1,\r\n    2,\r\n    5,\r\n    3,\r\n    4\r\n  ]\r\n}\r\n```\r\n\r\n## Code\r\n\r\n### Pseudocode\r\n\r\n```\r\nfunction bubbleSort(array):\r\n  n = length(array)\r\n  for pass = 0 to n - 2:\r\n    swapped = false\r\n    for j = 0 to n - 2 - pass:\r\n      if array[j] > array[j + 1]:\r\n        swap(array[j], array[j + 1])\r\n        swapped = true\r\n    if not swapped:\r\n      break          // Early termination\r\n  return array\r\n```\r\n\r\n### Python\r\n\r\n```python\r\ndef bubble_sort(array: list[int]) -> list[int]:\r\n    n = len(array)\r\n    for pass_num in range(n - 1):\r\n        swapped = False\r\n        for j in range(n - 1 - pass_num):\r\n            if array[j] > array[j + 1]:\r\n                array[j], array[j + 1] = array[j + 1], array[j]\r\n                swapped = True\r\n        if not swapped:\r\n            break\r\n    return array\r\n```\r\n\r\n### JavaScript\r\n\r\n```javascript\r\nfunction bubbleSort(array) {\r\n  const n = array.length;\r\n  for (let pass = 0; pass \u003C n - 1; pass++) {\r\n    let swapped = false;\r\n    for (let j = 0; j \u003C n - 1 - pass; j++) {\r\n      if (array[j] > array[j + 1]) {\r\n        [array[j], array[j + 1]] = [array[j + 1], array[j]];\r\n        swapped = true;\r\n      }\r\n    }\r\n    if (!swapped) break;\r\n  }\r\n  return array;\r\n}\r\n```\r\n\r\n## Key Concepts\r\n\r\n### Bubbling Up\r\n\r\nIn each pass, the largest unsorted element 'bubbles up' to its correct position at the end of the unsorted portion. After pass p, the element at index N-1-p is in its final sorted position.\r\n\r\n### Early Termination\r\n\r\nIf a complete pass makes no swaps, the array is already sorted. This optimization reduces the best-case time complexity from O(n) to O(n) for already-sorted arrays.\r\n\r\n### Stability\r\n\r\nBubble Sort is a stable sorting algorithm. Equal elements are never swapped (we use strict > comparison), so their relative order from the original array is preserved.\r\n\r\n### Adaptive Behavior\r\n\r\nBubble Sort is adaptive  it performs fewer operations on nearly-sorted arrays. The number of swaps equals the number of inversions in the input, and early termination can skip unnecessary passes.\r\n\r\n## Common Pitfalls\r\n\r\n- **O(n) worst case:** A reverse-sorted array requires the maximum number of comparisons and swaps: n(n-1)/2. For large arrays, this makes Bubble Sort impractical compared to O(n log n) algorithms like Merge Sort or Quick Sort.\r\n- **Forgetting early termination:** Without the swapped flag, Bubble Sort always runs all n-1 passes even on a sorted array. The early termination optimization is simple to implement and dramatically improves best-case performance.\r\n- **Using >= instead of >:** Using >= for comparison instead of > breaks stability. Equal elements would be swapped unnecessarily, changing their relative order from the original array.\r\n\r\n## Quiz\r\n\r\n**Q1: After 2 passes of Bubble Sort on an array of 6 elements, how many elements are guaranteed to be in their final positions?**\r\n\r\n- A) 1\r\n- B) 2\r\n- C) 3\r\n- D) 6\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) 2\r\n\r\nAfter each pass, the largest unsorted element bubbles to its correct position. After 2 passes, the 2 largest elements are in their final positions at the end of the array.\r\n\r\n\u003C/details>\r\n\r\n**Q2: What is the best-case time complexity of optimized Bubble Sort (with early termination)?**\r\n\r\n- A) O(1)\r\n- B) O(n)\r\n- C) O(n log n)\r\n- D) O(n)\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) O(n)\r\n\r\nOn an already-sorted array, Bubble Sort makes one pass with n-1 comparisons and zero swaps, then terminates early. This gives O(n) best-case time complexity.\r\n\r\n\u003C/details>\r\n\r\n**Q3: How many swaps does Bubble Sort perform on the array [3, 1, 2]?**\r\n\r\n- A) 1\r\n- B) 2\r\n- C) 3\r\n- D) 4\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) 2\r\n\r\nPass 1: Compare 3,1  swap ([1,3,2]), compare 3,2  swap ([1,2,3]). Pass 2: Compare 1,2  no swap. Early termination. Total: 2 swaps.\r\n\r\n\u003C/details>\r\n\r\n## Further Reading\r\n\r\n- [Bubble Sort  Wikipedia](https://en.wikipedia.org/wiki/Bubble_sort) (reference)\r\n- [VisuAlgo  Sorting Visualizations](https://visualgo.net/en/sorting) (interactive)\r\n\r\n## Related Algorithms\r\n\r\n- [Merge Sort](/docs/algorithms/merge-sort/)\r\n- [QuickSort](/docs/algorithms/quicksort/)\r\n- [Binary Search](/docs/algorithms/binary-search/)","src/content/docs/algorithms/bubble-sort.mdx","3f02f864f4ade154","algorithms/convolution",{"id":89,"data":91,"body":97,"filePath":98,"digest":99,"deferredRender":16},{"title":92,"description":93,"editUrl":16,"head":94,"template":50,"sidebar":95,"pagefind":16,"draft":39},"Convolution (2D)","How CNNs detect features: sliding a kernel across an input grid.",[],{"hidden":39,"attrs":96},{},"{/* This file is auto-generated by scripts/generate-algorithm-docs.py. */}\r\n{/* Do not edit manually  changes will be overwritten. */}\r\n{/* To update, modify the algorithm's meta.json and re-run the script. */}\r\n\r\n**Category:** Deep Learning  \r\n**Difficulty:** Intermediate  \r\n**Time Complexity:** `O(H  W  K  C)`  \r\n**Space Complexity:** `O(H  W + K)`\r\n\r\n## Overview\r\n\r\nConvolution is the core operation in Convolutional Neural Networks (CNNs). A small matrix called a kernel (or filter) slides across a 2D input (such as an image), computing the element-wise product between the kernel and each overlapping patch of the input, then summing the products to produce a single output value. This process generates an output feature map that highlights where certain features (edges, textures, patterns) appear in the input. The key insight of convolution is weight sharing  the same kernel weights are reused across all spatial positions, dramatically reducing the number of parameters compared to a fully connected layer. Convolution was popularized by LeCun et al. (1989) for handwritten digit recognition and remains the backbone of modern computer vision.\r\n\r\n## Try It\r\n\r\n- **Web:** [Open in Eigenvue ](https://eigenvue.web.app/algo/convolution)\r\n- **Python:**\r\n  ```python\r\n  import eigenvue\r\n  eigenvue.show(\"convolution\")\r\n  ```\r\n\r\n## Default Inputs\r\n\r\n```json\r\n{\r\n  \"input\": [\r\n    [\r\n      1,\r\n      2,\r\n      3,\r\n      0\r\n    ],\r\n    [\r\n      4,\r\n      5,\r\n      6,\r\n      1\r\n    ],\r\n    [\r\n      7,\r\n      8,\r\n      9,\r\n      2\r\n    ],\r\n    [\r\n      0,\r\n      1,\r\n      2,\r\n      3\r\n    ]\r\n  ],\r\n  \"kernel\": [\r\n    [\r\n      1,\r\n      0\r\n    ],\r\n    [\r\n      0,\r\n      -1\r\n    ]\r\n  ]\r\n}\r\n```\r\n\r\n## Input Examples\r\n\r\n### Default (44 input, 22 kernel)\r\n\r\n```json\r\n{\r\n  \"input\": [\r\n    [\r\n      1,\r\n      2,\r\n      3,\r\n      0\r\n    ],\r\n    [\r\n      4,\r\n      5,\r\n      6,\r\n      1\r\n    ],\r\n    [\r\n      7,\r\n      8,\r\n      9,\r\n      2\r\n    ],\r\n    [\r\n      0,\r\n      1,\r\n      2,\r\n      3\r\n    ]\r\n  ],\r\n  \"kernel\": [\r\n    [\r\n      1,\r\n      0\r\n    ],\r\n    [\r\n      0,\r\n      -1\r\n    ]\r\n  ]\r\n}\r\n```\r\n\r\n### Edge detection (33 kernel)\r\n\r\n```json\r\n{\r\n  \"input\": [\r\n    [\r\n      0,\r\n      0,\r\n      0,\r\n      0,\r\n      0\r\n    ],\r\n    [\r\n      0,\r\n      1,\r\n      1,\r\n      1,\r\n      0\r\n    ],\r\n    [\r\n      0,\r\n      1,\r\n      1,\r\n      1,\r\n      0\r\n    ],\r\n    [\r\n      0,\r\n      1,\r\n      1,\r\n      1,\r\n      0\r\n    ],\r\n    [\r\n      0,\r\n      0,\r\n      0,\r\n      0,\r\n      0\r\n    ]\r\n  ],\r\n  \"kernel\": [\r\n    [\r\n      -1,\r\n      -1,\r\n      -1\r\n    ],\r\n    [\r\n      -1,\r\n      8,\r\n      -1\r\n    ],\r\n    [\r\n      -1,\r\n      -1,\r\n      -1\r\n    ]\r\n  ]\r\n}\r\n```\r\n\r\n## Code\r\n\r\n### Pseudocode\r\n\r\n```\r\nfunction convolve2D(input, kernel):\r\n  H, W = dimensions(input)\r\n  kH, kW = dimensions(kernel)\r\n  outH = H - kH + 1\r\n  outW = W - kW + 1\r\n  output = zeros(outH, outW)\r\n\r\n  for row in range(outH):\r\n    for col in range(outW):\r\n      sum = 0\r\n      for kr in range(kH):\r\n        for kc in range(kW):\r\n          sum += input[row+kr][col+kc] * kernel[kr][kc]\r\n      output[row][col] = sum\r\n  return output\r\n```\r\n\r\n## Key Concepts\r\n\r\n### Kernel Sliding\r\n\r\nThe kernel slides across the input one position at a time (stride=1). At each position, it computes the dot product between the kernel and the overlapping input patch. This produces one value in the output feature map.\r\n\r\n### Weight Sharing\r\n\r\nThe same kernel weights are applied at every position in the input. This means the network detects the same feature regardless of where it appears  a property called translation equivariance.\r\n\r\n### Feature Maps\r\n\r\nEach kernel produces one feature map. Using multiple kernels (channels) allows the network to detect different features (edges, textures, shapes) simultaneously.\r\n\r\n## Common Pitfalls\r\n\r\n- **Output size reduction:** Without padding, convolution reduces spatial dimensions: output_size = input_size - kernel_size + 1. Use padding='same' to maintain dimensions.\r\n- **Kernel size must be  input size:** The kernel cannot be larger than the input in any dimension. This is a hard constraint of the convolution operation.\r\n\r\n## Quiz\r\n\r\n**Q1: What is the output size of convolving a 55 input with a 33 kernel (no padding, stride 1)?**\r\n\r\n- A) 55\r\n- B) 44\r\n- C) 33\r\n- D) 22\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** C) 33\r\n\r\nOutput size = input_size - kernel_size + 1 = 5 - 3 + 1 = 3. So the output is 33.\r\n\r\n\u003C/details>\r\n\r\n## Further Reading\r\n\r\n- [Backpropagation Applied to Handwritten Zip Code Recognition (LeCun et al., 1989)](https://ieeexplore.ieee.org/document/6795724) (paper)\r\n- [Convolution  Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network) (article)\r\n\r\n## Related Algorithms\r\n\r\n- [Feedforward Neural Network](/docs/algorithms/feedforward-network/)\r\n- [Backpropagation](/docs/algorithms/backpropagation/)\r\n\r\n## Prerequisites\r\n\r\n- [Single Neuron / Perceptron](/docs/algorithms/perceptron/)","src/content/docs/algorithms/convolution.mdx","2b5ea7b038a51ff3","algorithms/dfs",{"id":100,"data":102,"body":108,"filePath":109,"digest":110,"deferredRender":16},{"title":103,"description":104,"editUrl":16,"head":105,"template":50,"sidebar":106,"pagefind":16,"draft":39},"Depth-First Search","Explore a graph by going as deep as possible before backtracking.",[],{"hidden":39,"attrs":107},{},"{/* This file is auto-generated by scripts/generate-algorithm-docs.py. */}\r\n{/* Do not edit manually  changes will be overwritten. */}\r\n{/* To update, modify the algorithm's meta.json and re-run the script. */}\r\n\r\n**Category:** Classical  \r\n**Difficulty:** Intermediate  \r\n**Time Complexity:** `O(V + E)`  \r\n**Space Complexity:** `O(V)`\r\n\r\n## Overview\r\n\r\nDFS explores a graph by following each branch as far as possible before backtracking. It uses a stack (explicit or via recursion) and is useful for cycle detection, topological sorting, and finding connected components. DFS does NOT guarantee shortest paths.\r\n\r\n## Try It\r\n\r\n- **Web:** [Open in Eigenvue ](https://eigenvue.web.app/algo/dfs)\r\n- **Python:**\r\n  ```python\r\n  import eigenvue\r\n  eigenvue.show(\"dfs\")\r\n  ```\r\n\r\n## Default Inputs\r\n\r\n```json\r\n{\r\n  \"adjacencyList\": {\r\n    \"A\": [\r\n      \"B\",\r\n      \"C\"\r\n    ],\r\n    \"B\": [\r\n      \"A\",\r\n      \"D\",\r\n      \"E\"\r\n    ],\r\n    \"C\": [\r\n      \"A\",\r\n      \"F\"\r\n    ],\r\n    \"D\": [\r\n      \"B\"\r\n    ],\r\n    \"E\": [\r\n      \"B\",\r\n      \"F\"\r\n    ],\r\n    \"F\": [\r\n      \"C\",\r\n      \"E\"\r\n    ]\r\n  },\r\n  \"positions\": {\r\n    \"A\": {\r\n      \"x\": 0.1,\r\n      \"y\": 0.5\r\n    },\r\n    \"B\": {\r\n      \"x\": 0.35,\r\n      \"y\": 0.2\r\n    },\r\n    \"C\": {\r\n      \"x\": 0.35,\r\n      \"y\": 0.8\r\n    },\r\n    \"D\": {\r\n      \"x\": 0.65,\r\n      \"y\": 0.1\r\n    },\r\n    \"E\": {\r\n      \"x\": 0.65,\r\n      \"y\": 0.5\r\n    },\r\n    \"F\": {\r\n      \"x\": 0.9,\r\n      \"y\": 0.8\r\n    }\r\n  },\r\n  \"startNode\": \"A\",\r\n  \"targetNode\": \"F\"\r\n}\r\n```\r\n\r\n## Input Examples\r\n\r\n### 6-node graph\r\n\r\n```json\r\n{\r\n  \"adjacencyList\": {\r\n    \"A\": [\r\n      \"B\",\r\n      \"C\"\r\n    ],\r\n    \"B\": [\r\n      \"A\",\r\n      \"D\",\r\n      \"E\"\r\n    ],\r\n    \"C\": [\r\n      \"A\",\r\n      \"F\"\r\n    ],\r\n    \"D\": [\r\n      \"B\"\r\n    ],\r\n    \"E\": [\r\n      \"B\",\r\n      \"F\"\r\n    ],\r\n    \"F\": [\r\n      \"C\",\r\n      \"E\"\r\n    ]\r\n  },\r\n  \"positions\": {\r\n    \"A\": {\r\n      \"x\": 0.1,\r\n      \"y\": 0.5\r\n    },\r\n    \"B\": {\r\n      \"x\": 0.35,\r\n      \"y\": 0.2\r\n    },\r\n    \"C\": {\r\n      \"x\": 0.35,\r\n      \"y\": 0.8\r\n    },\r\n    \"D\": {\r\n      \"x\": 0.65,\r\n      \"y\": 0.1\r\n    },\r\n    \"E\": {\r\n      \"x\": 0.65,\r\n      \"y\": 0.5\r\n    },\r\n    \"F\": {\r\n      \"x\": 0.9,\r\n      \"y\": 0.8\r\n    }\r\n  },\r\n  \"startNode\": \"A\",\r\n  \"targetNode\": \"F\"\r\n}\r\n```\r\n\r\n### Linear chain\r\n\r\n```json\r\n{\r\n  \"adjacencyList\": {\r\n    \"A\": [\r\n      \"B\"\r\n    ],\r\n    \"B\": [\r\n      \"A\",\r\n      \"C\"\r\n    ],\r\n    \"C\": [\r\n      \"B\",\r\n      \"D\"\r\n    ],\r\n    \"D\": [\r\n      \"C\"\r\n    ]\r\n  },\r\n  \"positions\": {\r\n    \"A\": {\r\n      \"x\": 0.1,\r\n      \"y\": 0.5\r\n    },\r\n    \"B\": {\r\n      \"x\": 0.35,\r\n      \"y\": 0.5\r\n    },\r\n    \"C\": {\r\n      \"x\": 0.65,\r\n      \"y\": 0.5\r\n    },\r\n    \"D\": {\r\n      \"x\": 0.9,\r\n      \"y\": 0.5\r\n    }\r\n  },\r\n  \"startNode\": \"A\",\r\n  \"targetNode\": \"D\"\r\n}\r\n```\r\n\r\n## Code\r\n\r\n### Pseudocode\r\n\r\n```\r\nfunction DFS(graph, start, target?):\r\n  stack = [start]\r\n  predecessor[start] = null\r\n  visited = {}\r\n  while stack is not empty:\r\n    current = stack.pop()\r\n    if current in visited: continue\r\n    visited.add(current)\r\n    if current == target:\r\n      return reconstructPath(predecessor, target)\r\n    for each neighbor of current (reversed):\r\n      if neighbor not in visited:\r\n        predecessor[neighbor] = current\r\n        stack.push(neighbor)\r\n  return null  // target not found\r\n```\r\n\r\n### Python\r\n\r\n```python\r\ndef dfs(graph, start, target=None):\r\n    stack = [start]\r\n    visited = set()\r\n    pred = {start: None}\r\n    while stack:\r\n        current = stack.pop()\r\n        if current in visited:\r\n            continue\r\n        visited.add(current)\r\n        if current == target:\r\n            return reconstruct(pred, target)\r\n        for neighbor in reversed(sorted(graph[current])):\r\n            if neighbor not in visited:\r\n                if neighbor not in pred:\r\n                    pred[neighbor] = current\r\n                stack.append(neighbor)\r\n    return None\r\n```\r\n\r\n## Key Concepts\r\n\r\n### Stack-Based Exploration\r\n\r\nDFS uses a LIFO stack. The last node pushed is the first explored, causing the algorithm to go deep before wide.\r\n\r\n### Backtracking\r\n\r\nWhen DFS reaches a dead end (no unvisited neighbors), it backtracks by popping the stack until it finds a node with unexplored neighbors.\r\n\r\n### Does NOT Find Shortest Paths\r\n\r\nDFS may find a path to the target, but it is not guaranteed to be the shortest. Use BFS for shortest paths in unweighted graphs.\r\n\r\n## Common Pitfalls\r\n\r\n- **Not shortest path:** DFS can find a valid path but not the shortest one. The path depends on traversal order.\r\n- **Infinite loops without visited check:** In graphs with cycles, DFS will loop forever without a visited set.\r\n\r\n## Quiz\r\n\r\n**Q1: What data structure does DFS use?**\r\n\r\n- A) Queue\r\n- B) Stack\r\n- C) Priority Queue\r\n- D) Array\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) Stack\r\n\r\nDFS uses a LIFO stack (or recursion, which implicitly uses the call stack).\r\n\r\n\u003C/details>\r\n\r\n**Q2: Does DFS guarantee shortest paths?**\r\n\r\n- A) Yes\r\n- B) No\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) No\r\n\r\nNo. DFS explores depth-first and may find a longer path before a shorter one. Use BFS for shortest paths.\r\n\r\n\u003C/details>\r\n\r\n## Further Reading\r\n\r\n- [Wikipedia: DFS](https://en.wikipedia.org/wiki/Depth-first_search) (article)\r\n\r\n## Related Algorithms\r\n\r\n- [Breadth-First Search](/docs/algorithms/bfs/)\r\n- [Dijkstra's Shortest Path](/docs/algorithms/dijkstra/)","src/content/docs/algorithms/dfs.mdx","8c4678f5f19a4ee3","algorithms/dijkstra",{"id":111,"data":113,"body":119,"filePath":120,"digest":121,"deferredRender":16},{"title":114,"description":115,"editUrl":16,"head":116,"template":50,"sidebar":117,"pagefind":16,"draft":39},"Dijkstra's Shortest Path","Find shortest paths between nodes in a weighted graph using a greedy approach.",[],{"hidden":39,"attrs":118},{},"{/* This file is auto-generated by scripts/generate-algorithm-docs.py. */}\r\n{/* Do not edit manually  changes will be overwritten. */}\r\n{/* To update, modify the algorithm's meta.json and re-run the script. */}\r\n\r\n**Category:** Classical  \r\n**Difficulty:** Advanced  \r\n**Time Complexity:** `O((V + E) log V)`  \r\n**Space Complexity:** `O(V)`\r\n\r\n## Overview\r\n\r\nDijkstra's algorithm finds the shortest paths from a source node to all other nodes in a weighted graph. It works by maintaining a priority queue of tentative distances and greedily selecting the closest unvisited node at each step. It requires all edge weights to be non-negative.\r\n\r\n## Try It\r\n\r\n- **Web:** [Open in Eigenvue ](https://eigenvue.web.app/algo/dijkstra)\r\n- **Python:**\r\n  ```python\r\n  import eigenvue\r\n  eigenvue.show(\"dijkstra\")\r\n  ```\r\n\r\n## Default Inputs\r\n\r\n```json\r\n{\r\n  \"adjacencyList\": {\r\n    \"A\": [\r\n      {\r\n        \"to\": \"B\",\r\n        \"weight\": 4\r\n      },\r\n      {\r\n        \"to\": \"C\",\r\n        \"weight\": 2\r\n      }\r\n    ],\r\n    \"B\": [\r\n      {\r\n        \"to\": \"A\",\r\n        \"weight\": 4\r\n      },\r\n      {\r\n        \"to\": \"C\",\r\n        \"weight\": 1\r\n      },\r\n      {\r\n        \"to\": \"D\",\r\n        \"weight\": 5\r\n      }\r\n    ],\r\n    \"C\": [\r\n      {\r\n        \"to\": \"A\",\r\n        \"weight\": 2\r\n      },\r\n      {\r\n        \"to\": \"B\",\r\n        \"weight\": 1\r\n      },\r\n      {\r\n        \"to\": \"D\",\r\n        \"weight\": 8\r\n      },\r\n      {\r\n        \"to\": \"E\",\r\n        \"weight\": 10\r\n      }\r\n    ],\r\n    \"D\": [\r\n      {\r\n        \"to\": \"B\",\r\n        \"weight\": 5\r\n      },\r\n      {\r\n        \"to\": \"C\",\r\n        \"weight\": 8\r\n      },\r\n      {\r\n        \"to\": \"E\",\r\n        \"weight\": 2\r\n      }\r\n    ],\r\n    \"E\": [\r\n      {\r\n        \"to\": \"C\",\r\n        \"weight\": 10\r\n      },\r\n      {\r\n        \"to\": \"D\",\r\n        \"weight\": 2\r\n      }\r\n    ]\r\n  },\r\n  \"positions\": {\r\n    \"A\": {\r\n      \"x\": 0.1,\r\n      \"y\": 0.3\r\n    },\r\n    \"B\": {\r\n      \"x\": 0.4,\r\n      \"y\": 0.1\r\n    },\r\n    \"C\": {\r\n      \"x\": 0.4,\r\n      \"y\": 0.5\r\n    },\r\n    \"D\": {\r\n      \"x\": 0.7,\r\n      \"y\": 0.3\r\n    },\r\n    \"E\": {\r\n      \"x\": 0.9,\r\n      \"y\": 0.5\r\n    }\r\n  },\r\n  \"startNode\": \"A\",\r\n  \"targetNode\": \"E\"\r\n}\r\n```\r\n\r\n## Input Examples\r\n\r\n### 5-node weighted graph\r\n\r\n```json\r\n{\r\n  \"adjacencyList\": {\r\n    \"A\": [\r\n      {\r\n        \"to\": \"B\",\r\n        \"weight\": 4\r\n      },\r\n      {\r\n        \"to\": \"C\",\r\n        \"weight\": 2\r\n      }\r\n    ],\r\n    \"B\": [\r\n      {\r\n        \"to\": \"A\",\r\n        \"weight\": 4\r\n      },\r\n      {\r\n        \"to\": \"C\",\r\n        \"weight\": 1\r\n      },\r\n      {\r\n        \"to\": \"D\",\r\n        \"weight\": 5\r\n      }\r\n    ],\r\n    \"C\": [\r\n      {\r\n        \"to\": \"A\",\r\n        \"weight\": 2\r\n      },\r\n      {\r\n        \"to\": \"B\",\r\n        \"weight\": 1\r\n      },\r\n      {\r\n        \"to\": \"D\",\r\n        \"weight\": 8\r\n      },\r\n      {\r\n        \"to\": \"E\",\r\n        \"weight\": 10\r\n      }\r\n    ],\r\n    \"D\": [\r\n      {\r\n        \"to\": \"B\",\r\n        \"weight\": 5\r\n      },\r\n      {\r\n        \"to\": \"C\",\r\n        \"weight\": 8\r\n      },\r\n      {\r\n        \"to\": \"E\",\r\n        \"weight\": 2\r\n      }\r\n    ],\r\n    \"E\": [\r\n      {\r\n        \"to\": \"C\",\r\n        \"weight\": 10\r\n      },\r\n      {\r\n        \"to\": \"D\",\r\n        \"weight\": 2\r\n      }\r\n    ]\r\n  },\r\n  \"positions\": {\r\n    \"A\": {\r\n      \"x\": 0.1,\r\n      \"y\": 0.3\r\n    },\r\n    \"B\": {\r\n      \"x\": 0.4,\r\n      \"y\": 0.1\r\n    },\r\n    \"C\": {\r\n      \"x\": 0.4,\r\n      \"y\": 0.5\r\n    },\r\n    \"D\": {\r\n      \"x\": 0.7,\r\n      \"y\": 0.3\r\n    },\r\n    \"E\": {\r\n      \"x\": 0.9,\r\n      \"y\": 0.5\r\n    }\r\n  },\r\n  \"startNode\": \"A\",\r\n  \"targetNode\": \"E\"\r\n}\r\n```\r\n\r\n### Linear chain\r\n\r\n```json\r\n{\r\n  \"adjacencyList\": {\r\n    \"A\": [\r\n      {\r\n        \"to\": \"B\",\r\n        \"weight\": 3\r\n      }\r\n    ],\r\n    \"B\": [\r\n      {\r\n        \"to\": \"A\",\r\n        \"weight\": 3\r\n      },\r\n      {\r\n        \"to\": \"C\",\r\n        \"weight\": 5\r\n      }\r\n    ],\r\n    \"C\": [\r\n      {\r\n        \"to\": \"B\",\r\n        \"weight\": 5\r\n      },\r\n      {\r\n        \"to\": \"D\",\r\n        \"weight\": 2\r\n      }\r\n    ],\r\n    \"D\": [\r\n      {\r\n        \"to\": \"C\",\r\n        \"weight\": 2\r\n      }\r\n    ]\r\n  },\r\n  \"positions\": {\r\n    \"A\": {\r\n      \"x\": 0.1,\r\n      \"y\": 0.5\r\n    },\r\n    \"B\": {\r\n      \"x\": 0.35,\r\n      \"y\": 0.5\r\n    },\r\n    \"C\": {\r\n      \"x\": 0.65,\r\n      \"y\": 0.5\r\n    },\r\n    \"D\": {\r\n      \"x\": 0.9,\r\n      \"y\": 0.5\r\n    }\r\n  },\r\n  \"startNode\": \"A\",\r\n  \"targetNode\": \"D\"\r\n}\r\n```\r\n\r\n### Unreachable target\r\n\r\n```json\r\n{\r\n  \"adjacencyList\": {\r\n    \"A\": [\r\n      {\r\n        \"to\": \"B\",\r\n        \"weight\": 1\r\n      }\r\n    ],\r\n    \"B\": [\r\n      {\r\n        \"to\": \"A\",\r\n        \"weight\": 1\r\n      }\r\n    ],\r\n    \"C\": []\r\n  },\r\n  \"positions\": {\r\n    \"A\": {\r\n      \"x\": 0.2,\r\n      \"y\": 0.5\r\n    },\r\n    \"B\": {\r\n      \"x\": 0.5,\r\n      \"y\": 0.5\r\n    },\r\n    \"C\": {\r\n      \"x\": 0.8,\r\n      \"y\": 0.5\r\n    }\r\n  },\r\n  \"startNode\": \"A\",\r\n  \"targetNode\": \"C\"\r\n}\r\n```\r\n\r\n## Code\r\n\r\n### Pseudocode\r\n\r\n```\r\nfunction dijkstra(graph, source):\r\n  dist[source] = 0\r\n  for each vertex v  source:\r\n    dist[v] = \r\n  PQ = priority queue with (source, 0)\r\n  while PQ is not empty:\r\n    u = PQ.extractMin()\r\n    for each neighbor v of u:\r\n      newDist = dist[u] + weight(u, v)\r\n      if newDist \u003C dist[v]:\r\n        dist[v] = newDist\r\n        predecessor[v] = u\r\n        PQ.insert(v, newDist)\r\n  return dist, predecessor\r\n```\r\n\r\n### Python\r\n\r\n```python\r\nimport heapq\r\n\r\ndef dijkstra(graph, source):\r\n    dist = {v: float('inf') for v in graph}\r\n    dist[source] = 0\r\n    pq = [(0, source)]\r\n    predecessor = {v: None for v in graph}\r\n    visited = set()\r\n    \r\n    while pq:\r\n        d, u = heapq.heappop(pq)\r\n        if u in visited:\r\n            continue\r\n        visited.add(u)\r\n        for v, w in graph[u]:\r\n            if dist[u] + w \u003C dist[v]:\r\n                dist[v] = dist[u] + w\r\n                predecessor[v] = u\r\n                heapq.heappush(pq, (dist[v], v))\r\n    return dist, predecessor\r\n```\r\n\r\n## Key Concepts\r\n\r\n### Greedy Strategy\r\n\r\nDijkstra's greedily picks the unvisited node with the smallest tentative distance. This works because with non-negative weights, once we process a node, we've already found its shortest path.\r\n\r\n### Relaxation\r\n\r\nWhen we find a shorter path to a node through a newly processed node, we 'relax' the edge by updating the distance. This is the core operation: dist[v] = min(dist[v], dist[u] + weight(u,v)).\r\n\r\n### Priority Queue\r\n\r\nThe priority queue efficiently extracts the minimum-distance node. Using a binary heap gives O(log V) extraction and insertion.\r\n\r\n### Non-Negative Weights Required\r\n\r\nDijkstra's fails with negative edge weights because the greedy assumption breaks. Use Bellman-Ford for graphs with negative edges.\r\n\r\n## Common Pitfalls\r\n\r\n- **Negative edge weights:** If any edge has a negative weight, Dijkstra's can produce incorrect results. Always verify all weights  0.\r\n- **Stale PQ entries:** When using a simple heap, the same node may appear multiple times. Always check if visited when extracting.\r\n\r\n## Quiz\r\n\r\n**Q1: Why does Dijkstra's algorithm fail with negative edge weights?**\r\n\r\n- A) It would cause an infinite loop\r\n- B) The greedy minimum extraction may process a node before finding its true shortest path\r\n- C) Negative numbers can't be stored in the priority queue\r\n- D) The algorithm was not designed for negative numbers\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) The greedy minimum extraction may process a node before finding its true shortest path\r\n\r\nWith negative edges, a node processed early might have its distance improved later through a negative-weight edge. The greedy extraction assumes the extracted distance is final.\r\n\r\n\u003C/details>\r\n\r\n**Q2: What is the time complexity of Dijkstra's with a binary heap?**\r\n\r\n- A) O(V)\r\n- B) O(V + E)\r\n- C) O((V + E) log V)\r\n- D) O(V  E)\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** C) O((V + E) log V)\r\n\r\nWith a binary heap: V extractions  O(log V) + E relaxations  O(log V) = O((V+E) log V).\r\n\r\n\u003C/details>\r\n\r\n## Further Reading\r\n\r\n- [Wikipedia: Dijkstra's Algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm) (article)\r\n- [Computerphile: Dijkstra's Algorithm](https://www.youtube.com/watch?v=GazC3A4OQTE) (video)\r\n\r\n## Related Algorithms\r\n\r\n- [Breadth-First Search](/docs/algorithms/bfs/)\r\n- [Depth-First Search](/docs/algorithms/dfs/)\r\n\r\n## Prerequisites\r\n\r\n- [Breadth-First Search](/docs/algorithms/bfs/)","src/content/docs/algorithms/dijkstra.mdx","1cd740c5bf7bbace","algorithms/feedforward-network",{"id":122,"data":124,"body":130,"filePath":131,"digest":132,"deferredRender":16},{"title":125,"description":126,"editUrl":16,"head":127,"template":50,"sidebar":128,"pagefind":16,"draft":39},"Feedforward Neural Network","Watch data flow forward through a multi-layer neural network.",[],{"hidden":39,"attrs":129},{},"{/* This file is auto-generated by scripts/generate-algorithm-docs.py. */}\r\n{/* Do not edit manually  changes will be overwritten. */}\r\n{/* To update, modify the algorithm's meta.json and re-run the script. */}\r\n\r\n**Category:** Deep Learning  \r\n**Difficulty:** Intermediate  \r\n**Time Complexity:** `O( n_l  n_{l-1})`  \r\n**Space Complexity:** `O( n_l)`\r\n\r\n## Overview\r\n\r\nA feedforward neural network passes data through multiple layers of neurons. Each layer applies a linear transformation (weights  inputs + bias) followed by a non-linear activation function. This visualization shows the forward propagation process layer by layer, revealing how raw inputs are transformed into predictions through successive non-linear transformations.\r\n\r\n## Try It\r\n\r\n- **Web:** [Open in Eigenvue ](https://eigenvue.web.app/algo/feedforward-network)\r\n- **Python:**\r\n  ```python\r\n  import eigenvue\r\n  eigenvue.show(\"feedforward-network\")\r\n  ```\r\n\r\n## Default Inputs\r\n\r\n```json\r\n{\r\n  \"inputValues\": [\r\n    0.5,\r\n    0.8\r\n  ],\r\n  \"layerSizes\": [\r\n    2,\r\n    3,\r\n    1\r\n  ],\r\n  \"activationFunction\": \"sigmoid\",\r\n  \"seed\": \"eigenvue-ff\"\r\n}\r\n```\r\n\r\n## Input Examples\r\n\r\n### 2-3-1 network (sigmoid)\r\n\r\n```json\r\n{\r\n  \"inputValues\": [\r\n    0.5,\r\n    0.8\r\n  ],\r\n  \"layerSizes\": [\r\n    2,\r\n    3,\r\n    1\r\n  ],\r\n  \"activationFunction\": \"sigmoid\",\r\n  \"seed\": \"eigenvue-ff\"\r\n}\r\n```\r\n\r\n### 3-4-4-2 deep network\r\n\r\n```json\r\n{\r\n  \"inputValues\": [\r\n    0.3,\r\n    0.7,\r\n    0.5\r\n  ],\r\n  \"layerSizes\": [\r\n    3,\r\n    4,\r\n    4,\r\n    2\r\n  ],\r\n  \"activationFunction\": \"relu\",\r\n  \"seed\": \"deep-net\"\r\n}\r\n```\r\n\r\n### 2-2-1 minimal\r\n\r\n```json\r\n{\r\n  \"inputValues\": [\r\n    1.0,\r\n    0.0\r\n  ],\r\n  \"layerSizes\": [\r\n    2,\r\n    2,\r\n    1\r\n  ],\r\n  \"activationFunction\": \"sigmoid\",\r\n  \"seed\": \"minimal\"\r\n}\r\n```\r\n\r\n## Code\r\n\r\n### Pseudocode\r\n\r\n```\r\nfunction forward(x, weights, biases, activation_fn):\r\n  a = x                              // input layer activations\r\n  for L = 1 to num_layers - 1:\r\n    z = weights[L] @ a + biases[L]   // linear transform\r\n    a = activation_fn(z)              // non-linear activation\r\n  return a                            // network output\r\n```\r\n\r\n### Python\r\n\r\n```python\r\nimport numpy as np\r\n\r\ndef forward(x, weights, biases, activation='sigmoid'):\r\n    a = np.array(x)\r\n    for W, b in zip(weights, biases):\r\n        z = W @ a + b\r\n        a = 1/(1+np.exp(-z)) if activation == 'sigmoid' else np.maximum(0, z)\r\n    return a\r\n```\r\n\r\n## Key Concepts\r\n\r\n### Layer-by-Layer Transformation\r\n\r\nEach layer takes the previous layer's output, applies Wa+b then activation.\r\n\r\n### Hidden Representations\r\n\r\nHidden layers learn internal representations of the input data.\r\n\r\n### Universal Approximation\r\n\r\nA network with at least one hidden layer can approximate any continuous function.\r\n\r\n## Common Pitfalls\r\n\r\n- **Depth vs Width:** More layers allow hierarchical features; more neurons increase per-layer capacity.\r\n\r\n## Quiz\r\n\r\n**Q1: A network has layer sizes [3, 4, 2]. How many weight parameters does it have (excluding biases)?**\r\n\r\n- A) 8\r\n- B) 12\r\n- C) 20\r\n- D) 24\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** C) 20\r\n\r\nLayer 01: 34 = 12 weights. Layer 12: 42 = 8 weights. Total: 20.\r\n\r\n\u003C/details>\r\n\r\n## Further Reading\r\n\r\n- [3Blue1Brown  Neural Networks series](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) (video)\r\n\r\n## Related Algorithms\r\n\r\n- [Backpropagation](/docs/algorithms/backpropagation/)\r\n- [Single Neuron / Perceptron](/docs/algorithms/perceptron/)\r\n\r\n## Prerequisites\r\n\r\n- [Single Neuron / Perceptron](/docs/algorithms/perceptron/)","src/content/docs/algorithms/feedforward-network.mdx","10fc8eaee6b12c1d","algorithms/gradient-descent",{"id":133,"data":135,"body":141,"filePath":142,"digest":143,"deferredRender":16},{"title":136,"description":137,"editUrl":16,"head":138,"template":50,"sidebar":139,"pagefind":16,"draft":39},"Gradient Descent","Optimization by following the steepest downhill direction on the loss landscape.",[],{"hidden":39,"attrs":140},{},"{/* This file is auto-generated by scripts/generate-algorithm-docs.py. */}\r\n{/* Do not edit manually  changes will be overwritten. */}\r\n{/* To update, modify the algorithm's meta.json and re-run the script. */}\r\n\r\n**Category:** Deep Learning  \r\n**Difficulty:** Intermediate  \r\n**Time Complexity:** `O(T  d)`  \r\n**Space Complexity:** `O(d)`\r\n\r\n## Overview\r\n\r\nGradient descent is the optimization algorithm that trains neural networks by iteratively adjusting parameters to minimize a loss function. At each step, it computes the gradient (vector of partial derivatives) of the loss with respect to the parameters, which points in the direction of steepest ascent. Moving parameters in the opposite direction (downhill) reduces the loss. The learning rate controls step size. Variants include SGD (stochastic gradient descent, which uses mini-batches), SGD with momentum (which accumulates velocity to overcome local plateaus), and Adam (which adapts the learning rate per parameter using first and second moment estimates). Gradient descent visualized on a 2D loss landscape shows the optimization trajectory converging toward a minimum.\r\n\r\n## Try It\r\n\r\n- **Web:** [Open in Eigenvue ](https://eigenvue.web.app/algo/gradient-descent)\r\n- **Python:**\r\n  ```python\r\n  import eigenvue\r\n  eigenvue.show(\"gradient-descent\")\r\n  ```\r\n\r\n## Default Inputs\r\n\r\n```json\r\n{\r\n  \"startX\": 3,\r\n  \"startY\": 3,\r\n  \"learningRate\": 0.1,\r\n  \"optimizer\": \"sgd\",\r\n  \"numSteps\": 20\r\n}\r\n```\r\n\r\n## Input Examples\r\n\r\n### Default (SGD)\r\n\r\n```json\r\n{\r\n  \"startX\": 3,\r\n  \"startY\": 3,\r\n  \"learningRate\": 0.1,\r\n  \"optimizer\": \"sgd\",\r\n  \"numSteps\": 20\r\n}\r\n```\r\n\r\n### Momentum optimizer\r\n\r\n```json\r\n{\r\n  \"startX\": -3,\r\n  \"startY\": 4,\r\n  \"learningRate\": 0.05,\r\n  \"optimizer\": \"momentum\",\r\n  \"numSteps\": 30\r\n}\r\n```\r\n\r\n### Adam optimizer\r\n\r\n```json\r\n{\r\n  \"startX\": 4,\r\n  \"startY\": -2,\r\n  \"learningRate\": 0.1,\r\n  \"optimizer\": \"adam\",\r\n  \"numSteps\": 25\r\n}\r\n```\r\n\r\n## Code\r\n\r\n### Pseudocode\r\n\r\n```\r\nfunction gradientDescent(f, startParams, learningRate, numSteps):\r\n  params = startParams\r\n  trajectory = [params]\r\n\r\n  for step in range(numSteps):\r\n    grad = computeGradient(f, params)  // f/params\r\n    params = params - learningRate * grad  // step downhill\r\n    trajectory.append(params)\r\n\r\n  return trajectory\r\n```\r\n\r\n## Key Concepts\r\n\r\n### Gradient as Direction of Steepest Ascent\r\n\r\nThe gradient f(x) is a vector pointing in the direction where the function increases fastest. To minimize, we move in the opposite direction: x_new = x_old -   f(x_old).\r\n\r\n### Learning Rate\r\n\r\nThe learning rate  controls how large each step is. Too large  overshooting and divergence. Too small  slow convergence. Finding the right learning rate is one of the most important hyperparameter choices in deep learning.\r\n\r\n### Momentum\r\n\r\nMomentum accumulates a velocity vector from past gradients, helping the optimizer build speed in consistent directions and dampen oscillations. It is like a ball rolling downhill that carries momentum past small bumps.\r\n\r\n### Adam Optimizer\r\n\r\nAdam (Adaptive Moment Estimation) adapts the learning rate for each parameter individually using running averages of the first moment (mean) and second moment (variance) of gradients. This makes it robust across a wide range of problems.\r\n\r\n## Common Pitfalls\r\n\r\n- **Local minima and saddle points:** Gradient descent can get stuck at local minima or saddle points where the gradient is zero. Momentum and Adam help escape these by maintaining velocity.\r\n- **Learning rate too high causes divergence:** If the learning rate is too large, the optimizer overshoots the minimum and the loss increases instead of decreasing, potentially diverging to infinity.\r\n\r\n## Quiz\r\n\r\n**Q1: In gradient descent, why do we subtract the gradient rather than add it?**\r\n\r\n- A) To increase the function value\r\n- B) Because the gradient points uphill and we want to go downhill\r\n- C) To normalize the parameters\r\n- D) Because subtraction is faster than addition\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) Because the gradient points uphill and we want to go downhill\r\n\r\nThe gradient points in the direction of steepest ascent. Since we want to minimize the loss, we move in the opposite direction by subtracting.\r\n\r\n\u003C/details>\r\n\r\n## Further Reading\r\n\r\n- [Adam: A Method for Stochastic Optimization (Kingma & Ba, 2015)](https://arxiv.org/abs/1412.6980) (paper)\r\n- [Gradient Descent  Wikipedia](https://en.wikipedia.org/wiki/Gradient_descent) (article)\r\n\r\n## Related Algorithms\r\n\r\n- [Backpropagation](/docs/algorithms/backpropagation/)\r\n- [Feedforward Neural Network](/docs/algorithms/feedforward-network/)\r\n\r\n## Prerequisites\r\n\r\n- [Backpropagation](/docs/algorithms/backpropagation/)","src/content/docs/algorithms/gradient-descent.mdx","7427c4c8e58ab769","algorithms",{"id":144,"data":146,"body":152,"filePath":153,"digest":154,"deferredRender":16},{"title":147,"description":148,"editUrl":16,"head":149,"template":50,"sidebar":150,"pagefind":16,"draft":39},"Algorithm Reference","Complete reference for all 17 algorithms available in Eigenvue, organized by category.",[],{"hidden":39,"attrs":151},{},"Eigenvue includes **17 algorithms** across three categories: Classical Algorithms, Deep Learning, and Generative AI. Each algorithm provides step-by-step visual explanations, interactive playback controls, and shareable deep links. Click any algorithm name below to view its detailed documentation, including input options, step format, and educational content.\n\n## Classical Algorithms\n\nFoundational algorithms for searching, sorting, and graph traversal. These form the basis of computer science education and are essential building blocks for more advanced topics.\n\n| Algorithm | Difficulty | Time Complexity | Space Complexity |\n|-----------|------------|-----------------|------------------|\n| [Binary Search](/docs/algorithms/binary-search/) | Beginner | O(log n) | O(1) |\n| [Bubble Sort](/docs/algorithms/bubble-sort/) | Beginner | O(n) | O(1) |\n| [QuickSort](/docs/algorithms/quicksort/) | Intermediate | O(n log n) avg | O(log n) |\n| [Merge Sort](/docs/algorithms/merge-sort/) | Intermediate | O(n log n) | O(n) |\n| [BFS](/docs/algorithms/bfs/) | Intermediate | O(V + E) | O(V) |\n| [DFS](/docs/algorithms/dfs/) | Intermediate | O(V + E) | O(V) |\n| [Dijkstra's](/docs/algorithms/dijkstra/) | Advanced | O((V + E) log V) | O(V) |\n\n## Deep Learning\n\nCore building blocks of neural networks, from single neurons to gradient-based optimization. These visualizations help build intuition for how deep learning models learn.\n\n| Algorithm | Difficulty | Key Concept |\n|-----------|------------|-------------|\n| [Perceptron](/docs/algorithms/perceptron/) | Beginner | Single neuron, activation functions |\n| [Feedforward Network](/docs/algorithms/feedforward-network/) | Intermediate | Multi-layer signal propagation |\n| [Backpropagation](/docs/algorithms/backpropagation/) | Intermediate | Gradient computation, chain rule |\n| [Convolution](/docs/algorithms/convolution/) | Intermediate | Kernel sliding, feature maps |\n| [Gradient Descent](/docs/algorithms/gradient-descent/) | Intermediate | Optimization, loss landscapes |\n\n## Generative AI\n\nComponents of modern generative AI architectures, focusing on the transformer pipeline from raw text to contextualized representations.\n\n| Algorithm | Difficulty | Key Concept |\n|-----------|------------|-------------|\n| [Tokenization (BPE)](/docs/algorithms/tokenization-bpe/) | Beginner | Byte-pair encoding, vocabulary |\n| [Token Embeddings](/docs/algorithms/token-embeddings/) | Intermediate | Vector representations, similarity |\n| [Self-Attention](/docs/algorithms/self-attention/) | Intermediate | Q/K/V projections, attention weights |\n| [Multi-Head Attention](/docs/algorithms/multi-head-attention/) | Advanced | Multiple heads, concatenation |\n| [Transformer Block](/docs/algorithms/transformer-block/) | Advanced | Full block: attention + FFN + residual |","src/content/docs/algorithms/index.mdx","9bd29cf03523d26e","algorithms/merge-sort",{"id":155,"data":157,"body":163,"filePath":164,"digest":165,"deferredRender":16},{"title":158,"description":159,"editUrl":16,"head":160,"template":50,"sidebar":161,"pagefind":16,"draft":39},"Merge Sort","Stable divide-and-conquer sort that merges sorted sub-arrays.",[],{"hidden":39,"attrs":162},{},"{/* This file is auto-generated by scripts/generate-algorithm-docs.py. */}\r\n{/* Do not edit manually  changes will be overwritten. */}\r\n{/* To update, modify the algorithm's meta.json and re-run the script. */}\r\n\r\n**Category:** Classical  \r\n**Difficulty:** Intermediate  \r\n**Time Complexity:** `O(n log n)`  \r\n**Space Complexity:** `O(n)`\r\n\r\n## Overview\r\n\r\nMerge Sort divides the array into halves recursively until each sub-array has one element, then merges them back in sorted order. It guarantees O(n log n) time in all cases but requires O(n) extra space for the merge buffer.\r\n\r\n## Try It\r\n\r\n- **Web:** [Open in Eigenvue ](https://eigenvue.web.app/algo/merge-sort)\r\n- **Python:**\r\n  ```python\r\n  import eigenvue\r\n  eigenvue.show(\"merge-sort\")\r\n  ```\r\n\r\n## Default Inputs\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    38,\r\n    27,\r\n    43,\r\n    3,\r\n    9,\r\n    82,\r\n    10\r\n  ]\r\n}\r\n```\r\n\r\n## Input Examples\r\n\r\n### Default unsorted\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    38,\r\n    27,\r\n    43,\r\n    3,\r\n    9,\r\n    82,\r\n    10\r\n  ]\r\n}\r\n```\r\n\r\n### Power of two (8 elements)\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    8,\r\n    3,\r\n    6,\r\n    1,\r\n    5,\r\n    2,\r\n    7,\r\n    4\r\n  ]\r\n}\r\n```\r\n\r\n### Already sorted\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    1,\r\n    2,\r\n    3,\r\n    4,\r\n    5,\r\n    6\r\n  ]\r\n}\r\n```\r\n\r\n### Single element\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    42\r\n  ]\r\n}\r\n```\r\n\r\n## Code\r\n\r\n### Pseudocode\r\n\r\n```\r\nfunction mergeSort(array):\r\n  if length(array) \u003C= 1:\r\n    return array\r\n  for size = 1, 2, 4, ... while size \u003C n:\r\n    for left = 0, 2*size, 4*size, ...:\r\n      mid = min(left + size - 1, n - 1)\r\n      right = min(left + 2*size - 1, n - 1)\r\n      merge(array, left, mid, right)\r\n\r\nfunction merge(array, left, mid, right):\r\n  aux = copy of array[left..right]\r\n  i = left, j = mid + 1, k = left\r\n  while i \u003C= mid and j \u003C= right:\r\n    if aux[i] \u003C= aux[j]: array[k] = aux[i]; i++\r\n    else: array[k] = aux[j]; j++\r\n    k++\r\n  copy remaining elements\r\n```\r\n\r\n### Python\r\n\r\n```python\r\ndef merge_sort(arr):\r\n    n = len(arr)\r\n    size = 1\r\n    while size \u003C n:\r\n        for left in range(0, n, 2 * size):\r\n            mid = min(left + size - 1, n - 1)\r\n            right = min(left + 2 * size - 1, n - 1)\r\n            merge(arr, left, mid, right)\r\n        size *= 2\r\n\r\ndef merge(arr, left, mid, right):\r\n    temp = arr[left:right + 1]\r\n    i, j, k = 0, mid - left + 1, left\r\n    while i \u003C= mid - left and j \u003C len(temp):\r\n        if temp[i] \u003C= temp[j]:\r\n            arr[k] = temp[i]; i += 1\r\n        else:\r\n            arr[k] = temp[j]; j += 1\r\n        k += 1\r\n    while i \u003C= mid - left:\r\n        arr[k] = temp[i]; i += 1; k += 1\r\n    while j \u003C len(temp):\r\n        arr[k] = temp[j]; j += 1; k += 1\r\n```\r\n\r\n## Key Concepts\r\n\r\n### Divide and Conquer\r\n\r\nMerge Sort divides the problem into smaller sub-problems (halving), solves them, and combines (merges) the results.\r\n\r\n### Stability\r\n\r\nWhen equal elements appear, we always take from the left half first, preserving their original relative order.\r\n\r\n### Guaranteed O(n log n)\r\n\r\nUnlike QuickSort, Merge Sort always runs in O(n log n) regardless of input order. The trade-off is O(n) extra space.\r\n\r\n### Auxiliary Space\r\n\r\nThe merge step requires a temporary buffer of size n. This is the main disadvantage compared to in-place sorting algorithms.\r\n\r\n## Common Pitfalls\r\n\r\n- **Forgetting to copy back:** After merging into the auxiliary buffer, the values must be copied back into the main array. Forgetting this means subsequent merges operate on stale data.\r\n- **Off-by-one in merge boundaries:** The mid index belongs to the LEFT sub-array [left..mid], and the right starts at mid+1. Getting this wrong duplicates or skips elements.\r\n\r\n## Quiz\r\n\r\n**Q1: What is the space complexity of Merge Sort?**\r\n\r\n- A) O(1)\r\n- B) O(log n)\r\n- C) O(n)\r\n- D) O(n log n)\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** C) O(n)\r\n\r\nMerge Sort needs O(n) auxiliary space for the merge buffer, plus O(log n) for the recursion stack. The dominant term is O(n).\r\n\r\n\u003C/details>\r\n\r\n**Q2: Is Merge Sort stable?**\r\n\r\n- A) Yes\r\n- B) No\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** A) Yes\r\n\r\nYes. When two elements are equal, the merge takes from the left sub-array first, preserving original relative order.\r\n\r\n\u003C/details>\r\n\r\n## Further Reading\r\n\r\n- [Wikipedia: Merge Sort](https://en.wikipedia.org/wiki/Merge_sort) (article)\r\n\r\n## Related Algorithms\r\n\r\n- [Bubble Sort](/docs/algorithms/bubble-sort/)\r\n- [QuickSort](/docs/algorithms/quicksort/)\r\n\r\n## Prerequisites\r\n\r\n- [Binary Search](/docs/algorithms/binary-search/)","src/content/docs/algorithms/merge-sort.mdx","fd5315be4d344b5e","algorithms/multi-head-attention",{"id":166,"data":168,"body":174,"filePath":175,"digest":176,"deferredRender":16},{"title":169,"description":170,"editUrl":16,"head":171,"template":50,"sidebar":172,"pagefind":16,"draft":39},"Multi-Head Attention","See how multiple attention heads capture different relationships simultaneously.",[],{"hidden":39,"attrs":173},{},"{/* This file is auto-generated by scripts/generate-algorithm-docs.py. */}\r\n{/* Do not edit manually  changes will be overwritten. */}\r\n{/* To update, modify the algorithm's meta.json and re-run the script. */}\r\n\r\n**Category:** Generative AI  \r\n**Difficulty:** Advanced  \r\n**Time Complexity:** `O(h  n  d_k)`  \r\n**Space Complexity:** `O(h  n + n  d)`\r\n\r\n## Overview\r\n\r\nMulti-Head Attention is a core mechanism in the Transformer architecture that runs several attention functions in parallel. Instead of computing a single attention pass with d_model-dimensional keys, queries, and values, the model splits them into h heads, each operating on d_k = d_model / h dimensions. Each head learns to attend to different aspects of the input  one head might focus on syntactic relationships while another captures semantic similarity. After all heads compute their attention independently, their outputs are concatenated back into a d_model-dimensional vector and passed through a final linear projection W_O. This projection allows the model to mix information across heads. The key insight is that multiple smaller attention operations are more expressive than a single large one, enabling the model to jointly attend to information from different representation subspaces at different positions.\r\n\r\n## Try It\r\n\r\n- **Web:** [Open in Eigenvue ](https://eigenvue.web.app/algo/multi-head-attention)\r\n- **Python:**\r\n  ```python\r\n  import eigenvue\r\n  eigenvue.show(\"multi-head-attention\")\r\n  ```\r\n\r\n## Default Inputs\r\n\r\n```json\r\n{\r\n  \"tokens\": [\r\n    \"The\",\r\n    \"cat\",\r\n    \"sat\",\r\n    \"on\"\r\n  ],\r\n  \"embeddingDim\": 8,\r\n  \"numHeads\": 2\r\n}\r\n```\r\n\r\n## Input Examples\r\n\r\n### 2 heads, dim=8\r\n\r\n```json\r\n{\r\n  \"tokens\": [\r\n    \"The\",\r\n    \"cat\",\r\n    \"sat\",\r\n    \"on\"\r\n  ],\r\n  \"embeddingDim\": 8,\r\n  \"numHeads\": 2\r\n}\r\n```\r\n\r\n### 4 heads, dim=8\r\n\r\n```json\r\n{\r\n  \"tokens\": [\r\n    \"I\",\r\n    \"love\",\r\n    \"deep\",\r\n    \"learning\"\r\n  ],\r\n  \"embeddingDim\": 8,\r\n  \"numHeads\": 4\r\n}\r\n```\r\n\r\n## Code\r\n\r\n### Pseudocode\r\n\r\n```\r\nfunction multiHeadAttention(X, numHeads):\r\n  d_k = d_model / numHeads\r\n  for h = 1 to numHeads:\r\n    Q_h = X  W_Q_h    // [n, d_k]\r\n    K_h = X  W_K_h    // [n, d_k]\r\n    V_h = X  W_V_h    // [n, d_k]\r\n    scores_h = (Q_h  K_h) / d_k\r\n    weights_h = softmax(scores_h)  // row-wise\r\n    head_h = weights_h  V_h\r\n  concat = [head_1 ; head_2 ; ... ; head_h]  // [n, d_model]\r\n  output = concat  W_O\r\n  return output\r\n```\r\n\r\n### Python\r\n\r\n```python\r\ndef multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads):\r\n    d_model = X.shape[-1]\r\n    d_k = d_model // num_heads\r\n    heads = []\r\n    for h in range(num_heads):\r\n        Q_h = X @ W_Q[h]  # [n, d_k]\r\n        K_h = X @ W_K[h]  # [n, d_k]\r\n        V_h = X @ W_V[h]  # [n, d_k]\r\n        scores = (Q_h @ K_h.T) / math.sqrt(d_k)\r\n        weights = softmax(scores, axis=-1)\r\n        heads.append(weights @ V_h)\r\n    concat = np.concatenate(heads, axis=-1)  # [n, d_model]\r\n    return concat @ W_O\r\n```\r\n\r\n### JavaScript\r\n\r\n```javascript\r\nfunction multiHeadAttention(X, numHeads, W_Qs, W_Ks, W_Vs, W_O) {\r\n  const d_k = X[0].length / numHeads;\r\n  const heads = [];\r\n  for (let h = 0; h \u003C numHeads; h++) {\r\n    const Q = matMul(X, W_Qs[h]);\r\n    const K = matMul(X, W_Ks[h]);\r\n    const V = matMul(X, W_Vs[h]);\r\n    const scores = scale(matMul(Q, transpose(K)), 1 / Math.sqrt(d_k));\r\n    const weights = softmaxRows(scores);\r\n    heads.push(matMul(weights, V));\r\n  }\r\n  const concat = concatenate(heads); // [n, d_model]\r\n  return matMul(concat, W_O);\r\n}\r\n```\r\n\r\n## Key Concepts\r\n\r\n### Why Multiple Heads?\r\n\r\nA single attention head can only learn one type of relationship between tokens. Multiple heads allow the model to simultaneously attend to different aspects  for example, one head might learn syntactic dependencies (subject-verb agreement) while another learns semantic relationships (word meaning similarity). This is analogous to having multiple 'perspectives' on the same data.\r\n\r\n### d_k vs d_model\r\n\r\nEach head operates on d_k = d_model / numHeads dimensions, NOT the full d_model. This means the total computation is roughly the same as single-head attention with full dimensionality, but the model gains the expressiveness of multiple independent attention patterns. The scaling factor in each head uses d_k, not d_model.\r\n\r\n### Concatenation and W_O Projection\r\n\r\nAfter all heads compute their outputs (each of shape [n, d_k]), they are concatenated along the feature dimension to form a [n, d_model] matrix. The final W_O projection (shape [d_model, d_model]) mixes information across heads, allowing the model to combine the different perspectives into a unified representation.\r\n\r\n## Common Pitfalls\r\n\r\n- **d_model must be divisible by numHeads:** If d_model is not evenly divisible by numHeads, the dimension split is impossible. For example, d_model=7 with numHeads=2 fails because 7/2 = 3.5 is not an integer. Always choose numHeads to be a factor of d_model.\r\n- **Scaling factor uses d_k, not d_model:** A common mistake is scaling by d_model instead of d_k. Each head's attention scores are dot products of d_k-dimensional vectors, so the expected magnitude scales with d_k. Using d_model would under-scale the scores, leading to overly uniform attention weights.\r\n- **Head outputs must be concatenated, not summed:** The outputs from different heads are concatenated along the feature axis, not summed or averaged. Summing would lose information and reduce the effective dimensionality. Concatenation preserves all head outputs and lets W_O learn how to combine them.\r\n\r\n## Quiz\r\n\r\n**Q1: If d_model = 12 and numHeads = 3, what is d_k?**\r\n\r\n- A) 12\r\n- B) 3\r\n- C) 4\r\n- D) 36\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** C) 4\r\n\r\nd_k = d_model / numHeads = 12 / 3 = 4. Each head operates on 4-dimensional queries, keys, and values.\r\n\r\n\u003C/details>\r\n\r\n**Q2: Why is multi-head attention more expressive than single-head attention?**\r\n\r\n- A) It uses more total parameters\r\n- B) Each head can learn different attention patterns in different subspaces\r\n- C) It is computationally faster\r\n- D) It uses a larger scaling factor\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) Each head can learn different attention patterns in different subspaces\r\n\r\nMultiple heads allow the model to jointly attend to information from different representation subspaces. Each head can specialize in different types of relationships (syntactic, semantic, positional, etc.).\r\n\r\n\u003C/details>\r\n\r\n**Q3: What is the shape of the concatenated output before the W_O projection?**\r\n\r\n- A) [seqLen, d_k]\r\n- B) [seqLen, d_model]\r\n- C) [numHeads, seqLen, d_k]\r\n- D) [seqLen, numHeads]\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) [seqLen, d_model]\r\n\r\nEach head produces [seqLen, d_k]. Concatenating numHeads outputs gives [seqLen, numHeads  d_k] = [seqLen, d_model], since d_k = d_model / numHeads.\r\n\r\n\u003C/details>\r\n\r\n## Further Reading\r\n\r\n- [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762) (article)\r\n- [The Illustrated Transformer  Jay Alammar](https://jalammar.github.io/illustrated-transformer/) (article)\r\n- [Multi-Head Attention  Wikipedia](https://en.wikipedia.org/wiki/Attention_(machine_learning)#Multi-head_attention) (reference)\r\n\r\n## Related Algorithms\r\n\r\n- [Self-Attention (Scaled Dot-Product)](/docs/algorithms/self-attention/)\r\n- [Transformer Block](/docs/algorithms/transformer-block/)\r\n\r\n## Prerequisites\r\n\r\n- [Self-Attention (Scaled Dot-Product)](/docs/algorithms/self-attention/)","src/content/docs/algorithms/multi-head-attention.mdx","8fb3981d412116fb","algorithms/perceptron",{"id":177,"data":179,"body":185,"filePath":186,"digest":187,"deferredRender":16},{"title":180,"description":181,"editUrl":16,"head":182,"template":50,"sidebar":183,"pagefind":16,"draft":39},"Single Neuron / Perceptron","Watch a single neuron compute: inputs  weights + bias  activation  output.",[],{"hidden":39,"attrs":184},{},"{/* This file is auto-generated by scripts/generate-algorithm-docs.py. */}\r\n{/* Do not edit manually  changes will be overwritten. */}\r\n{/* To update, modify the algorithm's meta.json and re-run the script. */}\r\n\r\n**Category:** Deep Learning  \r\n**Difficulty:** Beginner  \r\n**Time Complexity:** `O(n)`  \r\n**Space Complexity:** `O(1)`\r\n\r\n## Overview\r\n\r\nThe perceptron is the fundamental building block of neural networks. A single neuron takes multiple inputs, multiplies each by a learned weight, sums the results, adds a bias term, and passes the total through an activation function. This visualization walks through each computation step, showing how inputs are transformed into an output, and how changing weights and bias affects the decision.\r\n\r\n## Try It\r\n\r\n- **Web:** [Open in Eigenvue ](https://eigenvue.web.app/algo/perceptron)\r\n- **Python:**\r\n  ```python\r\n  import eigenvue\r\n  eigenvue.show(\"perceptron\")\r\n  ```\r\n\r\n## Default Inputs\r\n\r\n```json\r\n{\r\n  \"inputs\": [\r\n    0.5,\r\n    0.8\r\n  ],\r\n  \"weights\": [\r\n    0.6,\r\n    -0.3\r\n  ],\r\n  \"bias\": 0.1,\r\n  \"activationFunction\": \"sigmoid\"\r\n}\r\n```\r\n\r\n## Input Examples\r\n\r\n### AND gate (step function)\r\n\r\n```json\r\n{\r\n  \"inputs\": [\r\n    1,\r\n    1\r\n  ],\r\n  \"weights\": [\r\n    0.5,\r\n    0.5\r\n  ],\r\n  \"bias\": -0.7,\r\n  \"activationFunction\": \"step\"\r\n}\r\n```\r\n\r\n### OR gate (step function)\r\n\r\n```json\r\n{\r\n  \"inputs\": [\r\n    1,\r\n    0\r\n  ],\r\n  \"weights\": [\r\n    0.5,\r\n    0.5\r\n  ],\r\n  \"bias\": -0.2,\r\n  \"activationFunction\": \"step\"\r\n}\r\n```\r\n\r\n### Three inputs (sigmoid)\r\n\r\n```json\r\n{\r\n  \"inputs\": [\r\n    0.3,\r\n    0.7,\r\n    0.5\r\n  ],\r\n  \"weights\": [\r\n    0.4,\r\n    -0.2,\r\n    0.8\r\n  ],\r\n  \"bias\": -0.1,\r\n  \"activationFunction\": \"sigmoid\"\r\n}\r\n```\r\n\r\n## Code\r\n\r\n### Pseudocode\r\n\r\n```\r\nfunction perceptron(inputs, weights, bias, activation_fn):\r\n  z = 0\r\n  for i in range(len(inputs)):\r\n    z += inputs[i] * weights[i]    // weighted sum\r\n  z += bias                         // add bias\r\n  output = activation_fn(z)          // apply activation\r\n  return output\r\n```\r\n\r\n### Python\r\n\r\n```python\r\nimport numpy as np\r\n\r\ndef perceptron(x, w, b, activation='sigmoid'):\r\n    z = np.dot(x, w) + b\r\n    if activation == 'sigmoid':\r\n        return 1 / (1 + np.exp(-z))\r\n    elif activation == 'relu':\r\n        return max(0, z)\r\n    elif activation == 'step':\r\n        return 1 if z >= 0 else 0\r\n```\r\n\r\n### JavaScript\r\n\r\n```javascript\r\nfunction perceptron(inputs, weights, bias, activation) {\r\n  const z = inputs.reduce((sum, x, i) => sum + x * weights[i], 0) + bias;\r\n  if (activation === 'sigmoid') return 1 / (1 + Math.exp(-z));\r\n  if (activation === 'relu') return Math.max(0, z);\r\n  if (activation === 'step') return z >= 0 ? 1 : 0;\r\n}\r\n```\r\n\r\n## Key Concepts\r\n\r\n### Weighted Sum\r\n\r\nEach input is multiplied by its weight. The weight controls how much influence that input has.\r\n\r\n### Bias Term\r\n\r\nThe bias shifts the activation threshold. It allows the neuron to fire even when all inputs are zero.\r\n\r\n### Activation Function\r\n\r\nWithout activation, a neuron is just a linear function. Activation functions introduce non-linearity for learning complex patterns.\r\n\r\n### The Perceptron's Limitation\r\n\r\nA single perceptron can only learn linearly separable patterns. It can learn AND and OR but NOT XOR.\r\n\r\n## Common Pitfalls\r\n\r\n- **Weight Initialization Matters:** If all weights start at zero, all neurons compute the same thing.\r\n- **Sigmoid Saturation:** When |z| is large, sigmoid derivative is nearly 0  the vanishing gradient problem.\r\n\r\n## Quiz\r\n\r\n**Q1: A neuron has inputs [1, 0], weights [0.5, 0.5], and bias -0.7. Using a step activation function, what is the output?**\r\n\r\n- A) 0\r\n- B) 1\r\n- C) 0.5\r\n- D) -0.2\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** A) 0\r\n\r\nz = 10.5 + 00.5 + (-0.7) = -0.2. Since z &lt; 0, step(-0.2) = 0.\r\n\r\n\u003C/details>\r\n\r\n**Q2: Why can't a single perceptron learn XOR?**\r\n\r\n- A) XOR requires more than 2 inputs\r\n- B) XOR is not a linear function  no single line can separate the outputs\r\n- C) The step function can't handle XOR\r\n- D) XOR requires negative weights\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) XOR is not a linear function  no single line can separate the outputs\r\n\r\nXOR outputs 1 for (0,1) and (1,0) but 0 for (0,0) and (1,1). No single straight line can separate these two classes.\r\n\r\n\u003C/details>\r\n\r\n## Further Reading\r\n\r\n- [Rosenblatt 1958  The Perceptron](https://psycnet.apa.org/doi/10.1037/h0042519) (paper)\r\n- [3Blue1Brown  But what is a neural network?](https://www.youtube.com/watch?v=aircAruvnKk) (video)\r\n\r\n## Related Algorithms\r\n\r\n- [Feedforward Neural Network](/docs/algorithms/feedforward-network/)\r\n- [Backpropagation](/docs/algorithms/backpropagation/)","src/content/docs/algorithms/perceptron.mdx","19806bc4d130d3ac","algorithms/quicksort",{"id":188,"data":190,"body":196,"filePath":197,"digest":198,"deferredRender":16},{"title":191,"description":192,"editUrl":16,"head":193,"template":50,"sidebar":194,"pagefind":16,"draft":39},"QuickSort","Divide-and-conquer sort using pivot partitioning.",[],{"hidden":39,"attrs":195},{},"{/* This file is auto-generated by scripts/generate-algorithm-docs.py. */}\r\n{/* Do not edit manually  changes will be overwritten. */}\r\n{/* To update, modify the algorithm's meta.json and re-run the script. */}\r\n\r\n**Category:** Classical  \r\n**Difficulty:** Intermediate  \r\n**Time Complexity:** `O(n log n)`  \r\n**Space Complexity:** `O(log n)`\r\n\r\n## Overview\r\n\r\nQuickSort selects a pivot element, partitions the array so all elements less than the pivot come before it and all greater come after, then recursively sorts the sub-arrays. Average-case O(n log n) but worst-case O(n) when the pivot is poorly chosen.\r\n\r\n## Try It\r\n\r\n- **Web:** [Open in Eigenvue ](https://eigenvue.web.app/algo/quicksort)\r\n- **Python:**\r\n  ```python\r\n  import eigenvue\r\n  eigenvue.show(\"quicksort\")\r\n  ```\r\n\r\n## Default Inputs\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    38,\r\n    27,\r\n    43,\r\n    3,\r\n    9,\r\n    82,\r\n    10\r\n  ]\r\n}\r\n```\r\n\r\n## Input Examples\r\n\r\n### Default unsorted\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    38,\r\n    27,\r\n    43,\r\n    3,\r\n    9,\r\n    82,\r\n    10\r\n  ]\r\n}\r\n```\r\n\r\n### Already sorted (worst case)\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    1,\r\n    2,\r\n    3,\r\n    4,\r\n    5,\r\n    6,\r\n    7,\r\n    8\r\n  ]\r\n}\r\n```\r\n\r\n### With duplicates\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    5,\r\n    3,\r\n    8,\r\n    3,\r\n    2,\r\n    5,\r\n    1\r\n  ]\r\n}\r\n```\r\n\r\n### Reverse sorted\r\n\r\n```json\r\n{\r\n  \"array\": [\r\n    8,\r\n    7,\r\n    6,\r\n    5,\r\n    4,\r\n    3,\r\n    2,\r\n    1\r\n  ]\r\n}\r\n```\r\n\r\n## Code\r\n\r\n### Pseudocode\r\n\r\n```\r\nfunction quickSort(array, low, high):\r\n  if low \u003C high:\r\n    pivotIdx = partition(array, low, high)\r\n    quickSort(array, low, pivotIdx - 1)\r\n    quickSort(array, pivotIdx + 1, high)\r\n\r\nfunction partition(array, low, high):\r\n  pivot = array[high]\r\n  i = low - 1\r\n  for j = low to high - 1:\r\n    if array[j] \u003C= pivot:\r\n      i++\r\n      swap(array[i], array[j])\r\n  swap(array[i + 1], array[high])\r\n  return i + 1\r\n```\r\n\r\n### Python\r\n\r\n```python\r\ndef quicksort(arr, low=0, high=None):\r\n    if high is None:\r\n        high = len(arr) - 1\r\n    if low \u003C high:\r\n        pi = partition(arr, low, high)\r\n        quicksort(arr, low, pi - 1)\r\n        quicksort(arr, pi + 1, high)\r\n\r\ndef partition(arr, low, high):\r\n    pivot = arr[high]\r\n    i = low - 1\r\n    for j in range(low, high):\r\n        if arr[j] \u003C= pivot:\r\n            i += 1\r\n            arr[i], arr[j] = arr[j], arr[i]\r\n    arr[i + 1], arr[high] = arr[high], arr[i + 1]\r\n    return i + 1\r\n```\r\n\r\n### JavaScript\r\n\r\n```javascript\r\nfunction quickSort(arr, low = 0, high = arr.length - 1) {\r\n  if (low \u003C high) {\r\n    const pi = partition(arr, low, high);\r\n    quickSort(arr, low, pi - 1);\r\n    quickSort(arr, pi + 1, high);\r\n  }\r\n}\r\n\r\nfunction partition(arr, low, high) {\r\n  const pivot = arr[high];\r\n  let i = low - 1;\r\n  for (let j = low; j \u003C high; j++) {\r\n    if (arr[j] \u003C= pivot) {\r\n      i++;\r\n      [arr[i], arr[j]] = [arr[j], arr[i]];\r\n    }\r\n  }\r\n  [arr[i + 1], arr[high]] = [arr[high], arr[i + 1]];\r\n  return i + 1;\r\n}\r\n```\r\n\r\n## Key Concepts\r\n\r\n### Divide and Conquer\r\n\r\nQuickSort divides the problem by partitioning around a pivot, conquers by recursively sorting sub-arrays, and combines trivially (the array is sorted in-place).\r\n\r\n### Lomuto Partition\r\n\r\nThe Lomuto scheme picks the last element as pivot and maintains a boundary i: everything at or before i is  pivot. Simple to implement but O(n) on already-sorted input.\r\n\r\n### In-Place Sorting\r\n\r\nQuickSort sorts in-place using O(log n) stack space for recursion. No auxiliary array is needed (unlike Merge Sort).\r\n\r\n### Not Stable\r\n\r\nQuickSort is not stable  the partition step can change the relative order of equal elements.\r\n\r\n## Common Pitfalls\r\n\r\n- **Worst-case O(n):** With Lomuto partition and already-sorted input, each partition removes only one element, leading to O(n). Randomized pivot or median-of-three avoids this in practice.\r\n- **Off-by-one in partition:** The boundary i starts at low - 1 (NOT low). The pivot swap goes to i + 1 (NOT i). Getting these wrong corrupts the invariant.\r\n\r\n## Quiz\r\n\r\n**Q1: What is the worst-case input for QuickSort with last-element pivot?**\r\n\r\n- A) Random array\r\n- B) Already sorted array\r\n- C) Array with all equal elements\r\n- D) Both B and C\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** D) Both B and C\r\n\r\nBoth already-sorted and all-equal arrays cause the worst case with Lomuto partition. Each partition puts all elements on one side, giving n partitions of n-1 elements each  O(n).\r\n\r\n\u003C/details>\r\n\r\n**Q2: Is QuickSort stable?**\r\n\r\n- A) Yes\r\n- B) No\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) No\r\n\r\nNo. The partition step swaps non-adjacent elements, which can change the relative order of equal elements.\r\n\r\n\u003C/details>\r\n\r\n## Further Reading\r\n\r\n- [Wikipedia: QuickSort](https://en.wikipedia.org/wiki/Quicksort) (article)\r\n\r\n## Related Algorithms\r\n\r\n- [Bubble Sort](/docs/algorithms/bubble-sort/)\r\n- [Merge Sort](/docs/algorithms/merge-sort/)\r\n\r\n## Prerequisites\r\n\r\n- [Binary Search](/docs/algorithms/binary-search/)","src/content/docs/algorithms/quicksort.mdx","5e8e171b5d238903","algorithms/token-embeddings",{"id":199,"data":201,"body":207,"filePath":208,"digest":209,"deferredRender":16},{"title":202,"description":203,"editUrl":16,"head":204,"template":50,"sidebar":205,"pagefind":16,"draft":39},"Token Embeddings","Map tokens to dense numerical vectors using an embedding lookup table.",[],{"hidden":39,"attrs":206},{},"{/* This file is auto-generated by scripts/generate-algorithm-docs.py. */}\r\n{/* Do not edit manually  changes will be overwritten. */}\r\n{/* To update, modify the algorithm's meta.json and re-run the script. */}\r\n\r\n**Category:** Generative AI  \r\n**Difficulty:** Intermediate  \r\n**Time Complexity:** `O(n  d)`  \r\n**Space Complexity:** `O(V  d)`\r\n\r\n## Overview\r\n\r\nToken embeddings are the first step in any transformer-based language model. Each token in the vocabulary is assigned a dense vector of real numbers  its embedding. These vectors are learned during training so that semantically similar tokens end up with similar vectors (high cosine similarity). This visualization shows the embedding lookup process step by step: given a sequence of tokens, each token is mapped to its embedding vector, and pairwise cosine similarities are computed to illustrate the geometric relationships between tokens in the embedding space.\r\n\r\n## Try It\r\n\r\n- **Web:** [Open in Eigenvue ](https://eigenvue.web.app/algo/token-embeddings)\r\n- **Python:**\r\n  ```python\r\n  import eigenvue\r\n  eigenvue.show(\"token-embeddings\")\r\n  ```\r\n\r\n## Default Inputs\r\n\r\n```json\r\n{\r\n  \"tokens\": [\r\n    \"The\",\r\n    \"cat\",\r\n    \"sat\"\r\n  ],\r\n  \"embeddingDim\": 4\r\n}\r\n```\r\n\r\n## Input Examples\r\n\r\n### Default  3 tokens, dim 4\r\n\r\n```json\r\n{\r\n  \"tokens\": [\r\n    \"The\",\r\n    \"cat\",\r\n    \"sat\"\r\n  ],\r\n  \"embeddingDim\": 4\r\n}\r\n```\r\n\r\n### Similar words  4 tokens, dim 6\r\n\r\n```json\r\n{\r\n  \"tokens\": [\r\n    \"king\",\r\n    \"queen\",\r\n    \"man\",\r\n    \"woman\"\r\n  ],\r\n  \"embeddingDim\": 6\r\n}\r\n```\r\n\r\n## Code\r\n\r\n### Pseudocode\r\n\r\n```\r\nfunction embedTokens(tokens, embeddingTable, d):\r\n  embeddings = empty matrix [len(tokens), d]\r\n  for i = 0 to len(tokens) - 1:\r\n    tokenId = lookupId(tokens[i])\r\n    embeddings[i] = embeddingTable[tokenId]   // d-dimensional vector\r\n  return embeddings\r\n\r\n// Cosine similarity: cos() = (A  B) / (A  B)\r\n```\r\n\r\n### Python\r\n\r\n```python\r\nimport numpy as np\r\n\r\ndef embed_tokens(tokens: list[str], embedding_table: np.ndarray, token_to_id: dict[str, int]) -> np.ndarray:\r\n    \"\"\"Look up embeddings for a list of tokens.\"\"\"\r\n    ids = [token_to_id[t] for t in tokens]\r\n    return embedding_table[ids]  # shape: [len(tokens), d]\r\n\r\ndef cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\r\n    \"\"\"Compute cosine similarity between two vectors.\"\"\"\r\n    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\r\n```\r\n\r\n## Key Concepts\r\n\r\n### Embedding as Lookup\r\n\r\nAn embedding layer is essentially a lookup table. Each token ID maps to a row in the embedding matrix. There is no computation involved  just an index-based retrieval of a pre-stored vector. The embedding matrix has shape [V, d] where V is the vocabulary size and d is the embedding dimension.\r\n\r\n### Cosine Similarity\r\n\r\nCosine similarity measures the angle between two vectors, ignoring their magnitude. It is computed as cos() = (A  B) / (A  B) and ranges from -1 (opposite directions) to +1 (same direction). In embedding space, high cosine similarity between two token vectors suggests semantic relatedness.\r\n\r\n### Embedding Dimensions\r\n\r\nThe embedding dimension d determines the expressiveness of token representations. Small models use d=64 or d=128, while GPT-3 uses d=12288. Higher dimensions can capture more nuanced semantic relationships but require more memory and computation. The choice of d is a key architectural hyperparameter.\r\n\r\n### Learned Representations\r\n\r\nEmbedding vectors are not hand-crafted  they are learned during model training via backpropagation. The training process adjusts vectors so that tokens appearing in similar contexts end up with similar embeddings. This is the distributional hypothesis: 'a word is characterized by the company it keeps.'\r\n\r\n## Common Pitfalls\r\n\r\n- **Embeddings are learned, not fixed:** A common misconception is that embeddings are predetermined or based on character similarity. In reality, the embedding for each token is learned during training. The token 'cat' and 'cat' (same string) will always have the same embedding, but 'cat' and 'bat' may have very different embeddings despite differing by one character.\r\n- **Confusing embedding dimension with vocabulary size:** The embedding matrix has shape [V, d] where V is the vocabulary size (number of unique tokens) and d is the embedding dimension (vector length). These are independent parameters. A vocabulary of 50,000 tokens with 512-dimensional embeddings produces a 50,000  512 matrix.\r\n- **Cosine similarity is not distance:** Cosine similarity measures the angle between vectors, not their Euclidean distance. Two vectors can have high cosine similarity (pointing in the same direction) but very different magnitudes. For some applications, L2 distance or dot product similarity may be more appropriate.\r\n\r\n## Quiz\r\n\r\n**Q1: What is the shape of an embedding matrix for a vocabulary of 10,000 tokens with embedding dimension 256?**\r\n\r\n- A) [256, 10000]\r\n- B) [10000, 256]\r\n- C) [256, 256]\r\n- D) [10000, 10000]\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) [10000, 256]\r\n\r\nThe embedding matrix has shape [V, d] = [10000, 256]. Each of the 10,000 tokens has a 256-dimensional embedding vector, so each row corresponds to one token.\r\n\r\n\u003C/details>\r\n\r\n**Q2: What does a cosine similarity of 0 between two embedding vectors indicate?**\r\n\r\n- A) The tokens are identical\r\n- B) The tokens are opposite in meaning\r\n- C) The embedding vectors are orthogonal (perpendicular)\r\n- D) One of the vectors is a zero vector\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** C) The embedding vectors are orthogonal (perpendicular)\r\n\r\nA cosine similarity of 0 means the two vectors are orthogonal  they point in perpendicular directions in the embedding space. This suggests the tokens are unrelated in the learned representation. A similarity of +1 means identical direction, and -1 means opposite direction.\r\n\r\n\u003C/details>\r\n\r\n## Further Reading\r\n\r\n- [Word2Vec  Efficient Estimation of Word Representations (Mikolov et al., 2013)](https://arxiv.org/abs/1301.3781) (article)\r\n- [The Illustrated Word2Vec  Jay Alammar](https://jalammar.github.io/illustrated-word2vec/) (article)\r\n- [Embedding  PyTorch Documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) (reference)\r\n\r\n## Related Algorithms\r\n\r\n- [Self-Attention (Scaled Dot-Product)](/docs/algorithms/self-attention/)\r\n- [BPE Tokenization](/docs/algorithms/tokenization-bpe/)\r\n\r\n## Prerequisites\r\n\r\n- [BPE Tokenization](/docs/algorithms/tokenization-bpe/)","src/content/docs/algorithms/token-embeddings.mdx","44b8090056808d99","algorithms/tokenization-bpe",{"id":210,"data":212,"body":218,"filePath":219,"digest":220,"deferredRender":16},{"title":213,"description":214,"editUrl":16,"head":215,"template":50,"sidebar":216,"pagefind":16,"draft":39},"BPE Tokenization","Break text into subword tokens using Byte-Pair Encoding merge rules.",[],{"hidden":39,"attrs":217},{},"{/* This file is auto-generated by scripts/generate-algorithm-docs.py. */}\r\n{/* Do not edit manually  changes will be overwritten. */}\r\n{/* To update, modify the algorithm's meta.json and re-run the script. */}\r\n\r\n**Category:** Generative AI  \r\n**Difficulty:** Beginner  \r\n**Time Complexity:** `O(n  m)`  \r\n**Space Complexity:** `O(n)`\r\n\r\n## Overview\r\n\r\nByte-Pair Encoding (BPE) is the dominant tokenization algorithm used in modern large language models including GPT, LLaMA, and others. Starting from individual characters, BPE iteratively applies learned merge rules that combine the most frequent adjacent token pairs into single tokens. This process produces a subword vocabulary that balances between character-level and word-level representations  common words become single tokens while rare words are split into meaningful subword units. This visualization walks through the merge process step by step, showing how raw text is transformed into the token sequence that a language model actually processes.\r\n\r\n## Try It\r\n\r\n- **Web:** [Open in Eigenvue ](https://eigenvue.web.app/algo/tokenization-bpe)\r\n- **Python:**\r\n  ```python\r\n  import eigenvue\r\n  eigenvue.show(\"tokenization-bpe\")\r\n  ```\r\n\r\n## Default Inputs\r\n\r\n```json\r\n{\r\n  \"text\": \"lowest\",\r\n  \"mergeRules\": [\r\n    [\r\n      [\r\n        \"l\",\r\n        \"o\"\r\n      ],\r\n      \"lo\"\r\n    ],\r\n    [\r\n      [\r\n        \"lo\",\r\n        \"w\"\r\n      ],\r\n      \"low\"\r\n    ],\r\n    [\r\n      [\r\n        \"e\",\r\n        \"s\"\r\n      ],\r\n      \"es\"\r\n    ],\r\n    [\r\n      [\r\n        \"es\",\r\n        \"t\"\r\n      ],\r\n      \"est\"\r\n    ],\r\n    [\r\n      [\r\n        \"low\",\r\n        \"est\"\r\n      ],\r\n      \"lowest\"\r\n    ]\r\n  ]\r\n}\r\n```\r\n\r\n## Input Examples\r\n\r\n### Default  \"lowest\"\r\n\r\n```json\r\n{\r\n  \"text\": \"lowest\",\r\n  \"mergeRules\": [\r\n    [\r\n      [\r\n        \"l\",\r\n        \"o\"\r\n      ],\r\n      \"lo\"\r\n    ],\r\n    [\r\n      [\r\n        \"lo\",\r\n        \"w\"\r\n      ],\r\n      \"low\"\r\n    ],\r\n    [\r\n      [\r\n        \"e\",\r\n        \"s\"\r\n      ],\r\n      \"es\"\r\n    ],\r\n    [\r\n      [\r\n        \"es\",\r\n        \"t\"\r\n      ],\r\n      \"est\"\r\n    ],\r\n    [\r\n      [\r\n        \"low\",\r\n        \"est\"\r\n      ],\r\n      \"lowest\"\r\n    ]\r\n  ]\r\n}\r\n```\r\n\r\n### Simple  \"ab\"\r\n\r\n```json\r\n{\r\n  \"text\": \"aaabdaaabac\",\r\n  \"mergeRules\": [\r\n    [\r\n      [\r\n        \"a\",\r\n        \"a\"\r\n      ],\r\n      \"aa\"\r\n    ],\r\n    [\r\n      [\r\n        \"aa\",\r\n        \"a\"\r\n      ],\r\n      \"aaa\"\r\n    ],\r\n    [\r\n      [\r\n        \"a\",\r\n        \"b\"\r\n      ],\r\n      \"ab\"\r\n    ],\r\n    [\r\n      [\r\n        \"aaa\",\r\n        \"b\"\r\n      ],\r\n      \"aaab\"\r\n    ]\r\n  ]\r\n}\r\n```\r\n\r\n### No merges apply\r\n\r\n```json\r\n{\r\n  \"text\": \"xyz\",\r\n  \"mergeRules\": [\r\n    [\r\n      [\r\n        \"a\",\r\n        \"b\"\r\n      ],\r\n      \"ab\"\r\n    ],\r\n    [\r\n      [\r\n        \"c\",\r\n        \"d\"\r\n      ],\r\n      \"cd\"\r\n    ]\r\n  ]\r\n}\r\n```\r\n\r\n## Code\r\n\r\n### Pseudocode\r\n\r\n```\r\nfunction tokenizeBPE(text, mergeRules):\r\n  tokens = splitIntoCharacters(text)\r\n  for each (left, right)  replacement in mergeRules:\r\n    i = 0\r\n    while i \u003C length(tokens) - 1:\r\n      if tokens[i] == left AND tokens[i+1] == right:\r\n        tokens = tokens[0..i] + [replacement] + tokens[i+2..]\r\n      else:\r\n        i = i + 1\r\n  return tokens\r\n```\r\n\r\n### Python\r\n\r\n```python\r\ndef tokenize_bpe(text: str, merge_rules: list[tuple[tuple[str, str], str]]) -> list[str]:\r\n    tokens = list(text)\r\n    for (left, right), replacement in merge_rules:\r\n        i = 0\r\n        while i \u003C len(tokens) - 1:\r\n            if tokens[i] == left and tokens[i + 1] == right:\r\n                tokens = tokens[:i] + [replacement] + tokens[i + 2:]\r\n            else:\r\n                i += 1\r\n    return tokens\r\n```\r\n\r\n### JavaScript\r\n\r\n```javascript\r\nfunction tokenizeBPE(text, mergeRules) {\r\n  let tokens = [...text];\r\n  for (const [[left, right], replacement] of mergeRules) {\r\n    let i = 0;\r\n    while (i \u003C tokens.length - 1) {\r\n      if (tokens[i] === left && tokens[i + 1] === right) {\r\n        tokens = [...tokens.slice(0, i), replacement, ...tokens.slice(i + 2)];\r\n      } else {\r\n        i++;\r\n      }\r\n    }\r\n  }\r\n  return tokens;\r\n}\r\n```\r\n\r\n## Key Concepts\r\n\r\n### Subword Tokenization\r\n\r\nBPE produces tokens that are between characters and full words. Common words like 'the' become single tokens, while rare words like 'unfathomable' are split into subword pieces like 'un', 'fath', 'om', 'able'. This gives a fixed-size vocabulary that can represent any text.\r\n\r\n### Merge Rules Are Learned\r\n\r\nThe merge rules are learned from a training corpus by repeatedly finding the most frequent adjacent pair and merging it. At inference time, these pre-learned rules are applied in order to tokenize new text. The order of rules matters  earlier rules are applied first.\r\n\r\n### Greedy Left-to-Right Application\r\n\r\nEach merge rule is applied greedily from left to right across the token sequence. When a matching pair is found, it is merged immediately and the scan continues from the same position (to handle consecutive matches). This deterministic process ensures the same input always produces the same tokens.\r\n\r\n### Vocabulary Size Trade-off\r\n\r\nMore merge rules mean larger vocabulary and shorter token sequences (fewer tokens per text), but require more memory for the embedding table. Fewer rules mean smaller vocabulary but longer sequences. GPT models typically use 50,000-100,000 merge rules.\r\n\r\n## Common Pitfalls\r\n\r\n- **Merge order matters:** Applying merge rules in a different order can produce different token sequences. The rules must be applied in the exact order they were learned during training. Shuffling or reordering rules will break tokenization.\r\n- **BPE is not morphological:** BPE splits are based purely on statistical frequency, not linguistic morphology. The subword units may not correspond to meaningful morphemes. For example, 'unhappy' might split as 'un' + 'happy' (linguistically meaningful) or as 'unh' + 'appy' (not meaningful), depending on the training data.\r\n- **Unknown characters:** If the input text contains characters not seen during training, BPE cannot merge them and they remain as individual character tokens. Modern implementations use byte-level BPE to handle any input byte, avoiding true out-of-vocabulary situations.\r\n\r\n## Quiz\r\n\r\n**Q1: Given tokens ['l', 'o', 'w'] and merge rule (['l', 'o'], 'lo'), what is the result?**\r\n\r\n- A) ['lo', 'w']\r\n- B) ['l', 'ow']\r\n- C) ['low']\r\n- D) ['l', 'o', 'w']\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** A) ['lo', 'w']\r\n\r\nThe merge rule combines 'l' and 'o' into 'lo'. The 'w' is not part of this merge and remains separate. Result: ['lo', 'w'].\r\n\r\n\u003C/details>\r\n\r\n**Q2: Why are BPE merge rules applied in a specific order?**\r\n\r\n- A) For computational efficiency only\r\n- B) Because the order was learned from training data and affects the output\r\n- C) The order doesn't matter  any order gives the same result\r\n- D) To ensure alphabetical token ordering\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) Because the order was learned from training data and affects the output\r\n\r\nMerge rules are learned in order of frequency from the training corpus. Applying them in a different order can produce different tokenizations because earlier merges change which pairs are available for later merges.\r\n\r\n\u003C/details>\r\n\r\n**Q3: What is the initial token sequence for the text 'hello'?**\r\n\r\n- A) ['hello']\r\n- B) ['hel', 'lo']\r\n- C) ['h', 'e', 'l', 'l', 'o']\r\n- D) ['he', 'll', 'o']\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** C) ['h', 'e', 'l', 'l', 'o']\r\n\r\nBPE always starts by splitting the input text into individual characters. Each character becomes its own token. Merge rules are then applied to combine adjacent tokens.\r\n\r\n\u003C/details>\r\n\r\n## Further Reading\r\n\r\n- [Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2016)](https://arxiv.org/abs/1508.07909) (article)\r\n- [Byte-Pair Encoding  Wikipedia](https://en.wikipedia.org/wiki/Byte_pair_encoding) (reference)\r\n- [Hugging Face Tokenizers Documentation](https://huggingface.co/docs/tokenizers) (reference)\r\n\r\n## Related Algorithms\r\n\r\n- [Token Embeddings](/docs/algorithms/token-embeddings/)\r\n- [Self-Attention (Scaled Dot-Product)](/docs/algorithms/self-attention/)","src/content/docs/algorithms/tokenization-bpe.mdx","153a1336d1415cd9","algorithms/self-attention",{"id":221,"data":223,"body":229,"filePath":230,"digest":231,"deferredRender":16},{"title":224,"description":225,"editUrl":16,"head":226,"template":50,"sidebar":227,"pagefind":16,"draft":39},"Self-Attention (Scaled Dot-Product)","Watch how each token decides which other tokens to pay attention to.",[],{"hidden":39,"attrs":228},{},"{/* This file is auto-generated by scripts/generate-algorithm-docs.py. */}\r\n{/* Do not edit manually  changes will be overwritten. */}\r\n{/* To update, modify the algorithm's meta.json and re-run the script. */}\r\n\r\n**Category:** Generative AI  \r\n**Difficulty:** Intermediate  \r\n**Time Complexity:** `O(n  d)`  \r\n**Space Complexity:** `O(n + n  d)`\r\n\r\n## Overview\r\n\r\nSelf-attention is the core mechanism inside Transformer models. Given a sequence of token embeddings, each token creates three vectors: a Query (what am I looking for?), a Key (what do I contain?), and a Value (what information do I provide?). Attention scores are computed as the dot product of queries and keys, scaled by 1/d_k to prevent gradient vanishing, then normalized with softmax to produce a probability distribution. Each token's output is a weighted sum of all value vectors, where the weights reflect how relevant each other token is. This allows every token to directly attend to every other token in the sequence, capturing long-range dependencies that recurrent models struggle with. Self-attention is the building block of the Transformer architecture introduced in 'Attention Is All You Need' (Vaswani et al., 2017).\r\n\r\n## Try It\r\n\r\n- **Web:** [Open in Eigenvue ](https://eigenvue.web.app/algo/self-attention)\r\n- **Python:**\r\n  ```python\r\n  import eigenvue\r\n  eigenvue.show(\"self-attention\")\r\n  ```\r\n\r\n## Default Inputs\r\n\r\n```json\r\n{\r\n  \"tokens\": [\r\n    \"The\",\r\n    \"cat\",\r\n    \"sat\"\r\n  ],\r\n  \"embeddingDim\": 4\r\n}\r\n```\r\n\r\n## Input Examples\r\n\r\n### 3 tokens, dim 4 (default)\r\n\r\n```json\r\n{\r\n  \"tokens\": [\r\n    \"The\",\r\n    \"cat\",\r\n    \"sat\"\r\n  ],\r\n  \"embeddingDim\": 4\r\n}\r\n```\r\n\r\n### 4 tokens, dim 4\r\n\r\n```json\r\n{\r\n  \"tokens\": [\r\n    \"The\",\r\n    \"cat\",\r\n    \"sat\",\r\n    \"down\"\r\n  ],\r\n  \"embeddingDim\": 4\r\n}\r\n```\r\n\r\n### Short sentence, dim 3\r\n\r\n```json\r\n{\r\n  \"tokens\": [\r\n    \"I\",\r\n    \"love\",\r\n    \"AI\"\r\n  ],\r\n  \"embeddingDim\": 3\r\n}\r\n```\r\n\r\n### 5 tokens, dim 4\r\n\r\n```json\r\n{\r\n  \"tokens\": [\r\n    \"The\",\r\n    \"quick\",\r\n    \"brown\",\r\n    \"fox\",\r\n    \"jumps\"\r\n  ],\r\n  \"embeddingDim\": 4\r\n}\r\n```\r\n\r\n## Code\r\n\r\n### Pseudocode\r\n\r\n```\r\nfunction SelfAttention(X, W_Q, W_K, W_V):\r\n  Q = X  W_Q            // Query projection\r\n  K = X  W_K            // Key projection\r\n  V = X  W_V            // Value projection\r\n  scores = Q  K        // Raw attention scores\r\n  scaled = scores / d_k  // Scale to stabilize gradients\r\n  weights = softmax(scaled, dim=row)  // Normalize each row\r\n  for each query token q:\r\n    output[q] =  weights[q][j]  V[j]  // Weighted sum\r\n  return output\r\n```\r\n\r\n## Key Concepts\r\n\r\n### Query, Key, Value Intuition\r\n\r\nThink of attention as an information retrieval system. The Query is a search query ('what am I looking for?'), the Key is an index entry ('what do I contain?'), and the Value is the actual content ('here is my information'). The dot product of Q and K measures relevance, and the result is used to weight the Values.\r\n\r\n### Softmax Normalization\r\n\r\nSoftmax converts raw attention scores into a probability distribution where all weights are non-negative and each row sums to exactly 1.0. This means each token distributes 100% of its attention across all tokens in the sequence, with higher weights on more relevant tokens.\r\n\r\n### Scaling by 1/d_k\r\n\r\nWhen the embedding dimension d_k is large, dot products grow in magnitude (their variance scales with d_k). Large dot products push softmax into saturated regions where gradients are extremely small, making learning slow. Dividing by d_k keeps the variance at approximately 1, ensuring healthy gradients.\r\n\r\n### Quadratic Complexity\r\n\r\nSelf-attention computes a score for every pair of tokens, giving O(n) time and space complexity in sequence length. This is why Transformers struggle with very long sequences (e.g., 100K+ tokens) and why research into efficient attention variants (linear attention, sparse attention) is active.\r\n\r\n## Common Pitfalls\r\n\r\n- **Scaling factor is d_k, NOT d_model:** In multi-head attention, each head has dimension d_k = d_model / num_heads. The scaling factor uses d_k (the per-head dimension), not d_model (the full model dimension). Using d_model would over-scale and flatten the attention distribution.\r\n- **Softmax is applied row-wise, not element-wise:** Softmax must be applied independently to each row of the score matrix. Each row corresponds to one query token's attention distribution. Applying softmax to the entire matrix or column-wise would produce incorrect attention weights.\r\n- **Attention weights do not encode position:** Pure self-attention is permutation-equivariant  it treats the input as a set, not a sequence. Without positional encodings added to the embeddings, the model cannot distinguish 'the cat sat' from 'sat cat the'.\r\n\r\n## Quiz\r\n\r\n**Q1: What does the softmax function ensure about each row of the attention weight matrix?**\r\n\r\n- A) All values are between -1 and 1\r\n- B) All values are non-negative and each row sums to 1.0\r\n- C) The matrix is symmetric\r\n- D) The diagonal values are the largest\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) All values are non-negative and each row sums to 1.0\r\n\r\nSoftmax exponentiates each score (making it positive) and divides by the row sum, guaranteeing all weights are non-negative and each row sums to exactly 1.0. This creates a valid probability distribution over the key tokens for each query token.\r\n\r\n\u003C/details>\r\n\r\n**Q2: Why are attention scores divided by d_k before applying softmax?**\r\n\r\n- A) To make the matrix square\r\n- B) To reduce memory usage\r\n- C) To prevent large dot products from causing vanishing gradients in softmax\r\n- D) To normalize the output to unit length\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** C) To prevent large dot products from causing vanishing gradients in softmax\r\n\r\nWhen d_k is large, dot products tend to have large magnitudes. Large inputs to softmax produce outputs very close to 0 or 1, where gradients are near zero. Scaling by 1/d_k keeps dot product magnitudes manageable, ensuring softmax operates in a region with healthy gradients.\r\n\r\n\u003C/details>\r\n\r\n**Q3: What is the time complexity of self-attention with respect to sequence length n?**\r\n\r\n- A) O(n)\r\n- B) O(n log n)\r\n- C) O(n  d)\r\n- D) O(n)\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** C) O(n  d)\r\n\r\nSelf-attention computes the dot product of every query with every key (n pairs), and each dot product involves d_k dimensions, giving O(n  d) time. The n factor is why self-attention is expensive for long sequences.\r\n\r\n\u003C/details>\r\n\r\n## Further Reading\r\n\r\n- [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762) (paper)\r\n- [The Illustrated Transformer  Jay Alammar](https://jalammar.github.io/illustrated-transformer/) (article)\r\n- [Self-Attention  Wikipedia](https://en.wikipedia.org/wiki/Attention_(machine_learning)) (reference)\r\n\r\n## Related Algorithms\r\n\r\n- [Multi-Head Attention](/docs/algorithms/multi-head-attention/)\r\n- [Token Embeddings](/docs/algorithms/token-embeddings/)\r\n\r\n## Prerequisites\r\n\r\n- [Token Embeddings](/docs/algorithms/token-embeddings/)","src/content/docs/algorithms/self-attention.mdx","6aa7273be74ada06","algorithms/transformer-block",{"id":232,"data":234,"body":240,"filePath":241,"digest":242,"deferredRender":16},{"title":235,"description":236,"editUrl":16,"head":237,"template":50,"sidebar":238,"pagefind":16,"draft":39},"Transformer Block","Follow data through a complete transformer encoder block step by step.",[],{"hidden":39,"attrs":239},{},"{/* This file is auto-generated by scripts/generate-algorithm-docs.py. */}\r\n{/* Do not edit manually  changes will be overwritten. */}\r\n{/* To update, modify the algorithm's meta.json and re-run the script. */}\r\n\r\n**Category:** Generative AI  \r\n**Difficulty:** Advanced  \r\n**Time Complexity:** `O(n  d + n  d  d_ff)`  \r\n**Space Complexity:** `O(n + n  d_ff)`\r\n\r\n## Overview\r\n\r\nA transformer block is the fundamental building unit of models like BERT, GPT, and LLaMA. It processes input embeddings through a sequence of sublayers: Multi-Head Self-Attention  Add & Layer Norm  Feed-Forward Network  Add & Layer Norm. Residual connections (skip connections) add the input of each sublayer to its output before normalization, preserving the original signal and enabling deep networks. This visualization shows every computation: attention scores, residual additions, layer normalization, and the two-layer FFN with ReLU activation.\r\n\r\n## Try It\r\n\r\n- **Web:** [Open in Eigenvue ](https://eigenvue.web.app/algo/transformer-block)\r\n- **Python:**\r\n  ```python\r\n  import eigenvue\r\n  eigenvue.show(\"transformer-block\")\r\n  ```\r\n\r\n## Default Inputs\r\n\r\n```json\r\n{\r\n  \"tokens\": [\r\n    \"The\",\r\n    \"cat\",\r\n    \"sat\"\r\n  ],\r\n  \"embeddingDim\": 4,\r\n  \"ffnDim\": 8,\r\n  \"numHeads\": 1\r\n}\r\n```\r\n\r\n## Input Examples\r\n\r\n### 3 tokens, 1 head (dim=4, ffn=8)\r\n\r\n```json\r\n{\r\n  \"tokens\": [\r\n    \"The\",\r\n    \"cat\",\r\n    \"sat\"\r\n  ],\r\n  \"embeddingDim\": 4,\r\n  \"ffnDim\": 8,\r\n  \"numHeads\": 1\r\n}\r\n```\r\n\r\n### 4 tokens, 2 heads (dim=8, ffn=16)\r\n\r\n```json\r\n{\r\n  \"tokens\": [\r\n    \"The\",\r\n    \"cat\",\r\n    \"sat\",\r\n    \"down\"\r\n  ],\r\n  \"embeddingDim\": 8,\r\n  \"ffnDim\": 16,\r\n  \"numHeads\": 2\r\n}\r\n```\r\n\r\n## Code\r\n\r\n### Pseudocode\r\n\r\n```\r\nfunction TransformerBlock(X, W_attn, W_ffn):\r\n  // Sublayer 1: Self-Attention + Add & Norm\r\n  attn_output = MultiHeadAttention(X)\r\n  residual_1 = X + attn_output          // residual connection\r\n  norm_1 = LayerNorm(residual_1)\r\n\r\n  // Sublayer 2: FFN + Add & Norm\r\n  ffn_output = FFN(norm_1)\r\n    = ReLU(norm_1  W + b)  W + b\r\n  residual_2 = norm_1 + ffn_output       // residual connection\r\n  norm_2 = LayerNorm(residual_2)\r\n\r\n  return norm_2\r\n```\r\n\r\n### Python\r\n\r\n```python\r\ndef transformer_block(X, attn_params, ffn_params):\r\n    # Self-Attention + Add & Norm\r\n    attn_out = multi_head_attention(X, **attn_params)\r\n    residual_1 = X + attn_out\r\n    norm_1 = layer_norm(residual_1)\r\n    \r\n    # FFN + Add & Norm\r\n    ffn_out = relu(norm_1 @ W1 + b1) @ W2 + b2\r\n    residual_2 = norm_1 + ffn_out\r\n    norm_2 = layer_norm(residual_2)\r\n    return norm_2\r\n```\r\n\r\n## Key Concepts\r\n\r\n### Residual Connections\r\n\r\nEach sublayer's output is added to its input: output = x + sublayer(x). This 'skip connection' ensures that gradients can flow directly through the network, enabling training of very deep models (100+ layers). Without residuals, deep transformers fail to train.\r\n\r\n### Layer Normalization\r\n\r\nAfter each residual addition, layer normalization normalizes each token's vector to have mean  0 and variance  1. This stabilizes the internal activations and helps the model train faster. Unlike batch normalization, layer norm operates on individual examples.\r\n\r\n### Feed-Forward Network\r\n\r\nThe FFN applies two linear transformations with a ReLU activation: FFN(x) = max(0, xW + b)W + b. The hidden dimension (d_ff) is typically 4 the model dimension. This gives each token a nonlinear transformation independent of other tokens.\r\n\r\n### Block Stacking\r\n\r\nReal transformers stack 6 (original), 12 (BERT-base), 24 (GPT-2), or 96+ (GPT-4) identical blocks. Each block refines the representations. The output of one block is the input to the next.\r\n\r\n## Common Pitfalls\r\n\r\n- **Add Then Norm, Not Norm Then Add:** The original Transformer uses Post-Norm: LayerNorm(x + sublayer(x)). Some implementations use Pre-Norm: x + sublayer(LayerNorm(x)). This visualization uses Post-Norm as in the original paper.\r\n- **FFN Is Per-Token:** Unlike attention which mixes information across tokens, the FFN processes each token independently with the same weights. Cross-token interaction only happens in the attention sublayer.\r\n\r\n## Quiz\r\n\r\n**Q1: What is the purpose of the residual connection in a transformer block?**\r\n\r\n- A) To reduce the number of parameters\r\n- B) To allow gradients to flow directly through the network\r\n- C) To increase the embedding dimension\r\n- D) To apply nonlinearity\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** B) To allow gradients to flow directly through the network\r\n\r\nResidual connections create a direct path for gradients, preventing the vanishing gradient problem in deep networks. The identity mapping lets the network learn the 'delta' (what to add) rather than the full transformation.\r\n\r\n\u003C/details>\r\n\r\n**Q2: If d_model = 512 and d_ff = 2048, how many parameters does the FFN have (ignoring biases)?**\r\n\r\n- A) 512  512\r\n- B) 512  2048\r\n- C) 2  512  2048\r\n- D) 512  2048  512\r\n\r\n\u003Cdetails>\r\n\u003Csummary>Show answer\u003C/summary>\r\n\r\n**Answer:** C) 2  512  2048\r\n\r\nThe FFN has two weight matrices: W [512, 2048] and W [2048, 512]. Total parameters = 5122048 + 2048512 = 2  512  2048 = 2,097,152.\r\n\r\n\u003C/details>\r\n\r\n## Further Reading\r\n\r\n- [Vaswani et al. 2017  Attention Is All You Need](https://arxiv.org/abs/1706.03762) (paper)\r\n- [The Annotated Transformer  Harvard NLP](https://nlp.seas.harvard.edu/2018/04/03/attention.html) (tutorial)\r\n\r\n## Related Algorithms\r\n\r\n- [Multi-Head Attention](/docs/algorithms/multi-head-attention/)\r\n- [Self-Attention (Scaled Dot-Product)](/docs/algorithms/self-attention/)\r\n\r\n## Prerequisites\r\n\r\n- [Multi-Head Attention](/docs/algorithms/multi-head-attention/)","src/content/docs/algorithms/transformer-block.mdx","e1934b68d8dda3d3","api-reference/layout-system",{"id":243,"data":245,"body":251,"filePath":252,"digest":253,"deferredRender":16},{"title":246,"description":247,"editUrl":16,"head":248,"template":50,"sidebar":249,"pagefind":16,"draft":39},"Layout System","Reference for the layout system.",[],{"hidden":39,"attrs":250},{},"The layout system is the bridge between abstract visual actions and concrete rendering on screen. A layout defines how algorithm data is spatially arranged, which visual primitives are used, and how visual actions translate to visual changes.\n\n---\n\n## Concept\n\nA **layout** is a rendering strategy. It determines:\n\n- How the algorithm's state is spatially arranged (e.g., array cells in a row, graph nodes in a force-directed layout, a heatmap grid).\n- Which visual primitives (elements, connections, containers, annotations, overlays) are available.\n- How each visual action type maps to changes in those primitives.\n- What layout-specific configuration is read from the algorithm's `meta.json` `visual.components` field.\n\nLayouts are not tied to individual algorithms. Multiple algorithms share the same layout. For example, binary search, bubble sort, and quicksort all use the `array-with-pointers` layout with different configurations.\n\n---\n\n## Layout Registry\n\nLayouts are managed through a registry that maps layout names (strings) to layout implementations.\n\n### `registerLayout`\n\nRegisters a new layout implementation in the global registry.\n\n```typescript\nfunction registerLayout(name: string, factory: LayoutFactory): void\n```\n\n| Parameter | Type             | Description                                                       |\n| --------- | ---------------- | ----------------------------------------------------------------- |\n| `name`    | `string`         | The layout identifier. Must match the `visual.layout` values used in `meta.json`. |\n| `factory` | `LayoutFactory`  | A factory function that creates a layout instance given configuration. |\n\n### `getLayout`\n\nRetrieves a registered layout by name.\n\n```typescript\nfunction getLayout(name: string): LayoutFactory | undefined\n```\n\n| Parameter | Type     | Description                              |\n| --------- | -------- | ---------------------------------------- |\n| `name`    | `string` | The layout identifier to look up.        |\n\nReturns `undefined` if no layout is registered with the given name.\n\n---\n\n## Registered Layouts\n\nEigenvue ships with 10 built-in layouts, organized by domain.\n\n| Layout Name           | Category         | Visual Primitives                                    | Algorithms                                                   |\n| --------------------- | ---------------- | ---------------------------------------------------- | ------------------------------------------------------------ |\n| `array-with-pointers` | Classical        | Array cells, named pointers, value labels, index labels | Binary search, bubble sort, quicksort (partition view)       |\n| `array-comparison`    | Classical        | Dual array tracks, merge indicators, comparison arcs | Merge sort                                                    |\n| `graph-network`       | Classical        | Nodes, edges, labels, visited/unvisited states       | BFS, DFS, Dijkstra                                            |\n| `neuron-diagram`      | Deep Learning    | Neurons, connections, activation values, bias nodes  | Single neuron, perceptron, activation functions                |\n| `layer-network`       | Deep Learning    | Layer columns, inter-layer connections, weight labels, gradient overlays | Forward propagation, backpropagation, neural network training |\n| `convolution-grid`    | Deep Learning    | Input grid, kernel overlay, output grid, product annotations | 2D convolution                                           |\n| `loss-landscape`      | Deep Learning    | 3D/contour surface, position marker, gradient arrows, trajectory path | Gradient descent, SGD, momentum, Adam                  |\n| `token-sequence`      | Generative AI    | Token cells, merge animations, embedding vectors, bar charts | BPE tokenization, token embeddings                      |\n| `attention-heatmap`   | Generative AI    | Token rows/columns, heatmap cells, Q/K/V matrices, score annotations | Self-attention, multi-head attention                  |\n| `layer-diagram`       | Generative AI    | Sublayer blocks, data flow arrows, residual connections, norm indicators | Transformer block                                   |\n\n---\n\n## Layout Function Contract\n\nEvery layout must implement the following contract.\n\n### Initialization\n\nThe layout receives its configuration from the algorithm's `meta.json` at initialization time:\n\n```typescript\ninterface LayoutConfig {\n  /** The visual.components object from meta.json. */\n  readonly components: Readonly\u003CRecord\u003Cstring, unknown>>;\n  /** The visual.theme overrides from meta.json. */\n  readonly theme?: {\n    readonly primary?: string;\n    readonly secondary?: string;\n  };\n  /** Canvas dimensions (set by the canvas manager). */\n  readonly width: number;\n  readonly height: number;\n}\n```\n\n### Rendering\n\nThe layout's primary responsibility is to render a step's visual state onto the canvas. This involves:\n\n1. **Reading the step's `state`** to determine positions, values, and structural data.\n2. **Processing the step's `visualActions`** in order, mapping each action type to changes in visual primitives.\n3. **Drawing visual primitives** onto the canvas using the rendering engine's drawing API.\n\n### Action Handling\n\nThe layout maintains a mapping from action types to handler functions:\n\n```typescript\ntype ActionHandler = (action: VisualAction, state: Record\u003Cstring, unknown>) => void;\n```\n\nFor unknown action types, the layout must silently skip the action (no-op). This preserves the open vocabulary design -- new action types can be added without breaking existing layouts.\n\n---\n\n## Layout Configuration from meta.json\n\nEach algorithm's `meta.json` specifies its layout and optional configuration in the `visual` section:\n\n```json\n{\n  \"visual\": {\n    \"layout\": \"array-with-pointers\",\n    \"theme\": {\n      \"primary\": \"#38bdf8\",\n      \"secondary\": \"#0284c7\"\n    },\n    \"components\": {\n      \"pointers\": [\n        { \"id\": \"left\", \"label\": \"L\", \"color\": \"#38bdf8\" },\n        { \"id\": \"right\", \"label\": \"R\", \"color\": \"#38bdf8\" },\n        { \"id\": \"mid\", \"label\": \"M\", \"color\": \"#f472b6\" }\n      ],\n      \"showIndices\": true,\n      \"showValues\": true,\n      \"highlightColor\": \"#fbbf24\",\n      \"foundColor\": \"#22c55e\"\n    }\n  }\n}\n```\n\nThe `components` object is entirely layout-specific. The rendering engine passes it through to the layout without interpretation. Each layout defines its own expected configuration shape.\n\n---\n\n## Layout-Specific Configuration\n\n### `array-with-pointers`\n\n| Component Key    | Type       | Description                                       |\n| ---------------- | ---------- | ------------------------------------------------- |\n| `pointers`       | `array`    | Array of pointer definitions with `id`, `label`, and `color`. |\n| `showIndices`    | `boolean`  | Whether to display 0-based index labels below cells. |\n| `showValues`     | `boolean`  | Whether to display values inside array cells.     |\n| `highlightColor` | `string`   | Default highlight color (hex).                    |\n| `foundColor`     | `string`   | Color for the found element (hex).                |\n| `dimColor`       | `string`   | Color for dimmed/eliminated elements.             |\n| `compareColor`   | `string`   | Color for elements being compared.                |\n\n### `array-comparison`\n\n| Component Key    | Type       | Description                                       |\n| ---------------- | ---------- | ------------------------------------------------- |\n| `tracks`         | `number`   | Number of array tracks to display (typically 2).  |\n| `showMergeZone`  | `boolean`  | Whether to show the merge output zone.            |\n\n### `graph-network`\n\n| Component Key    | Type       | Description                                       |\n| ---------------- | ---------- | ------------------------------------------------- |\n| `directed`       | `boolean`  | Whether edges have direction (arrows).            |\n| `weighted`       | `boolean`  | Whether to display edge weights.                  |\n| `nodeRadius`     | `number`   | Radius of graph nodes in pixels.                  |\n\n### `neuron-diagram`\n\n| Component Key    | Type       | Description                                       |\n| ---------------- | ---------- | ------------------------------------------------- |\n| `showBias`       | `boolean`  | Whether to display bias inputs.                   |\n| `activationFn`   | `string`   | Default activation function to display.           |\n\n### `layer-network`\n\n| Component Key    | Type       | Description                                       |\n| ---------------- | ---------- | ------------------------------------------------- |\n| `layerSizes`     | `number[]` | Number of neurons per layer for initial layout.   |\n| `showWeights`    | `boolean`  | Whether to display weight values on connections.  |\n| `showGradients`  | `boolean`  | Whether gradient display is enabled.              |\n\n### `convolution-grid`\n\n| Component Key    | Type       | Description                                       |\n| ---------------- | ---------- | ------------------------------------------------- |\n| `inputSize`      | `number[]` | Input grid dimensions `[rows, cols]`.             |\n| `kernelSize`     | `number[]` | Kernel dimensions `[rows, cols]`.                 |\n| `showProducts`   | `boolean`  | Whether to show element-wise products.            |\n\n### `loss-landscape`\n\n| Component Key    | Type       | Description                                       |\n| ---------------- | ---------- | ------------------------------------------------- |\n| `surfaceType`    | `string`   | Surface rendering mode: `\"contour\"` or `\"3d\"`.   |\n| `showTrajectory` | `boolean`  | Whether to display the optimization trajectory.   |\n| `showGradient`   | `boolean`  | Whether to show gradient arrows.                  |\n\n### `token-sequence`\n\n| Component Key    | Type       | Description                                       |\n| ---------------- | ---------- | ------------------------------------------------- |\n| `showIds`        | `boolean`  | Whether to display token IDs.                     |\n| `showEmbeddings` | `boolean`  | Whether the embedding vector panel is visible.    |\n\n### `attention-heatmap`\n\n| Component Key    | Type       | Description                                       |\n| ---------------- | ---------- | ------------------------------------------------- |\n| `showScores`     | `boolean`  | Whether to show raw scores alongside weights.     |\n| `showProjections`| `boolean`  | Whether Q/K/V projection panels are visible.      |\n| `colorScale`     | `string`   | Color scale for heatmap cells (e.g., `\"blues\"`, `\"viridis\"`). |\n\n### `layer-diagram`\n\n| Component Key    | Type       | Description                                       |\n| ---------------- | ---------- | ------------------------------------------------- |\n| `sublayers`      | `string[]` | Ordered list of sublayer IDs to display.          |\n| `showResidual`   | `boolean`  | Whether to show residual connection arrows.       |\n| `showNorm`       | `boolean`  | Whether to show layer normalization indicators.   |\n\n---\n\n## Creating a Custom Layout\n\nTo add a new layout to the system:\n\n1. **Create the layout module** implementing the layout factory interface.\n2. **Register it** using `registerLayout()` during application initialization.\n3. **Reference it** in algorithm `meta.json` files via the `visual.layout` field.\n\n```typescript\nimport { registerLayout } from \"@/engine/layout\";\n\nregisterLayout(\"my-custom-layout\", (config: LayoutConfig) => {\n  // Return a layout instance that implements the rendering contract\n  return {\n    render(step, canvas) {\n      // Read step.state and step.visualActions\n      // Draw primitives onto the canvas\n    },\n    dispose() {\n      // Clean up any resources\n    },\n  };\n});\n```\n\nThe layout name in `registerLayout` must exactly match the string used in `meta.json`'s `visual.layout` field.","src/content/docs/api-reference/layout-system.mdx","974df3d39e3b453f","api-reference/meta-json-schema",{"id":254,"data":256,"body":262,"filePath":263,"digest":264,"deferredRender":16},{"title":257,"description":258,"editUrl":16,"head":259,"template":50,"sidebar":260,"pagefind":16,"draft":39},"meta.json Schema","Algorithm metadata format reference.",[],{"hidden":39,"attrs":261},{},"Every algorithm in Eigenvue has a `meta.json` file that describes everything about the algorithm that is genuinely data (not logic): display information, educational content, visual configuration, SEO keywords, input schema, code samples for display, and relationships to other algorithms.\n\nThe `meta.json` file is located in the algorithm's directory:\n\n```\nalgorithms/\n  classical/\n    binary-search/\n      meta.json          \u003C-- algorithm metadata\n      generator.ts       \u003C-- step generator\n```\n\n---\n\n## AlgorithmMeta Interface\n\n```typescript\ninterface AlgorithmMeta {\n  readonly id: string;\n  readonly name: string;\n  readonly category: AlgorithmCategory;\n  readonly description: { readonly short: string; readonly long: string };\n  readonly complexity: { readonly time: string; readonly space: string; readonly level: DifficultyLevel };\n  readonly visual: { readonly layout: string; readonly theme?: { readonly primary?: string; readonly secondary?: string }; readonly components?: Readonly\u003CRecord\u003Cstring, unknown>> };\n  readonly inputs: { readonly schema: Readonly\u003CRecord\u003Cstring, unknown>>; readonly defaults: Readonly\u003CRecord\u003Cstring, unknown>>; readonly examples: readonly InputExample[] };\n  readonly code: { readonly implementations: { readonly pseudocode: string; readonly python?: string; readonly javascript?: string; readonly [key: string]: string | undefined }; readonly defaultLanguage: string };\n  readonly education: { readonly keyConcepts: readonly EducationEntry[]; readonly pitfalls: readonly EducationEntry[]; readonly quiz: readonly QuizQuestion[]; readonly resources: readonly ResourceLink[] };\n  readonly seo: { readonly keywords: readonly string[]; readonly ogDescription: string };\n  readonly prerequisites: readonly string[];\n  readonly related: readonly string[];\n  readonly author: string;\n  readonly version: string;\n}\n```\n\n---\n\n## Field Reference\n\n### Top-Level Fields\n\n| Field           | Type                | Required | Description                                                                                                     | Constraints                                                          |\n| --------------- | ------------------- | -------- | --------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------- |\n| `id`            | `string`            | Yes      | URL-safe unique identifier for this algorithm. Used in URLs, file paths, and cross-references.                  | Must match `^[a-z0-9][a-z0-9-]*$`.                                  |\n| `name`          | `string`            | Yes      | Human-readable display name.                                                                                    | Non-empty.                                                           |\n| `category`      | `AlgorithmCategory` | Yes      | Domain category.                                                                                                | One of: `\"classical\"`, `\"deep-learning\"`, `\"generative-ai\"`, `\"quantum\"`. |\n| `description`   | `object`            | Yes      | Short and long descriptions. See below.                                                                         |                                                                      |\n| `complexity`    | `object`            | Yes      | Time/space complexity and difficulty. See below.                                                                 |                                                                      |\n| `visual`        | `object`            | Yes      | Layout and visual configuration. See below.                                                                      |                                                                      |\n| `inputs`        | `object`            | Yes      | Input schema, defaults, and example presets. See below.                                                          |                                                                      |\n| `code`          | `object`            | Yes      | Display-only code implementations. See below.                                                                    |                                                                      |\n| `education`     | `object`            | Yes      | Educational content (concepts, pitfalls, quiz, resources). See below.                                            |                                                                      |\n| `seo`           | `object`            | Yes      | SEO keywords and Open Graph description. See below.                                                              |                                                                      |\n| `prerequisites` | `string[]`          | Yes      | IDs of algorithms that should be understood before this one. May be empty.                                       | Each value must be a valid algorithm ID.                             |\n| `related`       | `string[]`          | Yes      | IDs of related algorithms. May be empty.                                                                         | Each value must be a valid algorithm ID.                             |\n| `author`        | `string`            | Yes      | GitHub username of the primary author/contributor.                                                               | Non-empty.                                                           |\n| `version`       | `string`            | Yes      | Semantic version string.                                                                                        | Must follow semver (e.g., `\"1.0.0\"`).                               |\n\n### `description`\n\n| Field   | Type     | Required | Description                                              | Constraints            |\n| ------- | -------- | -------- | -------------------------------------------------------- | ---------------------- |\n| `short` | `string` | Yes      | Short description for cards and meta tags.               | Maximum 80 characters. |\n| `long`  | `string` | Yes      | Full description. Supports markdown formatting.          | Non-empty.             |\n\n### `complexity`\n\n| Field   | Type              | Required | Description                                              | Constraints                                                               |\n| ------- | ----------------- | -------- | -------------------------------------------------------- | ------------------------------------------------------------------------- |\n| `time`  | `string`          | Yes      | Time complexity in Big-O notation.                       | E.g., `\"O(log n)\"`, `\"O(n^2)\"`, `\"O(V + E)\"`.                          |\n| `space` | `string`          | Yes      | Space complexity in Big-O notation.                      | E.g., `\"O(1)\"`, `\"O(n)\"`.                                               |\n| `level` | `DifficultyLevel` | Yes      | Difficulty level for this algorithm.                     | One of: `\"beginner\"`, `\"intermediate\"`, `\"advanced\"`, `\"expert\"`.        |\n\n### `visual`\n\n| Field        | Type     | Required | Description                                                                                      | Constraints                              |\n| ------------ | -------- | -------- | ------------------------------------------------------------------------------------------------ | ---------------------------------------- |\n| `layout`     | `string` | Yes      | Name of the layout to use for rendering. Must be a registered layout in the rendering engine.    | See [Layout System](/docs/api-reference/layout-system) for valid values. |\n| `theme`      | `object` | No       | Color theme overrides.                                                                           |                                          |\n| `theme.primary`   | `string` | No  | Primary accent color override (hex).                                                             | Hex color string (e.g., `\"#38bdf8\"`).   |\n| `theme.secondary` | `string` | No  | Secondary accent color override (hex).                                                           | Hex color string (e.g., `\"#0284c7\"`).   |\n| `components` | `object` | No       | Layout-specific configuration. Shape depends on the layout. Read by the layout at initialization. |                                          |\n\n### `inputs`\n\n| Field      | Type                  | Required | Description                                                                                  | Constraints                |\n| ---------- | --------------------- | -------- | -------------------------------------------------------------------------------------------- | -------------------------- |\n| `schema`   | `object`              | Yes      | JSON Schema object defining the algorithm's input parameters. Used to generate the input editor UI and validate user-provided inputs. | Must be a valid JSON Schema fragment. |\n| `defaults` | `object`              | Yes      | Default parameter values for initial load.                                                   | Must conform to `schema`.  |\n| `examples` | `InputExample[]`      | Yes      | Named presets users can load from a dropdown.                                                | Array may be empty.        |\n\n#### `InputExample`\n\n| Field    | Type     | Required | Description                                            |\n| -------- | -------- | -------- | ------------------------------------------------------ |\n| `name`   | `string` | Yes      | Display name for this preset (e.g., \"Large Sorted Array\"). |\n| `values` | `object` | Yes      | Parameter values for this preset. Shape must match `inputs.schema`. |\n\n### `code`\n\n| Field             | Type     | Required | Description                                                                                              | Constraints                            |\n| ----------------- | -------- | -------- | -------------------------------------------------------------------------------------------------------- | -------------------------------------- |\n| `implementations` | `object` | Yes      | Display-only code shown in the code panel. These are clean, readable implementations for education -- NOT the generators. | `pseudocode` key is required. Others are optional. |\n| `implementations.pseudocode` | `string` | Yes | Pseudocode implementation. Always required.                                                      | Non-empty string.                      |\n| `implementations.python`     | `string` | No  | Python implementation.                                                                           |                                        |\n| `implementations.javascript` | `string` | No  | JavaScript implementation.                                                                       |                                        |\n| `defaultLanguage` | `string` | Yes      | Which language tab to show by default in the code panel.                                                 | Must be a key in `implementations`.    |\n\n### `education`\n\n| Field         | Type                | Required | Description                                | Constraints                   |\n| ------------- | ------------------- | -------- | ------------------------------------------ | ----------------------------- |\n| `keyConcepts` | `EducationEntry[]`  | Yes      | Key concepts the learner should understand.| Minimum 1 entry.              |\n| `pitfalls`    | `EducationEntry[]`  | Yes      | Common mistakes and misconceptions.        | Array may be empty.           |\n| `quiz`        | `QuizQuestion[]`    | Yes      | Self-assessment quiz questions.            | Array may be empty.           |\n| `resources`   | `ResourceLink[]`    | Yes      | External learning resources.               | Array may be empty.           |\n\n#### `EducationEntry`\n\n| Field         | Type     | Required | Description                                |\n| ------------- | -------- | -------- | ------------------------------------------ |\n| `title`       | `string` | Yes      | Entry title.                               |\n| `description` | `string` | Yes      | Entry description/explanation.             |\n\n#### `QuizQuestion`\n\n| Field          | Type       | Required | Description                                                                     | Constraints                                        |\n| -------------- | ---------- | -------- | ------------------------------------------------------------------------------- | -------------------------------------------------- |\n| `question`     | `string`   | Yes      | The quiz question.                                                              | Non-empty.                                         |\n| `options`      | `string[]` | Yes      | Array of answer option strings.                                                 | Minimum 2, maximum 6 options.                      |\n| `correctIndex` | `number`   | Yes      | 0-based index of the correct option.                                            | Must satisfy: `0 \u003C= correctIndex \u003C options.length`. |\n| `explanation`  | `string`   | Yes      | Explanation shown after the user answers.                                       | Non-empty.                                         |\n\n#### `ResourceLink`\n\n| Field   | Type     | Required | Description                                              | Constraints                                                                                    |\n| ------- | -------- | -------- | -------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |\n| `title` | `string` | Yes      | Link title.                                              | Non-empty.                                                                                     |\n| `url`   | `string` | Yes      | URL of the resource.                                     | Must be a valid URL.                                                                           |\n| `type`  | `string` | Yes      | Type of resource, for icon selection.                    | One of: `\"article\"`, `\"video\"`, `\"paper\"`, `\"course\"`, `\"documentation\"`, `\"interactive\"`.     |\n\n### `seo`\n\n| Field           | Type       | Required | Description                                     | Constraints            |\n| --------------- | ---------- | -------- | ----------------------------------------------- | ---------------------- |\n| `keywords`      | `string[]` | Yes      | Search engine keywords for this algorithm.      | Non-empty array.       |\n| `ogDescription` | `string`   | Yes      | Open Graph description override.                | Maximum 160 characters.|\n\n---\n\n## Supporting Types\n\n```typescript\ntype AlgorithmCategory = \"classical\" | \"deep-learning\" | \"generative-ai\" | \"quantum\";\n\ntype DifficultyLevel = \"beginner\" | \"intermediate\" | \"advanced\" | \"expert\";\n```\n\n---\n\n## Complete Annotated Example\n\nBelow is a complete `meta.json` file for binary search, annotated with comments (note: actual JSON files do not support comments -- they are shown here for documentation purposes).\n\n```json\n{\n  \"id\": \"binary-search\",\n  \"name\": \"Binary Search\",\n  \"category\": \"classical\",\n\n  \"description\": {\n    \"short\": \"Efficiently find a target in a sorted array by halving the search space.\",\n    \"long\": \"Binary search is a divide-and-conquer algorithm that finds a target value in a sorted array. At each step, it compares the target with the middle element and eliminates half of the remaining search space. This gives it O(log n) time complexity.\"\n  },\n\n  \"complexity\": {\n    \"time\": \"O(log n)\",\n    \"space\": \"O(1)\",\n    \"level\": \"beginner\"\n  },\n\n  \"visual\": {\n    \"layout\": \"array-with-pointers\",\n    \"theme\": {\n      \"primary\": \"#38bdf8\",\n      \"secondary\": \"#0284c7\"\n    },\n    \"components\": {\n      \"pointers\": [\n        { \"id\": \"left\", \"label\": \"L\", \"color\": \"#38bdf8\" },\n        { \"id\": \"right\", \"label\": \"R\", \"color\": \"#38bdf8\" },\n        { \"id\": \"mid\", \"label\": \"M\", \"color\": \"#f472b6\" }\n      ],\n      \"showIndices\": true,\n      \"showValues\": true,\n      \"highlightColor\": \"#fbbf24\",\n      \"foundColor\": \"#22c55e\",\n      \"dimColor\": \"rgba(160, 168, 192, 0.15)\",\n      \"compareColor\": \"#f472b6\"\n    }\n  },\n\n  \"inputs\": {\n    \"schema\": {\n      \"array\": {\n        \"type\": \"array\",\n        \"items\": { \"type\": \"number\", \"minimum\": -999, \"maximum\": 999 },\n        \"minItems\": 1,\n        \"maxItems\": 20,\n        \"description\": \"A sorted array of numbers. Must be in ascending order.\",\n        \"sorted\": true\n      },\n      \"target\": {\n        \"type\": \"number\",\n        \"minimum\": -999,\n        \"maximum\": 999,\n        \"description\": \"The value to search for.\"\n      }\n    },\n    \"defaults\": {\n      \"array\": [1, 3, 5, 7, 9, 11, 13, 15, 17, 19],\n      \"target\": 13\n    },\n    \"examples\": [\n      {\n        \"name\": \"Default (target found)\",\n        \"values\": { \"array\": [1, 3, 5, 7, 9, 11, 13, 15, 17, 19], \"target\": 13 }\n      },\n      {\n        \"name\": \"Target not found\",\n        \"values\": { \"array\": [2, 4, 6, 8, 10, 12, 14], \"target\": 5 }\n      },\n      {\n        \"name\": \"Single element (found)\",\n        \"values\": { \"array\": [42], \"target\": 42 }\n      }\n    ]\n  },\n\n  \"code\": {\n    \"implementations\": {\n      \"pseudocode\": \"function binarySearch(array, target):\\n  left = 0\\n  right = length(array) - 1\\n\\n  while left \u003C= right:\\n    mid = floor((left + right) / 2)\\n\\n    if array[mid] == target:\\n      return mid\\n\\n    if array[mid] \u003C target:\\n      left = mid + 1\\n    else:\\n      right = mid - 1\\n\\n  return -1\",\n      \"python\": \"def binary_search(array: list[int], target: int) -> int:\\n    left = 0\\n    right = len(array) - 1\\n\\n    while left \u003C= right:\\n        mid = (left + right) // 2\\n\\n        if array[mid] == target:\\n            return mid\\n\\n        if array[mid] \u003C target:\\n            left = mid + 1\\n        else:\\n            right = mid - 1\\n\\n    return -1\",\n      \"javascript\": \"function binarySearch(array, target) {\\n  let left = 0;\\n  let right = array.length - 1;\\n\\n  while (left \u003C= right) {\\n    const mid = Math.floor((left + right) / 2);\\n\\n    if (array[mid] === target) {\\n      return mid;\\n    }\\n\\n    if (array[mid] \u003C target) {\\n      left = mid + 1;\\n    } else {\\n      right = mid - 1;\\n    }\\n  }\\n\\n  return -1;\\n}\"\n    },\n    \"defaultLanguage\": \"pseudocode\"\n  },\n\n  \"education\": {\n    \"keyConcepts\": [\n      {\n        \"title\": \"Divide and Conquer\",\n        \"description\": \"Binary search works by repeatedly dividing the search interval in half.\"\n      },\n      {\n        \"title\": \"Sorted Input Requirement\",\n        \"description\": \"Binary search only works on sorted arrays.\"\n      },\n      {\n        \"title\": \"Logarithmic Time Complexity\",\n        \"description\": \"Each comparison eliminates half the remaining elements.\"\n      }\n    ],\n    \"pitfalls\": [\n      {\n        \"title\": \"Off-by-one in boundaries\",\n        \"description\": \"Using \u003C instead of \u003C= in the while condition causes bugs.\"\n      },\n      {\n        \"title\": \"Unsorted input\",\n        \"description\": \"Binary search silently produces wrong results on unsorted arrays.\"\n      }\n    ],\n    \"quiz\": [\n      {\n        \"question\": \"What is the maximum number of comparisons binary search needs for an array of 1024 elements?\",\n        \"options\": [\"10\", \"11\", \"32\", \"1024\"],\n        \"correctIndex\": 1,\n        \"explanation\": \"ceil(log2(1024)) = 10, plus one final check gives at most 11.\"\n      }\n    ],\n    \"resources\": [\n      {\n        \"title\": \"Binary Search - Wikipedia\",\n        \"url\": \"https://en.wikipedia.org/wiki/Binary_search_algorithm\",\n        \"type\": \"article\"\n      }\n    ]\n  },\n\n  \"seo\": {\n    \"keywords\": [\n      \"binary search visualization\",\n      \"binary search algorithm\",\n      \"binary search step by step\",\n      \"binary search animation\"\n    ],\n    \"ogDescription\": \"Interactive step-by-step visualization of binary search.\"\n  },\n\n  \"prerequisites\": [],\n  \"related\": [\"bubble-sort\", \"quicksort\", \"merge-sort\"],\n  \"author\": \"eigenvue\",\n  \"version\": \"1.0.0\"\n}\n```\n\n---\n\n## Validation\n\nThe `meta.json` file is validated at build time. The following rules are enforced:\n\n1. All required fields must be present.\n2. `id` must match the pattern `^[a-z0-9][a-z0-9-]*$` and must match the directory name.\n3. `category` must be one of the recognized category values.\n4. `complexity.level` must be one of the recognized difficulty levels.\n5. `visual.layout` must be a registered layout name.\n6. `inputs.defaults` must conform to `inputs.schema`.\n7. Each `inputs.examples[].values` must conform to `inputs.schema`.\n8. `code.implementations` must contain at least `pseudocode`.\n9. `code.defaultLanguage` must be a key in `code.implementations`.\n10. `education.keyConcepts` must have at least one entry.\n11. Each `quiz[].correctIndex` must be a valid index into the corresponding `options` array.\n12. Each `resources[].type` must be one of the recognized resource types.\n13. `seo.ogDescription` must not exceed 160 characters.\n14. `description.short` must not exceed 80 characters.\n15. `prerequisites` and `related` must reference valid algorithm IDs (checked across the registry).","src/content/docs/api-reference/meta-json-schema.mdx","a5b95ddfe48e6544","api-reference/python-api",{"id":265,"data":267,"body":273,"filePath":274,"digest":275,"deferredRender":16},{"title":268,"description":269,"editUrl":16,"head":270,"template":50,"sidebar":271,"pagefind":16,"draft":39},"Python Public API","Complete reference for the eigenvue Python package.",[],{"hidden":39,"attrs":272},{},"The `eigenvue` Python package provides a high-level interface for listing algorithms, generating step sequences, launching interactive visualizations, and embedding them in Jupyter notebooks. All four public functions are importable directly from the top-level `eigenvue` module.\n\n```python\nimport eigenvue\n```\n\n## Installation\n\n```bash\npip install eigenvue\n```\n\nThe package requires Python 3.10 or later.\n\n---\n\n## `eigenvue.list`\n\nReturns a list of all registered algorithms, optionally filtered by category.\n\n### Signature\n\n```python\ndef list(category: str | None = None) -> list[dict]\n```\n\n### Parameters\n\n| Parameter  | Type             | Required | Default | Description                                                                                                           |\n| ---------- | ---------------- | -------- | ------- | --------------------------------------------------------------------------------------------------------------------- |\n| `category` | `str \\| None`    | No       | `None`  | Filter by algorithm category. Valid values: `\"classical\"`, `\"deep-learning\"`, `\"generative-ai\"`, `\"quantum\"`. If `None`, returns all algorithms. |\n\n### Returns\n\n`list[dict]` -- A list of dictionaries, each containing the algorithm metadata fields `id`, `name`, `category`, `description`, and `complexity`.\n\n### Example\n\n```python\nimport eigenvue\n\n# List all algorithms\nall_algorithms = eigenvue.list()\nfor algo in all_algorithms:\n    print(f\"{algo['id']}: {algo['name']} ({algo['category']})\")\n\n# Filter by category\nclassical = eigenvue.list(category=\"classical\")\ngenai = eigenvue.list(category=\"generative-ai\")\n```\n\n### Example Return Value\n\n```python\n[\n    {\n        \"id\": \"binary-search\",\n        \"name\": \"Binary Search\",\n        \"category\": \"classical\",\n        \"description\": {\n            \"short\": \"Efficiently find a target in a sorted array by halving the search space.\",\n            \"long\": \"Binary search is a divide-and-conquer algorithm...\"\n        },\n        \"complexity\": {\n            \"time\": \"O(log n)\",\n            \"space\": \"O(1)\",\n            \"level\": \"beginner\"\n        }\n    },\n    # ... more algorithms\n]\n```\n\n### Exceptions\n\n| Exception      | Condition                                                    |\n| -------------- | ------------------------------------------------------------ |\n| `ValueError`   | `category` is not one of the recognized category strings.    |\n\n---\n\n## `eigenvue.show`\n\nLaunches an interactive visualization of the specified algorithm in a local web browser. Starts a lightweight local server on the given port and opens the algorithm page.\n\n### Signature\n\n```python\ndef show(\n    algorithm_id: str,\n    inputs: dict | None = None,\n    port: int | None = None,\n) -> None\n```\n\n### Parameters\n\n| Parameter      | Type            | Required | Default | Description                                                                                       |\n| -------------- | --------------- | -------- | ------- | ------------------------------------------------------------------------------------------------- |\n| `algorithm_id` | `str`           | Yes      | --      | The URL-safe identifier of the algorithm (e.g., `\"binary-search\"`, `\"self-attention\"`).          |\n| `inputs`       | `dict \\| None`  | No       | `None`  | Custom input parameters. Keys and values must match the algorithm's input schema. If `None`, the algorithm's default inputs from `meta.json` are used. |\n| `port`         | `int \\| None`   | No       | `None`  | Port number for the local server. If `None`, an available port is selected automatically.        |\n\n### Returns\n\n`None` -- The function opens a browser window and blocks until the server is stopped (e.g., via Ctrl+C).\n\n### Example\n\n```python\nimport eigenvue\n\n# Launch with default inputs\neigenvue.show(\"binary-search\")\n\n# Launch with custom inputs\neigenvue.show(\"binary-search\", inputs={\n    \"array\": [2, 4, 6, 8, 10, 12, 14, 16],\n    \"target\": 10,\n})\n\n# Specify a port\neigenvue.show(\"dijkstra\", port=8080)\n```\n\n### Exceptions\n\n| Exception        | Condition                                                                    |\n| ---------------- | ---------------------------------------------------------------------------- |\n| `ValueError`     | `algorithm_id` is not a recognized algorithm identifier.                     |\n| `ValueError`     | `inputs` does not conform to the algorithm's input schema.                   |\n| `OSError`        | The specified `port` is already in use.                                       |\n| `KeyboardInterrupt` | The user pressed Ctrl+C to stop the server (normal shutdown path).        |\n\n### Thread Safety\n\nThe local server runs in a background thread. Calling `show()` from multiple threads concurrently is not supported. If you need multiple visualizations, open them sequentially or use `jupyter()` for notebook embedding.\n\n---\n\n## `eigenvue.steps`\n\nGenerates the complete step sequence for an algorithm without launching a visualization. Useful for programmatic analysis, testing, and building custom integrations.\n\n### Signature\n\n```python\ndef steps(\n    algorithm_id: str,\n    inputs: dict | None = None,\n) -> list[dict]\n```\n\n### Parameters\n\n| Parameter      | Type            | Required | Default | Description                                                                                       |\n| -------------- | --------------- | -------- | ------- | ------------------------------------------------------------------------------------------------- |\n| `algorithm_id` | `str`           | Yes      | --      | The URL-safe identifier of the algorithm.                                                        |\n| `inputs`       | `dict \\| None`  | No       | `None`  | Custom input parameters. If `None`, the algorithm's default inputs are used.                     |\n\n### Returns\n\n`list[dict]` -- A list of step dictionaries conforming to the [Step Format Specification](/docs/api-reference/step-format). Each dictionary uses snake_case keys (Python convention). The list is guaranteed to be non-empty, with the last step having `is_terminal: True`.\n\n### Example\n\n```python\nimport eigenvue\n\n# Generate steps with default inputs\nresult = eigenvue.steps(\"binary-search\")\nprint(f\"Total steps: {len(result)}\")\n\n# Inspect individual steps\nfor s in result:\n    print(f\"Step {s['index']}: {s['title']}\")\n    print(f\"  Phase: {s.get('phase', 'N/A')}\")\n    print(f\"  Actions: {len(s['visual_actions'])}\")\n\n# Generate with custom inputs\nresult = eigenvue.steps(\"bubble-sort\", inputs={\n    \"array\": [5, 3, 8, 1, 2],\n})\n```\n\n### Example Return Value\n\n```python\n[\n    {\n        \"index\": 0,\n        \"id\": \"init\",\n        \"title\": \"Initialize Binary Search\",\n        \"explanation\": \"Set left = 0, right = 9, searching for target 13.\",\n        \"state\": {\n            \"array\": [1, 3, 5, 7, 9, 11, 13, 15, 17, 19],\n            \"target\": 13,\n            \"left\": 0,\n            \"right\": 9,\n            \"mid\": None,\n        },\n        \"visual_actions\": [\n            {\"type\": \"highlightRange\", \"from\": 0, \"to\": 9, \"color\": \"highlight\"},\n            {\"type\": \"movePointer\", \"id\": \"left\", \"to\": 0},\n            {\"type\": \"movePointer\", \"id\": \"right\", \"to\": 9},\n        ],\n        \"code_highlight\": {\"language\": \"pseudocode\", \"lines\": [2, 3]},\n        \"is_terminal\": False,\n        \"phase\": \"initialization\",\n    },\n    # ... more steps\n]\n```\n\n### Field Mapping\n\nThe Python API returns snake_case keys. The mapping from the TypeScript camelCase wire format is:\n\n| TypeScript (camelCase) | Python (snake_case)  |\n| ---------------------- | -------------------- |\n| `visualActions`        | `visual_actions`     |\n| `codeHighlight`        | `code_highlight`     |\n| `isTerminal`           | `is_terminal`        |\n\nNote that fields within `visual_actions` items retain their camelCase names (e.g., `nodeId`, `queryIdx`) because they are part of the visual action vocabulary, not the Step envelope.\n\n### Exceptions\n\n| Exception      | Condition                                                    |\n| -------------- | ------------------------------------------------------------ |\n| `ValueError`   | `algorithm_id` is not a recognized algorithm identifier.     |\n| `ValueError`   | `inputs` does not conform to the algorithm's input schema.   |\n| `RuntimeError` | The generator produced an invalid step sequence.             |\n\n### Thread Safety\n\n`steps()` is stateless and safe to call from multiple threads concurrently, provided each call operates on independent inputs. No global state is modified.\n\n---\n\n## `eigenvue.jupyter`\n\nRenders an interactive visualization inline in a Jupyter notebook. Returns an `IPython.display.IFrame` that embeds the Eigenvue visualization.\n\n### Signature\n\n```python\ndef jupyter(\n    algorithm_id: str,\n    inputs: dict | None = None,\n    width: int = 960,\n    height: int = 600,\n) -> IFrame\n```\n\n### Parameters\n\n| Parameter      | Type            | Required | Default | Description                                                                                       |\n| -------------- | --------------- | -------- | ------- | ------------------------------------------------------------------------------------------------- |\n| `algorithm_id` | `str`           | Yes      | --      | The URL-safe identifier of the algorithm.                                                        |\n| `inputs`       | `dict \\| None`  | No       | `None`  | Custom input parameters. If `None`, the algorithm's default inputs are used.                     |\n| `width`        | `int`           | No       | `960`   | Width of the embedded IFrame in pixels. Must be a positive integer.                              |\n| `height`       | `int`           | No       | `600`   | Height of the embedded IFrame in pixels. Must be a positive integer.                             |\n\n### Returns\n\n`IFrame` -- An `IPython.display.IFrame` object. In Jupyter notebooks, this renders automatically when returned as the last expression in a cell. It can also be displayed explicitly with `IPython.display.display()`.\n\n### Example\n\n```python\nimport eigenvue\n\n# Basic usage -- returns an IFrame that renders in the notebook\neigenvue.jupyter(\"binary-search\")\n```\n\n```python\n# Custom inputs and dimensions\neigenvue.jupyter(\n    \"self-attention\",\n    inputs={\"tokens\": [\"The\", \"cat\", \"sat\"]},\n    width=1200,\n    height=800,\n)\n```\n\n```python\n# Explicit display\nfrom IPython.display import display\n\niframe = eigenvue.jupyter(\"bubble-sort\", inputs={\"array\": [5, 3, 8, 1, 2]})\ndisplay(iframe)\n```\n\n### Exceptions\n\n| Exception        | Condition                                                    |\n| ---------------- | ------------------------------------------------------------ |\n| `ValueError`     | `algorithm_id` is not a recognized algorithm identifier.     |\n| `ValueError`     | `inputs` does not conform to the algorithm's input schema.   |\n| `ValueError`     | `width` or `height` is not a positive integer.               |\n| `ImportError`    | IPython is not installed (required for IFrame support).       |\n\n### Environment Requirements\n\nThis function requires a Jupyter-compatible environment (Jupyter Notebook, JupyterLab, Google Colab, or VS Code with the Jupyter extension). The `IPython` package must be installed. In non-Jupyter environments, use `show()` instead.\n\n---\n\n## Version\n\nThe current package version is accessible via:\n\n```python\nimport eigenvue\nprint(eigenvue.__version__)  # \"0.1.0\"\n```\n\n---\n\n## Error Handling Summary\n\nAll public functions validate their inputs eagerly and raise descriptive exceptions. The general pattern:\n\n```python\nimport eigenvue\n\ntry:\n    result = eigenvue.steps(\"nonexistent-algorithm\")\nexcept ValueError as e:\n    print(f\"Invalid input: {e}\")\nexcept RuntimeError as e:\n    print(f\"Generator failure: {e}\")\n```\n\nAll exceptions include the algorithm ID and a description of what went wrong.","src/content/docs/api-reference/python-api.mdx","4fd35691f7eed757","api-reference/step-format",{"id":276,"data":278,"body":284,"filePath":285,"digest":286,"deferredRender":16},{"title":279,"description":280,"editUrl":16,"head":281,"template":50,"sidebar":282,"pagefind":16,"draft":39},"Step Format Specification","The universal data contract.",[],{"hidden":39,"attrs":283},{},"The Step Format is the universal contract between algorithm generators and the rendering engine. Every generator -- whether written in TypeScript or Python -- produces an array of `Step` objects. Every renderer consumes `Step` objects without knowing which generator produced them. This separation is the foundation of Eigenvue's architecture.\n\n**Format Version: 1.0.0**\n\nModule: `@eigenvue/step-format`\n\n---\n\n## Design Principles\n\n1. **JSON-serializable**: All types must be serializable to JSON. No functions, no classes, no circular references, no `undefined` values.\n2. **Open vocabulary for visual actions**: The `type` field on visual actions is a plain string, not a closed enum. Renderers silently ignore action types they do not recognize.\n3. **Contiguous indexing**: Step indices are 0-based with no gaps.\n4. **Single terminal**: The last step in any sequence must have `isTerminal === true`, and only the last step.\n\n---\n\n## Step\n\nA single step in an algorithm's execution trace. Steps are the atomic unit of the platform: a generator produces an ordered array of steps, and the renderer displays one at a time.\n\n```typescript\ninterface Step {\n  readonly index: number;\n  readonly id: string;\n  readonly title: string;\n  readonly explanation: string;\n  readonly state: Readonly\u003CRecord\u003Cstring, unknown>>;\n  readonly visualActions: readonly VisualAction[];\n  readonly codeHighlight: CodeHighlight;\n  readonly isTerminal: boolean;\n  readonly phase?: string;\n}\n```\n\n### Fields\n\n| Field           | Type                                   | Required | Description                                                                                                                 |\n| --------------- | -------------------------------------- | -------- | --------------------------------------------------------------------------------------------------------------------------- |\n| `index`         | `number`                               | Yes      | 0-based position of this step in the sequence. **Invariant**: `steps[i].index === i`.                                       |\n| `id`            | `string`                               | Yes      | Template identifier (e.g., `\"compare_mid\"`, `\"found\"`). Must match `^[a-z0-9][a-z0-9_-]*$`. Not unique within a sequence -- the same id may recur across loop iterations. Used for analytics, bookmarking, and debugging. |\n| `title`         | `string`                               | Yes      | Short, human-readable heading. Recommended maximum: 80 characters. Shown in the step heading area of the UI.                |\n| `explanation`   | `string`                               | Yes      | Plain-language narration. Should be self-contained enough that a learner understands the step without reading the code. May reference concrete values (e.g., \"Comparing array[3]=7 with target 5\"). |\n| `state`         | `Readonly\u003CRecord\u003Cstring, unknown>>`    | Yes      | Snapshot of all algorithm variables at this point. Must be a flat or nested JSON-serializable object. No `undefined`, no functions, no class instances, no symbols. |\n| `visualActions` | `readonly VisualAction[]`              | Yes      | Ordered array of visual rendering instructions. Processed in array order. An empty array is valid (step has state/code changes but no visual effect). |\n| `codeHighlight` | `CodeHighlight`                        | Yes      | Identifies which source code lines correspond to this step.                                                                 |\n| `isTerminal`    | `boolean`                              | Yes      | `true` if and only if this is the final step. **Invariant**: Exactly one step has `isTerminal === true`, and it is `steps[steps.length - 1]`. |\n| `phase`         | `string`                               | No       | Optional grouping label (e.g., `\"initialization\"`, `\"search\"`, `\"result\"`). Consecutive steps with the same phase are visually grouped in the UI. |\n\n### Invariants\n\nThese invariants are enforced by the generator runner and tested by CI:\n\n1. `steps[i].index === i` for all `i` in `[0, steps.length)`.\n2. `id` is a non-empty string matching `^[a-z0-9][a-z0-9_-]*$`.\n3. `title` is non-empty and at most 200 characters.\n4. `state` contains only JSON-serializable values.\n5. The last step (and only the last step) has `isTerminal === true`.\n\n---\n\n## CodeHighlight\n\nIdentifies which lines of source code correspond to the current step. The code panel in the visualizer highlights these lines.\n\n```typescript\ninterface CodeHighlight {\n  readonly language: string;\n  readonly lines: readonly number[];\n}\n```\n\n| Field      | Type               | Required | Description                                                                                         |\n| ---------- | ------------------ | -------- | --------------------------------------------------------------------------------------------------- |\n| `language` | `string`           | Yes      | Which language tab to highlight. Must be one of the keys in the algorithm's `code.implementations` map from `meta.json` (e.g., `\"pseudocode\"`, `\"python\"`, `\"javascript\"`). |\n| `lines`    | `readonly number[]` | Yes     | 1-indexed line numbers to highlight. Must be a non-empty array of positive integers. Order does not matter; the renderer highlights all listed lines. |\n\n### Example\n\n```json\n{\n  \"language\": \"pseudocode\",\n  \"lines\": [5, 6]\n}\n```\n\n---\n\n## VisualAction\n\nA single instruction telling the renderer what to display for a step.\n\n```typescript\ninterface VisualAction {\n  readonly type: string;\n  readonly [key: string]: unknown;\n}\n```\n\n| Field   | Type      | Required | Description                                                                                |\n| ------- | --------- | -------- | ------------------------------------------------------------------------------------------ |\n| `type`  | `string`  | Yes      | The action type identifier in camelCase (e.g., `\"highlightElement\"`, `\"movePointer\"`). Renderers silently ignore unrecognized types. |\n| `[key]` | `unknown` | Varies   | Action-specific parameters. All values must be JSON-serializable.                          |\n\nVisual actions use an **open vocabulary**. The `type` field is a plain string, not a closed enum. This means new action types (e.g., for quantum computing) can be defined in generators before renderer support exists. Renderers must silently ignore action types they do not recognize.\n\nSee the [Visual Action Types](/docs/api-reference/visual-actions) reference for the complete catalog of known action types.\n\n---\n\n## StepSequence\n\nA complete, validated sequence of steps produced by a generator. This is the top-level structure that flows from generators to renderers.\n\n```typescript\ninterface StepSequence {\n  readonly formatVersion: number;\n  readonly algorithmId: string;\n  readonly inputs: Readonly\u003CRecord\u003Cstring, unknown>>;\n  readonly steps: readonly Step[];\n  readonly generatedAt: string;\n  readonly generatedBy: \"typescript\" | \"python\" | \"precomputed\";\n}\n```\n\n| Field           | Type                                    | Required | Description                                                                                          |\n| --------------- | --------------------------------------- | -------- | ---------------------------------------------------------------------------------------------------- |\n| `formatVersion` | `number`                                | Yes      | The step format major version. Currently `1`. Renderers should check this and refuse incompatible versions. |\n| `algorithmId`   | `string`                                | Yes      | The algorithm ID this sequence was generated for. Must match the `id` in `meta.json`. Pattern: `^[a-z0-9][a-z0-9-]*$`. |\n| `inputs`        | `Readonly\u003CRecord\u003Cstring, unknown>>`     | Yes      | The input parameters that produced this sequence. Stored for reproducibility.                        |\n| `steps`         | `readonly Step[]`                       | Yes      | The ordered array of steps. Must contain at least 1 step. `steps[i].index === i`. `steps[steps.length - 1].isTerminal === true`. |\n| `generatedAt`   | `string`                                | Yes      | ISO 8601 timestamp of generation (e.g., `\"2026-02-14T10:30:00.000Z\"`).                              |\n| `generatedBy`   | `\"typescript\" \\| \"python\" \\| \"precomputed\"` | Yes  | Which generator produced this sequence.                                                              |\n\n---\n\n## JSON Schema Reference\n\nThe step format can be validated against the following JSON Schema (draft 2020-12). This schema covers the `StepSequence` envelope and all nested types.\n\n```json\n{\n  \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n  \"$id\": \"https://eigenvue.com/schemas/step-sequence.json\",\n  \"title\": \"StepSequence\",\n  \"description\": \"A complete step sequence produced by an Eigenvue generator.\",\n  \"type\": \"object\",\n  \"required\": [\"formatVersion\", \"algorithmId\", \"inputs\", \"steps\", \"generatedAt\", \"generatedBy\"],\n  \"properties\": {\n    \"formatVersion\": {\n      \"type\": \"integer\",\n      \"const\": 1,\n      \"description\": \"Step format major version.\"\n    },\n    \"algorithmId\": {\n      \"type\": \"string\",\n      \"pattern\": \"^[a-z0-9][a-z0-9-]*$\",\n      \"description\": \"The algorithm identifier.\"\n    },\n    \"inputs\": {\n      \"type\": \"object\",\n      \"description\": \"The input parameters that produced this sequence.\"\n    },\n    \"steps\": {\n      \"type\": \"array\",\n      \"minItems\": 1,\n      \"items\": { \"$ref\": \"#/$defs/Step\" },\n      \"description\": \"Ordered array of steps.\"\n    },\n    \"generatedAt\": {\n      \"type\": \"string\",\n      \"format\": \"date-time\",\n      \"description\": \"ISO 8601 generation timestamp.\"\n    },\n    \"generatedBy\": {\n      \"type\": \"string\",\n      \"enum\": [\"typescript\", \"python\", \"precomputed\"],\n      \"description\": \"Which generator produced this sequence.\"\n    }\n  },\n  \"$defs\": {\n    \"Step\": {\n      \"type\": \"object\",\n      \"required\": [\"index\", \"id\", \"title\", \"explanation\", \"state\", \"visualActions\", \"codeHighlight\", \"isTerminal\"],\n      \"properties\": {\n        \"index\": { \"type\": \"integer\", \"minimum\": 0 },\n        \"id\": { \"type\": \"string\", \"pattern\": \"^[a-z0-9][a-z0-9_-]*$\" },\n        \"title\": { \"type\": \"string\", \"minLength\": 1, \"maxLength\": 200 },\n        \"explanation\": { \"type\": \"string\", \"minLength\": 1 },\n        \"state\": { \"type\": \"object\" },\n        \"visualActions\": {\n          \"type\": \"array\",\n          \"items\": { \"$ref\": \"#/$defs/VisualAction\" }\n        },\n        \"codeHighlight\": { \"$ref\": \"#/$defs/CodeHighlight\" },\n        \"isTerminal\": { \"type\": \"boolean\" },\n        \"phase\": { \"type\": \"string\" }\n      }\n    },\n    \"VisualAction\": {\n      \"type\": \"object\",\n      \"required\": [\"type\"],\n      \"properties\": {\n        \"type\": { \"type\": \"string\", \"minLength\": 1 }\n      },\n      \"additionalProperties\": true\n    },\n    \"CodeHighlight\": {\n      \"type\": \"object\",\n      \"required\": [\"language\", \"lines\"],\n      \"properties\": {\n        \"language\": { \"type\": \"string\", \"minLength\": 1 },\n        \"lines\": {\n          \"type\": \"array\",\n          \"items\": { \"type\": \"integer\", \"minimum\": 1 }\n        }\n      }\n    }\n  }\n}\n```\n\n---\n\n## Versioning\n\nThe step format uses a single integer version number (`formatVersion`), currently `1`. This represents the major version of the format.\n\n### Version Compatibility\n\n- **Renderers** should check `formatVersion` before processing a step sequence. If the version is higher than what the renderer supports, it should display an appropriate error rather than silently producing incorrect output.\n- **Generators** always produce steps at the current version.\n- **Breaking changes** (adding required fields, removing fields, changing field semantics) require incrementing the version.\n- **Additive changes** (new optional fields, new visual action types) do not require a version bump because renderers already ignore unknown fields and unknown action types.\n\nThe version constant is exported from the TypeScript module:\n\n```typescript\nimport { STEP_FORMAT_VERSION } from \"@/shared/types/step\";\n// STEP_FORMAT_VERSION === 1\n```\n\n---\n\n## Serialization Rules\n\nAll step data must conform to these serialization rules to ensure compatibility across TypeScript and Python generators and between server and client.\n\n### JSON Constraints\n\n1. All values must be JSON-serializable: `number`, `string`, `boolean`, `null`, arrays of these, or nested objects of these.\n2. `undefined` is **not** valid. Use `null` to represent absent values.\n3. No functions, class instances, `Symbol`, `BigInt`, `Date` objects, or circular references.\n4. Numbers must be finite. `NaN`, `Infinity`, and `-Infinity` are not valid JSON and must not appear.\n5. Arrays must not be sparse (no holes).\n\n### Wire Format\n\nThe JSON wire format uses **camelCase** field names (TypeScript convention). This is the canonical serialized form used for:\n\n- Pre-computed step files stored on disk\n- Data transfer between the generator and renderer\n- The `StepSequence` output of `runGenerator()`\n\n---\n\n## TypeScript to Python Field Mapping\n\nWhen step sequences cross the TypeScript/Python boundary, the **Step envelope** fields are converted between camelCase and snake_case. Fields **inside** visual actions retain camelCase because they are part of the visual action vocabulary, not the Step envelope.\n\n### Step Envelope Fields\n\n| TypeScript (camelCase) | Python (snake_case) | Type                                   |\n| ---------------------- | ------------------- | -------------------------------------- |\n| `index`                | `index`             | `number` / `int`                       |\n| `id`                   | `id`                | `string` / `str`                       |\n| `title`                | `title`             | `string` / `str`                       |\n| `explanation`          | `explanation`       | `string` / `str`                       |\n| `state`                | `state`             | `Record\u003Cstring, unknown>` / `dict`     |\n| `visualActions`        | `visual_actions`    | `VisualAction[]` / `list[dict]`        |\n| `codeHighlight`        | `code_highlight`    | `CodeHighlight` / `dict`               |\n| `isTerminal`           | `is_terminal`       | `boolean` / `bool`                     |\n| `phase`                | `phase`             | `string?` / `str \\| None`             |\n\n### CodeHighlight Fields\n\n| TypeScript (camelCase) | Python (snake_case) |\n| ---------------------- | ------------------- |\n| `language`             | `language`          |\n| `lines`               | `lines`             |\n\n### StepSequence Envelope Fields\n\n| TypeScript (camelCase) | Python (snake_case) |\n| ---------------------- | ------------------- |\n| `formatVersion`        | `format_version`    |\n| `algorithmId`          | `algorithm_id`      |\n| `inputs`               | `inputs`            |\n| `steps`                | `steps`             |\n| `generatedAt`          | `generated_at`      |\n| `generatedBy`          | `generated_by`      |\n\n### Visual Action Fields\n\nVisual action fields are **not** converted. They remain in camelCase regardless of language context. For example, `nodeId`, `queryIdx`, `fromLayer`, `toLayer`, `kernelHeight`, and `kernelWidth` are the same in both TypeScript and Python.\n\n```python\n# Python step with visual actions -- note camelCase inside actions\nstep = {\n    \"index\": 0,\n    \"id\": \"visit\",\n    \"title\": \"Visit Node A\",\n    \"explanation\": \"Visiting node A.\",\n    \"state\": {\"visited\": [\"A\"]},\n    \"visual_actions\": [\n        {\"type\": \"visitNode\", \"nodeId\": \"A\", \"color\": \"visited\"},  # camelCase\n    ],\n    \"code_highlight\": {\"language\": \"pseudocode\", \"lines\": [3]},\n    \"is_terminal\": False,\n}\n```\n\n---\n\n## Complete Step Example\n\n```json\n{\n  \"formatVersion\": 1,\n  \"algorithmId\": \"binary-search\",\n  \"inputs\": {\n    \"array\": [1, 3, 5, 7, 9, 11, 13, 15, 17, 19],\n    \"target\": 13\n  },\n  \"steps\": [\n    {\n      \"index\": 0,\n      \"id\": \"init\",\n      \"title\": \"Initialize Binary Search\",\n      \"explanation\": \"Set left = 0, right = 9. The entire array is the initial search space.\",\n      \"state\": {\n        \"array\": [1, 3, 5, 7, 9, 11, 13, 15, 17, 19],\n        \"target\": 13,\n        \"left\": 0,\n        \"right\": 9,\n        \"mid\": null\n      },\n      \"visualActions\": [\n        { \"type\": \"highlightRange\", \"from\": 0, \"to\": 9, \"color\": \"highlight\" },\n        { \"type\": \"movePointer\", \"id\": \"left\", \"to\": 0 },\n        { \"type\": \"movePointer\", \"id\": \"right\", \"to\": 9 }\n      ],\n      \"codeHighlight\": { \"language\": \"pseudocode\", \"lines\": [2, 3] },\n      \"isTerminal\": false,\n      \"phase\": \"initialization\"\n    },\n    {\n      \"index\": 1,\n      \"id\": \"compare_mid\",\n      \"title\": \"Compare Middle Element\",\n      \"explanation\": \"mid = floor((0 + 9) / 2) = 4. array[4] = 9. Since 9 \u003C 13, search the right half.\",\n      \"state\": {\n        \"array\": [1, 3, 5, 7, 9, 11, 13, 15, 17, 19],\n        \"target\": 13,\n        \"left\": 0,\n        \"right\": 9,\n        \"mid\": 4\n      },\n      \"visualActions\": [\n        { \"type\": \"movePointer\", \"id\": \"mid\", \"to\": 4 },\n        { \"type\": \"highlightElement\", \"index\": 4, \"color\": \"compare\" },\n        { \"type\": \"compareElements\", \"i\": 4, \"j\": -1, \"result\": \"less\" },\n        { \"type\": \"dimRange\", \"from\": 0, \"to\": 4 }\n      ],\n      \"codeHighlight\": { \"language\": \"pseudocode\", \"lines\": [5, 6, 10, 11] },\n      \"isTerminal\": false,\n      \"phase\": \"search\"\n    }\n  ],\n  \"generatedAt\": \"2026-02-14T10:30:00.000Z\",\n  \"generatedBy\": \"typescript\"\n}\n```","src/content/docs/api-reference/step-format.mdx","ec9747db4e5a2abb","api-reference/typescript-generator-api",{"id":287,"data":289,"body":295,"filePath":296,"digest":297,"deferredRender":16},{"title":290,"description":291,"editUrl":16,"head":292,"template":50,"sidebar":293,"pagefind":16,"draft":39},"TypeScript Generator API","Reference for the generator framework.",[],{"hidden":39,"attrs":294},{},"The TypeScript Generator API provides the framework for writing algorithm generators that produce step sequences. Generators are JavaScript generator functions (`function*`) that yield `Step` objects via a provided step builder. The runner executes these generators, validates the output, and packages it into a `StepSequence`.\n\nAll exports are available from `@/engine/generator`:\n\n```typescript\nimport {\n  createGenerator,\n  runGenerator,\n  GeneratorError,\n} from \"@/engine/generator\";\n\nimport type {\n  StepInput,\n  StepBuilderFn,\n  GeneratorFunction,\n  GeneratorDefinition,\n  CreateGeneratorConfig,\n  RunOptions,\n  Step,\n  VisualAction,\n  CodeHighlight,\n  StepSequence,\n} from \"@/engine/generator\";\n```\n\n---\n\n## `createGenerator`\n\nFactory function that creates a type-safe `GeneratorDefinition`. This is the primary entry point for defining a new algorithm generator.\n\n### Signature\n\n```typescript\nfunction createGenerator\u003CTInputs extends Record\u003Cstring, unknown>>(\n  config: CreateGeneratorConfig\u003CTInputs>,\n): GeneratorDefinition\u003CTInputs>\n```\n\n### Parameters\n\n| Parameter | Type                             | Description                                           |\n| --------- | -------------------------------- | ----------------------------------------------------- |\n| `config`  | `CreateGeneratorConfig\u003CTInputs>` | The algorithm ID and generator function. See below.   |\n\n### Returns\n\n`GeneratorDefinition\u003CTInputs>` -- An object that the runner can execute.\n\n### Throws\n\n- `Error` if `config.id` does not match the pattern `^[a-z0-9][a-z0-9-]*$`.\n\n### Example\n\n```typescript\nimport { createGenerator } from \"@/engine/generator\";\n\ninterface BinarySearchInputs {\n  array: number[];\n  target: number;\n}\n\nexport default createGenerator\u003CBinarySearchInputs>({\n  id: \"binary-search\",\n  *generate(inputs, step) {\n    const { array, target } = inputs;\n    let left = 0;\n    let right = array.length - 1;\n\n    yield step({\n      id: \"init\",\n      title: \"Initialize Binary Search\",\n      explanation: `Set left = 0, right = ${right}, searching for ${target}.`,\n      state: { array: [...array], target, left, right, mid: null },\n      visualActions: [\n        { type: \"highlightRange\", from: 0, to: right, color: \"highlight\" },\n        { type: \"movePointer\", id: \"left\", to: 0 },\n        { type: \"movePointer\", id: \"right\", to: right },\n      ],\n      codeHighlight: { language: \"pseudocode\", lines: [2, 3] },\n      phase: \"initialization\",\n    });\n\n    // ... search loop steps ...\n\n    yield step({\n      id: \"result\",\n      title: \"Search Complete\",\n      explanation: \"The algorithm has finished.\",\n      state: { array: [...array], target, left, right, mid: null },\n      visualActions: [],\n      codeHighlight: { language: \"pseudocode\", lines: [15] },\n      isTerminal: true,\n      phase: \"result\",\n    });\n  },\n});\n```\n\n---\n\n## `runGenerator`\n\nExecutes a generator definition with the given inputs and returns a validated `StepSequence`.\n\n### Signature\n\n```typescript\nfunction runGenerator\u003CTInputs extends Record\u003Cstring, unknown>>(\n  definition: GeneratorDefinition\u003CTInputs>,\n  inputs: TInputs,\n  options?: RunOptions,\n): StepSequence\n```\n\n### Parameters\n\n| Parameter    | Type                           | Required | Description                                                        |\n| ------------ | ------------------------------ | -------- | ------------------------------------------------------------------ |\n| `definition` | `GeneratorDefinition\u003CTInputs>` | Yes      | The generator definition (from `createGenerator()`).               |\n| `inputs`     | `TInputs`                      | Yes      | The algorithm's input parameters.                                  |\n| `options`    | `RunOptions`                   | No       | Optional configuration. See `RunOptions` below.                    |\n\n### Returns\n\n`StepSequence` -- A complete, validated step sequence ready for rendering.\n\n### Throws\n\n`GeneratorError` in the following cases:\n\n- The generator function itself throws an exception.\n- The generator exceeds the `maxSteps` limit.\n- The generated steps violate any step format invariant (empty sequence, non-contiguous indices, missing terminal step, multiple terminal steps, terminal step not last).\n- A visual action violates its constraints (e.g., `highlightRange` with `from > to`, `showAttentionWeights` with weights not summing to 1.0).\n\n### Invariants Enforced\n\nThe runner validates the complete step sequence before returning:\n\n1. **Non-empty**: At least one step must exist.\n2. **Index contiguity**: `steps[i].index === i` for all `i`.\n3. **Single terminal**: Exactly one step has `isTerminal === true`.\n4. **Terminal is last**: The terminal step is `steps[steps.length - 1]`.\n5. **Visual action constraints**: Action-specific rules (range ordering, attention weight summation, label/value length matching).\n\n### Example\n\n```typescript\nimport binarySearchGenerator from \"@/algorithms/classical/binary-search/generator\";\nimport { runGenerator } from \"@/engine/generator\";\n\nconst result = runGenerator(binarySearchGenerator, {\n  array: [1, 3, 5, 7, 9, 11, 13],\n  target: 7,\n});\n\nconsole.log(result.formatVersion);  // 1\nconsole.log(result.algorithmId);    // \"binary-search\"\nconsole.log(result.steps.length);   // e.g., 5\nconsole.log(result.generatedBy);    // \"typescript\"\nconsole.log(result.steps[result.steps.length - 1].isTerminal); // true\n```\n\n---\n\n## `RunOptions`\n\nConfiguration object for `runGenerator()`.\n\n```typescript\ninterface RunOptions {\n  readonly maxSteps?: number;\n}\n```\n\n| Field      | Type     | Default   | Description                                                                                      |\n| ---------- | -------- | --------- | ------------------------------------------------------------------------------------------------ |\n| `maxSteps` | `number` | `10_000`  | Maximum number of steps the generator may produce. Exceeding this limit throws `GeneratorError`. This is a safety limit to prevent infinite loops from locking the browser. |\n\n### Example\n\n```typescript\n// Allow up to 50,000 steps for a complex algorithm\nconst result = runGenerator(complexGenerator, inputs, {\n  maxSteps: 50_000,\n});\n```\n\n---\n\n## `GeneratorDefinition`\n\nThe output of `createGenerator()`. Bundles the algorithm ID with its generate function.\n\n```typescript\ninterface GeneratorDefinition\u003CTInputs extends Record\u003Cstring, unknown>> {\n  readonly id: string;\n  readonly generate: GeneratorFunction\u003CTInputs>;\n}\n```\n\n| Field      | Type                        | Description                                            |\n| ---------- | --------------------------- | ------------------------------------------------------ |\n| `id`       | `string`                    | The algorithm's URL-safe identifier (e.g., `\"binary-search\"`). Must match the `id` field in the algorithm's `meta.json`. |\n| `generate` | `GeneratorFunction\u003CTInputs>` | The generator function.                               |\n\n---\n\n## `GeneratorFunction`\n\nThe type signature for generator functions.\n\n```typescript\ntype GeneratorFunction\u003CTInputs extends Record\u003Cstring, unknown>> =\n  (inputs: TInputs, step: StepBuilderFn) => Generator\u003CStep, void, undefined>;\n```\n\nA generator function receives:\n\n1. `inputs` -- The algorithm's typed input parameters.\n2. `step` -- The step builder function (see `StepBuilderFn` below).\n\nIt must be a JavaScript generator function (`function*`) that yields `Step` objects created via the `step` builder.\n\n### Rules\n\n1. The generator **must** yield at least one step.\n2. The last yielded step **must** have `isTerminal: true`.\n3. No step other than the last may have `isTerminal: true`.\n4. The generator **must** terminate (no infinite loops).\n5. State objects in steps **must** be independent snapshots (deep copies of mutable data).\n\n---\n\n## `StepBuilderFn`\n\nThe function signature provided to generators for creating steps.\n\n```typescript\ntype StepBuilderFn = (input: StepInput) => Step;\n```\n\nThe step builder is provided by the runner. Generators call it to convert a `StepInput` into a complete `Step` object. The builder automatically:\n\n1. Assigns the correct `index` (auto-incrementing from 0).\n2. Sets `isTerminal` to `false` if not provided.\n3. Validates the step ID format, title length, and code highlight line numbers.\n\nGenerators should never construct `Step` objects directly. Always use the step builder.\n\n---\n\n## `StepInput`\n\nThe fields a generator provides when yielding a step. This is the input to the step builder function.\n\n```typescript\ninterface StepInput {\n  readonly id: string;\n  readonly title: string;\n  readonly explanation: string;\n  readonly state: Record\u003Cstring, unknown>;\n  readonly visualActions: readonly VisualAction[];\n  readonly codeHighlight: CodeHighlight;\n  readonly isTerminal?: boolean;\n  readonly phase?: string;\n}\n```\n\n| Field           | Type                       | Required | Description                                                              |\n| --------------- | -------------------------- | -------- | ------------------------------------------------------------------------ |\n| `id`            | `string`                   | Yes      | Template identifier (e.g., `\"compare_mid\"`). Must match `^[a-z0-9][a-z0-9_-]*$`. Not unique within a sequence. |\n| `title`         | `string`                   | Yes      | Short heading (1--200 characters).                                       |\n| `explanation`   | `string`                   | Yes      | Plain-language narration of what is happening.                           |\n| `state`         | `Record\u003Cstring, unknown>`  | Yes      | Snapshot of all algorithm variables. **Must be an independent copy** -- never pass mutable references. |\n| `visualActions` | `readonly VisualAction[]`  | Yes      | Ordered array of rendering instructions.                                 |\n| `codeHighlight` | `CodeHighlight`            | Yes      | Source code line mapping. Lines are 1-indexed positive integers.          |\n| `isTerminal`    | `boolean`                  | No       | Set to `true` only on the final step. Defaults to `false`.               |\n| `phase`         | `string`                   | No       | Grouping label (e.g., `\"initialization\"`, `\"search\"`, `\"result\"`).       |\n\n### State Snapshot Warning\n\nThe `state` field must contain an independent snapshot of the algorithm's variables. If you pass a reference to a mutable object, all steps will point to the same (mutated) object, producing incorrect behavior when the user steps backward.\n\n```typescript\n// CORRECT: spread creates a shallow copy\nyield step({\n  state: { array: [...array], target, left, right, mid },\n  // ...\n});\n\n// CORRECT: structuredClone for deeply nested state\nyield step({\n  state: structuredClone({ graph, visited, distances }),\n  // ...\n});\n\n// WRONG: array is a mutable reference\nyield step({\n  state: { array, target, left, right },\n  // ...\n});\n```\n\n---\n\n## `CreateGeneratorConfig`\n\nConfiguration object passed to `createGenerator()`.\n\n```typescript\ninterface CreateGeneratorConfig\u003CTInputs extends Record\u003Cstring, unknown>> {\n  readonly id: string;\n  readonly generate: GeneratorFunction\u003CTInputs>;\n}\n```\n\n| Field      | Type                          | Description                                                                      |\n| ---------- | ----------------------------- | -------------------------------------------------------------------------------- |\n| `id`       | `string`                      | URL-safe algorithm identifier. Must match `^[a-z0-9][a-z0-9-]*$`. Must match the `id` in the algorithm's `meta.json`. |\n| `generate` | `GeneratorFunction\u003CTInputs>`  | The generator function (must use `function*` syntax).                            |\n\n---\n\n## `GeneratorError`\n\nError class thrown when a generator produces invalid output or encounters a runtime failure. Includes structured context for debugging.\n\n```typescript\nclass GeneratorError extends Error {\n  readonly algorithmId: string;\n  readonly stepIndex: number;\n  readonly cause?: Error;\n\n  constructor(\n    algorithmId: string,\n    stepIndex: number,\n    message: string,\n    cause?: Error,\n  );\n}\n```\n\n| Property      | Type     | Description                                                                     |\n| ------------- | -------- | ------------------------------------------------------------------------------- |\n| `algorithmId` | `string` | The algorithm that caused the error.                                            |\n| `stepIndex`   | `number` | The step index at which the error occurred. `-1` if before any step was created.|\n| `cause`       | `Error`  | The original error, if this wraps an unexpected exception.                      |\n| `name`        | `string` | Always `\"GeneratorError\"`.                                                      |\n| `message`     | `string` | Formatted as `[algorithmId] Step stepIndex: description`.                       |\n\n### Error Message Format\n\n```\n[binary-search] Step 3: Invalid step ID \"COMPARE\". Must match pattern: ^[a-z0-9][a-z0-9_-]*$\n```\n\n### Example\n\n```typescript\nimport { runGenerator, GeneratorError } from \"@/engine/generator\";\n\ntry {\n  const result = runGenerator(myGenerator, inputs);\n} catch (error) {\n  if (error instanceof GeneratorError) {\n    console.error(`Algorithm: ${error.algorithmId}`);\n    console.error(`Failed at step: ${error.stepIndex}`);\n    console.error(`Reason: ${error.message}`);\n    if (error.cause) {\n      console.error(`Original error:`, error.cause);\n    }\n  }\n}\n```\n\n---\n\n## Re-exported Types\n\nThe generator module re-exports these types from `@/shared/types/step` for convenience, so generators can import everything from a single location:\n\n| Type            | Description                                       |\n| --------------- | ------------------------------------------------- |\n| `Step`          | A single step in an algorithm's execution trace.  |\n| `VisualAction`  | A rendering instruction within a step.            |\n| `CodeHighlight` | Source code line mapping for a step.               |\n| `StepSequence`  | A complete, validated sequence of steps.           |\n\nSee the [Step Format Specification](/docs/api-reference/step-format) for full documentation of these types.","src/content/docs/api-reference/typescript-generator-api.mdx","a7afe153418e23d2","architecture/design-decisions",{"id":298,"data":300,"body":306,"filePath":307,"digest":308,"deferredRender":16},{"title":301,"description":302,"editUrl":16,"head":303,"template":50,"sidebar":304,"pagefind":16,"draft":39},"Design Decisions","Core architectural decisions and rationale.",[],{"hidden":39,"attrs":305},{},"This document records the six core architectural decisions that shape Eigenvue. For each decision, we describe what was decided, why, and what consequences follow.\n\n---\n\n## 1. Algorithms Are Code, Not Config\n\n### Decision\n\nAlgorithm logic lives in generator functions written in TypeScript (and Python), not in declarative configuration files or domain-specific languages. The `meta.json` file stores metadata only -- display names, descriptions, input schemas, educational content, SEO keywords -- never algorithm logic.\n\n### Rationale\n\n- **Expressiveness.** Algorithms require loops, conditionals, recursion, and data structures. Expressing this in a configuration language would require inventing a Turing-complete DSL, which is just a worse programming language.\n- **Familiarity.** Contributors already know TypeScript and Python. There is no new language to learn.\n- **Tooling.** Generators get full IDE support: type checking, autocompletion, debugging, testing. Configuration files get none of this.\n- **Testability.** A generator is a pure function (inputs in, steps out) that can be unit tested with standard testing frameworks. No rendering engine needed.\n\n### Consequences\n\n- Every algorithm requires a `generator.ts` file with actual code, not just a `meta.json`.\n- Contributors must understand the generator API (the `step()` builder, the `StepInput` shape, the visual action vocabulary).\n- The barrier to contribution is slightly higher than a purely declarative system, but the expressiveness gained is substantial.\n- Algorithms can implement genuinely complex logic (e.g., recursive merge sort, transformer attention with matrix operations) that would be impractical in a configuration format.\n\n---\n\n## 2. Step Format Is the Stable API\n\n### Decision\n\nThe `Step` interface is the stable contract between generators and renderers. Generators produce `Step[]`. Renderers consume `Step[]`. Neither side depends on the other's internals.\n\n### Rationale\n\n- **Decoupling.** Generators can be rewritten, optimized, or replaced without affecting renderers, and vice versa. The only shared surface is the step format.\n- **Multi-language support.** Both TypeScript and Python generators produce the same step format. Renderers do not care which language produced the steps.\n- **Pre-computation.** Steps can be generated at build time and serialized to JSON files. The renderer loads and renders them identically to real-time generated steps.\n- **Testability.** Generators can be tested by asserting on the shape and content of their output steps, without involving the rendering engine.\n\n### Consequences\n\n- The step format must be carefully designed and versioned. Breaking changes to the step format affect both generators and renderers.\n- All step data must be JSON-serializable. No functions, no class instances, no circular references.\n- Visual actions use an open string vocabulary (not a closed enum) so that new action types can be added without changing the format version.\n- The step format carries some redundancy (e.g., state snapshots at every step) in exchange for simplicity and debuggability.\n\n---\n\n## 3. Visual Primitives Over Bespoke Layouts\n\n### Decision\n\nThe rendering engine provides a set of reusable visual primitives (elements, connections, containers, annotations, overlays) and a layout system that composes them. Algorithm visualizations are built from these primitives, not from bespoke, per-algorithm rendering code.\n\n### Rationale\n\n- **Reuse.** Multiple algorithms share the same layout. Binary search, bubble sort, and quicksort all use `array-with-pointers`. BFS, DFS, and Dijkstra all use `graph-network`. Sharing layouts eliminates duplicated rendering logic.\n- **Consistency.** All algorithm visualizations have a consistent look, feel, and interaction model because they are built from the same primitives.\n- **Maintainability.** Improvements to a layout (e.g., better animation, accessibility, responsive sizing) automatically benefit every algorithm that uses it.\n- **Scalability.** Adding a new algorithm in an existing domain (e.g., a new sorting algorithm) requires only a generator and a `meta.json` -- the layout already exists.\n\n### Consequences\n\n- New domains (e.g., quantum computing) require implementing new layouts before algorithms can be added.\n- There is a design tension between layout generality and algorithm-specific customization. The `visual.components` field in `meta.json` provides per-algorithm configuration within a layout.\n- Some highly specialized visualizations may not fit cleanly into the primitives model. The architecture accommodates this through the extensible layout registry, but it requires more work than adding a standard algorithm.\n- The 10 built-in layouts cover classical algorithms, deep learning, and generative AI. Additional domains will require additional layouts.\n\n---\n\n## 4. Pre-Computed Steps as First-Class\n\n### Decision\n\nStep sequences can be generated at build time, serialized as static JSON, and served directly to the client. The renderer treats pre-computed steps identically to real-time generated steps.\n\n### Rationale\n\n- **Performance.** The first page load does not require running the generator. Pre-computed steps are served as static files, resulting in near-instant visualization.\n- **SEO.** Search engine crawlers receive complete, rendered pages with all educational content and visualization data. There is no client-side generation required for indexing.\n- **Reliability.** Pre-computed steps are validated at build time. If a generator has a bug, it is caught during the build, not when a user visits the page.\n- **Low-powered clients.** Mobile devices, tablets, and older machines do not need to run complex generators. They receive pre-computed steps and only run the renderer.\n\n### Consequences\n\n- The build system must generate steps for each algorithm's default inputs and write them as JSON files.\n- The `StepSequence` type includes a `generatedBy` field (`\"typescript\"`, `\"python\"`, or `\"precomputed\"`) so the source can be identified, but the renderer does not branch on this field.\n- When users modify input parameters, the web application falls back to real-time TypeScript generation. Pre-computed steps only cover default inputs.\n- Storage cost scales linearly with the number of algorithms and default input sets. Step sequences are typically 10--100 KB when gzipped.\n\n---\n\n## 5. Every Page Is a Standalone Landing Page\n\n### Decision\n\nEvery algorithm page on the Eigenvue website is designed to function as a complete, self-contained landing page. A user arriving from a search engine sees a fully working visualization, educational content, and navigation -- not a skeleton that requires further clicks.\n\n### Rationale\n\n- **SEO as primary growth channel.** Organic search traffic is the most scalable acquisition channel for educational content. Each algorithm page targets specific search queries (e.g., \"binary search visualization\", \"how self-attention works\").\n- **Zero-click value.** The user gets value (a working visualization, a clear explanation) immediately upon landing. There is no onboarding flow, no sign-up wall, no \"click here to start.\"\n- **Shareability.** Every page has a clean URL, Open Graph metadata, and a self-contained experience. When shared on social media or in a classroom, the link works without context.\n- **Bounce rate reduction.** If the first thing a user sees is complete and useful, they are more likely to explore further.\n\n### Consequences\n\n- Every algorithm page must include: a working visualization (with pre-computed steps), educational content (key concepts, pitfalls), a code panel, and navigation to related algorithms.\n- The `meta.json` file must contain complete SEO metadata (`seo.keywords`, `seo.ogDescription`), educational content (`education.keyConcepts`, `education.pitfalls`, `education.quiz`), and cross-references (`prerequisites`, `related`).\n- Page weight must be managed carefully. Each page loads its own visualization, educational content, and potentially pre-computed steps. Lazy loading and code splitting are essential.\n- The URL structure must be stable. Changing an algorithm's URL breaks inbound links from search engines and external sites.\n\n---\n\n## 6. Architecture Supports Future Paid Tiers\n\n### Decision\n\nThe architecture is designed from the beginning to support future paid tiers (e.g., premium algorithms, classroom features, advanced tools) without requiring architectural changes. The free tier is complete and valuable on its own.\n\n### Rationale\n\n- **Sustainability.** An open educational platform needs a sustainable business model. The architecture should not constrain future monetization options.\n- **Minimal refactoring.** Adding paid features should be additive (new code, new endpoints, new components), not a restructuring of existing code.\n- **Clean separation.** The generator/step-format/renderer architecture already separates concerns cleanly. Access control can be layered on top without modifying the core.\n\n### Consequences\n\n- The algorithm registry supports metadata that can be used for access control (category, difficulty level, tags) without currently enforcing any restrictions.\n- The Python package API is designed so that additional functions (e.g., `eigenvue.export()`, `eigenvue.compare()`) can be added without breaking the existing public API.\n- The `meta.json` schema is extensible. New fields can be added without breaking existing algorithms.\n- All current algorithms and features are free. The paid tier is a future addition, not a current restriction. The architecture simply avoids painting itself into a corner.","src/content/docs/architecture/design-decisions.mdx","dff75fad4bfe2171","architecture/overview",{"id":309,"data":311,"body":317,"filePath":318,"digest":319,"deferredRender":16},{"title":312,"description":313,"editUrl":16,"head":314,"template":50,"sidebar":315,"pagefind":16,"draft":39},"Architecture Overview","High-level system architecture.",[],{"hidden":39,"attrs":316},{},"Eigenvue is an interactive visual learning platform for algorithms, AI, and quantum computing. This document describes the high-level architecture that enables the platform to serve as both a standalone website and a Python package, while keeping algorithm implementations separate from rendering logic.\n\n---\n\n## Three-Layer Separation\n\nThe architecture is organized into three distinct layers. Data flows in one direction: from generators, through the step format, to renderers.\n\n```\n\n                    GENERATORS                        \n  TypeScript generators    Python generators          \n  (web, real-time)         (package, Jupyter)         \n                                                      \n  Input: algorithm parameters                         \n  Output: Step[]                                      \n\n                       \n                       \n\n                  STEP FORMAT                         \n                                                      \n  The universal data contract (version 1.0.0)         \n  JSON-serializable, language-agnostic                \n  Open vocabulary for visual actions                  \n                                                      \n  See: /api-reference/step-format                     \n\n                       \n                       \n\n                   RENDERERS                          \n  Canvas rendering engine    Python IFrame viewer     \n  Layout system              Jupyter embedding        \n  Animation manager                                   \n                                                      \n  Input: Step[]                                       \n  Output: interactive visualization                   \n\n```\n\n### Layer 1: Generators\n\nGenerators are functions that take algorithm inputs (e.g., an array and a target value for binary search) and produce an ordered array of `Step` objects. Each step represents a single \"interesting moment\" in the algorithm's execution -- a comparison, a swap, a pointer move, a node visit.\n\nGenerators exist in two implementations:\n\n- **TypeScript generators** (`generator.ts` files) run in the browser for real-time, interactive visualization. They use JavaScript generator functions (`function*`) and yield steps via a provided step builder.\n- **Python generators** run in the Python package for programmatic access, Jupyter notebooks, and offline analysis.\n\nBoth implementations produce the same step format. The generator is the algorithm's logic; it knows nothing about rendering.\n\n### Layer 2: Step Format\n\nThe [Step Format](/docs/api-reference/step-format) is the stable API between generators and renderers. It defines the shape of a `Step` object: an index, an ID, a title, an explanation, a state snapshot, an array of visual actions, a code highlight, and terminal/phase metadata.\n\nThe step format is:\n\n- **JSON-serializable**: Every value can be serialized to JSON and deserialized without loss.\n- **Language-agnostic**: The same format works for TypeScript and Python generators.\n- **Open vocabulary**: Visual action types are plain strings, not a closed enum. Renderers ignore types they do not recognize, allowing new action types to be added without breaking existing renderers.\n- **Versioned**: The format has a major version number. Renderers check the version before processing.\n\n### Layer 3: Renderers\n\nRenderers consume step arrays and produce interactive visualizations. The primary renderer is the canvas-based rendering engine in the web application, which includes:\n\n- A **layout system** that maps algorithm types to spatial arrangements.\n- An **animation manager** that handles transitions between steps.\n- A **canvas manager** that handles sizing, DPI scaling, and the render loop.\n\nThe Python package provides a lightweight renderer that embeds the web visualization in an IFrame (for `show()` and `jupyter()`).\n\n---\n\n## Dual Distribution\n\nEigenvue is distributed through two channels that share the same algorithm definitions and step format.\n\n### Web Application\n\nThe web application is built with Next.js and serves as the primary distribution channel. It provides:\n\n- Interactive algorithm pages with real-time step generation.\n- Playback controls (play, pause, step forward, step backward).\n- Code panel with syntax highlighting and line-by-line correspondence.\n- Input editors for modifying algorithm parameters.\n- Educational content (key concepts, pitfalls, quizzes).\n- SEO-optimized standalone landing pages.\n\nTypeScript generators run directly in the browser. Steps are generated on-demand when the user loads a page or changes inputs.\n\n### Python Package\n\nThe `eigenvue` Python package (`pip install eigenvue`) provides:\n\n- `eigenvue.list()` -- Discover available algorithms.\n- `eigenvue.steps()` -- Generate step sequences programmatically.\n- `eigenvue.show()` -- Launch interactive visualization in a browser.\n- `eigenvue.jupyter()` -- Embed visualization inline in Jupyter notebooks.\n\nPython generators run in the Python process. The `show()` and `jupyter()` functions serve the web visualization with pre-computed steps.\n\nSee the [Python Public API](/docs/api-reference/python-api) for the complete reference.\n\n---\n\n## Algorithms Are Code, Not Config\n\nA core architectural principle is that algorithm logic lives in generator code (TypeScript and Python), not in configuration files. The `meta.json` file contains metadata -- display names, descriptions, input schemas, educational content -- but never algorithm logic.\n\nThis means:\n\n- **Generators are plain functions.** No class hierarchies, no plugin systems, no declarative configuration languages. A generator is a function that receives inputs and yields steps.\n- **Full programming language power.** Generators can use loops, conditionals, recursion, data structures -- anything the language supports. There are no constraints imposed by a configuration schema.\n- **Testable in isolation.** Generators are pure functions (inputs in, steps out) that can be unit tested without a browser or rendering engine.\n\nThe `meta.json` file and the `generator.ts` file are co-located in the same directory, but they serve fundamentally different purposes: `meta.json` is data; `generator.ts` is code.\n\n---\n\n## Pre-Computed Steps\n\nSteps can be generated at build time and served as static JSON files. This enables:\n\n- **Instant page loads.** The first visit to an algorithm page does not require running the generator -- pre-computed steps are served immediately.\n- **SEO indexing.** Search engine crawlers see complete, rendered pages.\n- **Reduced client computation.** Mobile devices and low-powered clients benefit from pre-computed steps.\n\nThe `StepSequence` object includes a `generatedBy` field that distinguishes the source:\n\n```typescript\nreadonly generatedBy: \"typescript\" | \"python\" | \"precomputed\";\n```\n\nPre-computed steps use `\"precomputed\"`. The renderer treats all three sources identically -- it does not care how the steps were produced.\n\nWhen the user modifies input parameters, the web application falls back to real-time generation using the TypeScript generator. Pre-computed steps are only used for default inputs.\n\n---\n\n## Standalone Landing Pages\n\nEvery algorithm page on the Eigenvue website is designed to function as a standalone landing page. This means:\n\n- **No navigation prerequisite.** A user arriving from a search engine directly on the binary search page sees a complete, self-contained experience -- not a blank page that requires navigating through a hierarchy.\n- **Full SEO metadata.** Each page has its own title, description, Open Graph tags, and structured data, all derived from `meta.json`.\n- **Complete educational content.** Key concepts, pitfalls, quizzes, and external resources are rendered directly on the page, not hidden behind navigation.\n- **Working visualization.** The algorithm visualization loads with default inputs and pre-computed steps, ready for interaction without any user configuration.\n\nThis architecture supports organic search traffic as the primary growth channel. Each algorithm page is a potential entry point to the platform.\n\n---\n\n## Directory Structure\n\n```\neigenvue/\n algorithms/                    # Algorithm definitions\n    classical/\n       binary-search/\n          meta.json          # Metadata (data)\n          generator.ts       # TypeScript generator (code)\n          tests/\n              generator.test.ts\n       bubble-sort/\n       quicksort/\n       merge-sort/\n       bfs/\n       dfs/\n       dijkstra/\n    generative-ai/\n        tokenization-bpe/\n        token-embeddings/\n        self-attention/\n        multi-head-attention/\n        transformer-block/\n shared/\n    types/\n        step.ts                # Step format type definitions\n web/                           # Next.js web application\n    src/\n        engine/\n            generator/         # Generator runner and types\n                index.ts\n                types.ts\n                GeneratorRunner.ts\n python/                        # Python package\n    src/\n        eigenvue/\n            __init__.py\n docs/                          # Documentation site (Starlight)\n```\n\n---\n\n## Data Flow Summary\n\n1. **Author time**: A contributor writes a `meta.json` and a `generator.ts` (and optionally a Python generator) in the `algorithms/` directory.\n2. **Build time**: The build system validates `meta.json`, runs generators with default inputs, and writes pre-computed step JSON files. The web application is built with Next.js.\n3. **Request time**: A user visits an algorithm page. The pre-computed steps are served. The visualization renders immediately.\n4. **Interaction time**: The user modifies inputs. The TypeScript generator runs in the browser and produces new steps in real time. The visualization updates.\n5. **Python time**: A developer calls `eigenvue.steps()` or `eigenvue.show()` in Python. The Python generator runs and produces steps. For `show()` and `jupyter()`, the web visualization is served with the generated steps.","src/content/docs/architecture/overview.mdx","f798d1d0be390c74","api-reference/visual-actions",{"id":320,"data":322,"body":328,"filePath":329,"digest":330,"deferredRender":16},{"title":323,"description":324,"editUrl":16,"head":325,"template":50,"sidebar":326,"pagefind":16,"draft":39},"Visual Action Types","Complete catalog of visual actions.",[],{"hidden":39,"attrs":327},{},"Visual actions are the rendering instructions embedded in each step. They tell the rendering engine what to display when a step is active. Every visual action has a `type` field (camelCase string) and action-specific parameters.\n\nVisual actions use an **open vocabulary** -- the `type` field is a plain string, not a closed enum. Renderers silently ignore action types they do not recognize. This allows new action types to be defined in generators before renderer support exists.\n\n```typescript\ninterface VisualAction {\n  readonly type: string;\n  readonly [key: string]: unknown;\n}\n```\n\nAll parameter values must be JSON-serializable.\n\n---\n\n## Core Actions\n\nThese are the fundamental actions used by classical algorithm visualizations (sorting, searching).\n\n| Type               | Parameters                                          | Description                                                                 | Used By                              |\n| ------------------ | --------------------------------------------------- | --------------------------------------------------------------------------- | ------------------------------------ |\n| `highlightElement` | `index: number`, `color?: string`                   | Highlight a single element (array cell, graph node). Default color: `\"highlight\"`. | Binary search, bubble sort, quicksort, merge sort |\n| `movePointer`      | `id: string`, `to: number`                          | Move a named pointer to a new position. `id` identifies the pointer (e.g., `\"left\"`, `\"right\"`, `\"mid\"`). | Binary search, quicksort             |\n| `highlightRange`   | `from: number`, `to: number`, `color?: string`      | Highlight a contiguous range of elements (inclusive on both ends). Constraint: `from \u003C= to`. | Binary search, merge sort, quicksort |\n| `dimRange`         | `from: number`, `to: number`                        | Visually de-emphasize a contiguous range. Constraint: `from \u003C= to`.         | Binary search                        |\n| `swapElements`     | `i: number`, `j: number`                            | Swap two elements. Both indices are 0-based.                                | Bubble sort, quicksort               |\n| `compareElements`  | `i: number`, `j: number`, `result: \"less\" \\| \"greater\" \\| \"equal\"` | Visually compare two elements and show the comparison result.    | Bubble sort, binary search, merge sort |\n| `markFound`        | `index: number`                                     | Mark an element as the found/target element.                                | Binary search                        |\n| `markNotFound`     | _(none)_                                            | Indicate that the target was not found. No specific index.                  | Binary search                        |\n| `showMessage`      | `text: string`, `messageType: \"info\" \\| \"success\" \\| \"warning\" \\| \"error\"` | Display a transient text message on the visualization. | All algorithms                       |\n\n### Core Action Details\n\n#### `highlightElement`\n\n```typescript\ninterface HighlightElementAction extends VisualAction {\n  readonly type: \"highlightElement\";\n  readonly index: number;     // 0-based element index\n  readonly color?: string;    // Semantic color: \"highlight\", \"compare\", \"found\", \"active\", \"inactive\"\n}\n```\n\n#### `movePointer`\n\n```typescript\ninterface MovePointerAction extends VisualAction {\n  readonly type: \"movePointer\";\n  readonly id: string;        // Pointer name (e.g., \"left\", \"right\", \"mid\")\n  readonly to: number;        // 0-based target index\n}\n```\n\n#### `highlightRange`\n\n```typescript\ninterface HighlightRangeAction extends VisualAction {\n  readonly type: \"highlightRange\";\n  readonly from: number;      // 0-based start index (inclusive)\n  readonly to: number;        // 0-based end index (inclusive). Must satisfy: from \u003C= to\n  readonly color?: string;\n}\n```\n\n#### `dimRange`\n\n```typescript\ninterface DimRangeAction extends VisualAction {\n  readonly type: \"dimRange\";\n  readonly from: number;      // 0-based start index (inclusive)\n  readonly to: number;        // 0-based end index (inclusive). Must satisfy: from \u003C= to\n}\n```\n\n#### `swapElements`\n\n```typescript\ninterface SwapElementsAction extends VisualAction {\n  readonly type: \"swapElements\";\n  readonly i: number;         // 0-based index of first element\n  readonly j: number;         // 0-based index of second element\n}\n```\n\n#### `compareElements`\n\n```typescript\ninterface CompareElementsAction extends VisualAction {\n  readonly type: \"compareElements\";\n  readonly i: number;\n  readonly j: number;\n  readonly result: \"less\" | \"greater\" | \"equal\";\n}\n```\n\n#### `markFound`\n\n```typescript\ninterface MarkFoundAction extends VisualAction {\n  readonly type: \"markFound\";\n  readonly index: number;     // 0-based index of the found element\n}\n```\n\n#### `markNotFound`\n\n```typescript\ninterface MarkNotFoundAction extends VisualAction {\n  readonly type: \"markNotFound\";\n  // No additional parameters\n}\n```\n\n#### `showMessage`\n\n```typescript\ninterface ShowMessageAction extends VisualAction {\n  readonly type: \"showMessage\";\n  readonly text: string;\n  readonly messageType: \"info\" | \"success\" | \"warning\" | \"error\";\n}\n```\n\n---\n\n## Graph Actions\n\nActions for graph-based algorithm visualizations (BFS, DFS, Dijkstra).\n\n| Type              | Parameters                                           | Description                                                               | Used By              |\n| ----------------- | ---------------------------------------------------- | ------------------------------------------------------------------------- | -------------------- |\n| `visitNode`       | `nodeId: string`, `color?: string`                   | Mark a graph node as visited. Default color: `\"visited\"`.                 | BFS, DFS, Dijkstra   |\n| `highlightEdge`   | `from: string`, `to: string`, `color?: string`       | Highlight an edge between two nodes. Default color: `\"highlight\"`.        | BFS, DFS, Dijkstra   |\n| `updateNodeValue` | `nodeId: string`, `value: number \\| string \\| null`  | Update the displayed value of a node (e.g., distance in Dijkstra). `null` clears the value. | Dijkstra             |\n\n### Graph Action Details\n\n#### `visitNode`\n\n```typescript\ninterface VisitNodeAction extends VisualAction {\n  readonly type: \"visitNode\";\n  readonly nodeId: string;    // Unique node identifier within the graph\n  readonly color?: string;    // Default: \"visited\"\n}\n```\n\n#### `highlightEdge`\n\n```typescript\ninterface HighlightEdgeAction extends VisualAction {\n  readonly type: \"highlightEdge\";\n  readonly from: string;      // Source node ID\n  readonly to: string;        // Target node ID\n  readonly color?: string;    // Default: \"highlight\"\n}\n```\n\n#### `updateNodeValue`\n\n```typescript\ninterface UpdateNodeValueAction extends VisualAction {\n  readonly type: \"updateNodeValue\";\n  readonly nodeId: string;\n  readonly value: number | string | null;   // null clears the display value\n}\n```\n\n---\n\n## Neural Network Actions\n\nActions for neural network visualizations (forward propagation, backpropagation, training).\n\n| Type                     | Parameters                                                                      | Description                                                        | Used By                     |\n| ------------------------ | ------------------------------------------------------------------------------- | ------------------------------------------------------------------ | --------------------------- |\n| `activateNeuron`         | `layer: number`, `index: number`, `value: number`                               | Activate (light up) a neuron in a network layer.                   | Forward propagation         |\n| `propagateSignal`        | `fromLayer: number`, `toLayer: number`                                          | Show signal propagating between layers.                            | Forward propagation         |\n| `showGradient`           | `layer: number`, `values: number[]`                                             | Display gradient values for a layer (one per neuron).              | Backpropagation             |\n| `showWeights`            | `fromLayer: number`, `toLayer: number`, `weights: number[][]`                   | Show weight values on connections. Shape: `[toLayerSize, fromLayerSize]`. | Forward propagation, training |\n| `showWeightGradients`    | `fromLayer: number`, `toLayer: number`, `gradients: number[][]`                 | Show weight gradient values. Shape: `[toLayerSize, fromLayerSize]`. `gradients[j][i] = dLoss/dw_ji`. | Backpropagation             |\n| `showBiasGradients`      | `layer: number`, `biasGradients: number[]`                                      | Show bias gradient values. `biasGradients[j] = dLoss/db_j`.       | Backpropagation             |\n| `updateWeights`          | `fromLayer: number`, `toLayer: number`, `oldWeights: number[][]`, `newWeights: number[][]`, `learningRate: number` | Show weight update after gradient descent. | Training                    |\n| `showPreActivation`      | `layer: number`, `index: number`, `z: number`, `weightedInputs: number[]`, `bias: number` | Show pre-activation value: `z = sum(w_i * x_i) + b`. | Forward propagation         |\n| `showActivationFunction` | `layer: number`, `index: number`, `functionName: string`, `input: number`, `output: number` | Show activation function being applied: `a = f(z)`. | Forward propagation         |\n| `showLoss`               | `loss: number`, `lossFunction: string`, `predictions: number[]`, `targets: number[]` | Show the computed loss value.                                | Training                    |\n\n### Neural Network Action Details\n\n#### `activateNeuron`\n\n```typescript\ninterface ActivateNeuronAction extends VisualAction {\n  readonly type: \"activateNeuron\";\n  readonly layer: number;     // 0-based layer index\n  readonly index: number;     // 0-based neuron index within the layer\n  readonly value: number;     // Activation value (typically 0-1, but not constrained)\n}\n```\n\n#### `propagateSignal`\n\n```typescript\ninterface PropagateSignalAction extends VisualAction {\n  readonly type: \"propagateSignal\";\n  readonly fromLayer: number; // 0-based source layer index\n  readonly toLayer: number;   // 0-based target layer index\n}\n```\n\n#### `showGradient`\n\n```typescript\ninterface ShowGradientAction extends VisualAction {\n  readonly type: \"showGradient\";\n  readonly layer: number;\n  readonly values: readonly number[];   // One gradient value per neuron\n}\n```\n\n#### `showWeights`\n\n```typescript\ninterface ShowWeightsAction extends VisualAction {\n  readonly type: \"showWeights\";\n  readonly fromLayer: number;\n  readonly toLayer: number;\n  readonly weights: readonly (readonly number[])[];   // Shape: [toLayerSize, fromLayerSize]\n  // weights[j][i] = weight from neuron i in fromLayer to neuron j in toLayer\n}\n```\n\n#### `showWeightGradients`\n\n```typescript\ninterface ShowWeightGradientsAction extends VisualAction {\n  readonly type: \"showWeightGradients\";\n  readonly fromLayer: number;\n  readonly toLayer: number;\n  readonly gradients: readonly (readonly number[])[];  // Shape: [toLayerSize, fromLayerSize]\n  // gradients[j][i] = dLoss/dw_ji = delta_j * a_i\n}\n```\n\n#### `showBiasGradients`\n\n```typescript\ninterface ShowBiasGradientsAction extends VisualAction {\n  readonly type: \"showBiasGradients\";\n  readonly layer: number;\n  readonly biasGradients: readonly number[];  // biasGradients[j] = dLoss/db_j = delta_j\n}\n```\n\n#### `updateWeights`\n\n```typescript\ninterface UpdateWeightsAction extends VisualAction {\n  readonly type: \"updateWeights\";\n  readonly fromLayer: number;\n  readonly toLayer: number;\n  readonly oldWeights: readonly (readonly number[])[];   // Before update\n  readonly newWeights: readonly (readonly number[])[];   // After update\n  readonly learningRate: number;\n  // newWeights[j][i] = oldWeights[j][i] - learningRate * dLoss/dw_ji\n}\n```\n\n#### `showPreActivation`\n\n```typescript\ninterface ShowPreActivationAction extends VisualAction {\n  readonly type: \"showPreActivation\";\n  readonly layer: number;\n  readonly index: number;\n  readonly z: number;                          // Pre-activation: z = sum(w_i * x_i) + b\n  readonly weightedInputs: readonly number[];  // Individual terms: w_i * x_i\n  readonly bias: number;                       // Bias term\n}\n```\n\n#### `showActivationFunction`\n\n```typescript\ninterface ShowActivationFunctionAction extends VisualAction {\n  readonly type: \"showActivationFunction\";\n  readonly layer: number;\n  readonly index: number;\n  readonly functionName: \"sigmoid\" | \"relu\" | \"tanh\" | \"step\";\n  readonly input: number;     // z (pre-activation)\n  readonly output: number;    // a = activation_fn(z)\n}\n```\n\n#### `showLoss`\n\n```typescript\ninterface ShowLossAction extends VisualAction {\n  readonly type: \"showLoss\";\n  readonly loss: number;\n  readonly lossFunction: \"mse\" | \"binary-cross-entropy\";\n  readonly predictions: readonly number[];\n  readonly targets: readonly number[];\n}\n```\n\n---\n\n## Convolution Actions\n\nActions for convolution operation visualizations.\n\n| Type                     | Parameters                                                            | Description                                                            | Used By        |\n| ------------------------ | --------------------------------------------------------------------- | ---------------------------------------------------------------------- | -------------- |\n| `highlightKernelPosition`| `row: number`, `col: number`, `kernelHeight: number`, `kernelWidth: number` | Highlight the current position of the convolution kernel on the input grid. | 2D Convolution |\n| `showConvolutionProducts`| `row: number`, `col: number`, `products: number[][]`, `sum: number`   | Show element-wise products between kernel and input patch at the current position. | 2D Convolution |\n| `writeOutputCell`        | `row: number`, `col: number`, `value: number`                         | Write a computed value into the output feature map.                    | 2D Convolution |\n\n### Convolution Action Details\n\n#### `highlightKernelPosition`\n\n```typescript\ninterface HighlightKernelPositionAction extends VisualAction {\n  readonly type: \"highlightKernelPosition\";\n  readonly row: number;           // Top-left row of kernel window on input\n  readonly col: number;           // Top-left column of kernel window on input\n  readonly kernelHeight: number;  // Kernel height\n  readonly kernelWidth: number;   // Kernel width\n}\n```\n\n#### `showConvolutionProducts`\n\n```typescript\ninterface ShowConvolutionProductsAction extends VisualAction {\n  readonly type: \"showConvolutionProducts\";\n  readonly row: number;\n  readonly col: number;\n  readonly products: readonly (readonly number[])[];\n  // products[kr][kc] = input[row + kr][col + kc] * kernel[kr][kc]\n  readonly sum: number;   // Sum of all products -- becomes the output value\n}\n```\n\n#### `writeOutputCell`\n\n```typescript\ninterface WriteOutputCellAction extends VisualAction {\n  readonly type: \"writeOutputCell\";\n  readonly row: number;     // Row in the output feature map\n  readonly col: number;     // Column in the output feature map\n  readonly value: number;   // Computed value to write\n}\n```\n\n---\n\n## Gradient Descent Actions\n\nActions for gradient descent and optimizer visualizations.\n\n| Type                    | Parameters                                                                                              | Description                                                      | Used By          |\n| ----------------------- | ------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- | ---------------- |\n| `showLandscapePosition` | `parameters: number[]`, `loss: number`, `gradient: number[]`                                            | Show the current position on the loss landscape.                 | Gradient descent |\n| `showDescentStep`       | `fromParameters: number[]`, `toParameters: number[]`, `fromLoss: number`, `toLoss: number`, `optimizer: string`, `learningRate: number` | Show a gradient descent step from current to new position. | Gradient descent |\n| `showTrajectory`        | `trajectory: {parameters: number[], loss: number}[]`, `optimizer: string`                               | Show the trajectory of all visited positions.                    | Gradient descent |\n| `showMomentum`          | `velocity: number[]`, `secondMoment?: number[]`                                                         | Show momentum/velocity vector. `secondMoment` is for Adam only. | Gradient descent |\n\n### Gradient Descent Action Details\n\n#### `showLandscapePosition`\n\n```typescript\ninterface ShowLandscapePositionAction extends VisualAction {\n  readonly type: \"showLandscapePosition\";\n  readonly parameters: readonly number[];   // Current parameter values\n  readonly loss: number;                    // Loss at this position\n  readonly gradient: readonly number[];     // Gradient vector at this position\n}\n```\n\n#### `showDescentStep`\n\n```typescript\ninterface ShowDescentStepAction extends VisualAction {\n  readonly type: \"showDescentStep\";\n  readonly fromParameters: readonly number[];\n  readonly toParameters: readonly number[];\n  readonly fromLoss: number;\n  readonly toLoss: number;\n  readonly optimizer: \"sgd\" | \"momentum\" | \"adam\";\n  readonly learningRate: number;\n}\n```\n\n#### `showTrajectory`\n\n```typescript\ninterface ShowTrajectoryAction extends VisualAction {\n  readonly type: \"showTrajectory\";\n  readonly trajectory: readonly {\n    readonly parameters: readonly number[];\n    readonly loss: number;\n  }[];\n  readonly optimizer: \"sgd\" | \"momentum\" | \"adam\";\n}\n```\n\n#### `showMomentum`\n\n```typescript\ninterface ShowMomentumAction extends VisualAction {\n  readonly type: \"showMomentum\";\n  readonly velocity: readonly number[];\n  readonly secondMoment?: readonly number[];   // Adam only; omit for plain momentum\n}\n```\n\n---\n\n## GenAI Actions\n\nActions for generative AI visualizations (tokenization, embeddings, attention mechanisms, transformer blocks).\n\n| Type                      | Parameters                                                                      | Description                                                                | Used By                 |\n| ------------------------- | ------------------------------------------------------------------------------- | -------------------------------------------------------------------------- | ----------------------- |\n| `showAttentionWeights`    | `queryIdx: number`, `weights: number[]`                                         | Show attention weights from a query token to all keys. Weights must sum to 1.0 (within 1e-6). | Self-attention, multi-head attention |\n| `highlightToken`          | `index: number`, `color?: string`                                               | Highlight a token in a sequence.                                           | Tokenization, embeddings, attention |\n| `updateBarChart`          | `values: number[]`, `labels?: string[]`                                         | Update a bar chart (e.g., probability distributions). If `labels` provided, `labels.length` must equal `values.length`. | Tokenization            |\n| `highlightTextSpan`       | `start: number`, `end: number`, `color?: string`                                | Highlight characters in source text. `start` inclusive, `end` exclusive.   | BPE tokenization        |\n| `mergeTokens`             | `leftIndex: number`, `rightIndex: number`, `result: string`                     | Show a BPE merge of two adjacent tokens.                                   | BPE tokenization        |\n| `showEmbedding`           | `tokenIndex: number`, `values: number[]`                                        | Display an embedding vector for a token.                                   | Token embeddings        |\n| `showSimilarity`          | `tokenA: number`, `tokenB: number`, `score: number`                             | Show cosine similarity between two embeddings. Score in [-1, 1].           | Token embeddings        |\n| `showProjectionMatrix`    | `projectionType: \"Q\" \\| \"K\" \\| \"V\"`, `matrix: number[][]`                      | Show Q, K, or V projection matrix. Shape: `[seqLen, d_k]`.                | Self-attention          |\n| `showAttentionScores`     | `scores: number[][]`                                                            | Show raw (pre-softmax) attention scores. Shape: `[seqLen, seqLen]`. `scores[i][j] = Q[i] . K[j]^T / sqrt(d_k)`. | Self-attention          |\n| `showFullAttentionMatrix` | `weights: number[][]`                                                           | Show full attention matrix after softmax. Each row sums to 1.0.            | Self-attention          |\n| `showWeightedValues`      | `queryIdx: number`, `contextVector: number[]`                                   | Show weighted value vectors (attention output) for a query.                | Self-attention          |\n| `activateHead`            | `headIndex: number`, `totalHeads: number`                                       | Activate a specific attention head.                                        | Multi-head attention    |\n| `showConcatenatedHeads`   | `matrix: number[][]`                                                            | Show concatenated multi-head output. Shape: `[seqLen, numHeads * d_v]`.    | Multi-head attention    |\n| `activateSublayer`        | `sublayerId: string`, `label: string`                                           | Show data flowing through a transformer sublayer.                          | Transformer block       |\n| `showResidualConnection`  | `input: number[]`, `sublayerOutput: number[]`, `result: number[]`               | Show residual connection: `result = input + sublayerOutput`.               | Transformer block       |\n| `showLayerNorm`           | `input: number[]`, `output: number[]`                                           | Show layer normalization. Output has mean ~0, variance ~1.                 | Transformer block       |\n\n### GenAI Action Details\n\n#### `showAttentionWeights`\n\n```typescript\ninterface ShowAttentionWeightsAction extends VisualAction {\n  readonly type: \"showAttentionWeights\";\n  readonly queryIdx: number;\n  readonly weights: readonly number[];\n  // Invariant: Math.abs(weights.reduce((a, b) => a + b, 0) - 1.0) \u003C= 1e-6\n  // Each weight must be in [0, 1]\n}\n```\n\n#### `highlightToken`\n\n```typescript\ninterface HighlightTokenAction extends VisualAction {\n  readonly type: \"highlightToken\";\n  readonly index: number;       // 0-based token index\n  readonly color?: string;      // Default: \"highlight\"\n}\n```\n\n#### `updateBarChart`\n\n```typescript\ninterface UpdateBarChartAction extends VisualAction {\n  readonly type: \"updateBarChart\";\n  readonly values: readonly number[];\n  readonly labels?: readonly string[];   // Must match values.length if provided\n}\n```\n\n#### `highlightTextSpan`\n\n```typescript\ninterface HighlightTextSpanAction extends VisualAction {\n  readonly type: \"highlightTextSpan\";\n  readonly start: number;       // 0-based start character index (inclusive)\n  readonly end: number;         // 0-based end character index (exclusive)\n  readonly color?: string;\n}\n```\n\n#### `mergeTokens`\n\n```typescript\ninterface MergeTokensAction extends VisualAction {\n  readonly type: \"mergeTokens\";\n  readonly leftIndex: number;   // 0-based index of left token\n  readonly rightIndex: number;  // 0-based index of right token\n  readonly result: string;      // The resulting merged token string\n}\n```\n\n#### `showEmbedding`\n\n```typescript\ninterface ShowEmbeddingAction extends VisualAction {\n  readonly type: \"showEmbedding\";\n  readonly tokenIndex: number;\n  readonly values: readonly number[];   // Length equals d_model\n}\n```\n\n#### `showSimilarity`\n\n```typescript\ninterface ShowSimilarityAction extends VisualAction {\n  readonly type: \"showSimilarity\";\n  readonly tokenA: number;\n  readonly tokenB: number;\n  readonly score: number;   // Cosine similarity in [-1, 1]\n  // score = (A . B) / (||A|| * ||B||)\n}\n```\n\n#### `showProjectionMatrix`\n\n```typescript\ninterface ShowProjectionMatrixAction extends VisualAction {\n  readonly type: \"showProjectionMatrix\";\n  readonly projectionType: \"Q\" | \"K\" | \"V\";\n  readonly matrix: readonly (readonly number[])[];   // Shape: [seqLen, d_k]\n}\n```\n\n#### `showAttentionScores`\n\n```typescript\ninterface ShowAttentionScoresAction extends VisualAction {\n  readonly type: \"showAttentionScores\";\n  readonly scores: readonly (readonly number[])[];   // Shape: [seqLen, seqLen]\n  // scores[i][j] = (Q[i] . K[j]) / sqrt(d_k)\n}\n```\n\n#### `showFullAttentionMatrix`\n\n```typescript\ninterface ShowFullAttentionMatrixAction extends VisualAction {\n  readonly type: \"showFullAttentionMatrix\";\n  readonly weights: readonly (readonly number[])[];   // Shape: [seqLen, seqLen]\n  // Invariant: each row sums to 1.0 (within 1e-6)\n}\n```\n\n#### `showWeightedValues`\n\n```typescript\ninterface ShowWeightedValuesAction extends VisualAction {\n  readonly type: \"showWeightedValues\";\n  readonly queryIdx: number;\n  readonly contextVector: readonly number[];   // Length: d_v\n}\n```\n\n#### `activateHead`\n\n```typescript\ninterface ActivateHeadAction extends VisualAction {\n  readonly type: \"activateHead\";\n  readonly headIndex: number;     // 0-based head index\n  readonly totalHeads: number;    // Total number of heads (for UI layout)\n}\n```\n\n#### `showConcatenatedHeads`\n\n```typescript\ninterface ShowConcatenatedHeadsAction extends VisualAction {\n  readonly type: \"showConcatenatedHeads\";\n  readonly matrix: readonly (readonly number[])[];   // Shape: [seqLen, numHeads * d_v]\n}\n```\n\n#### `activateSublayer`\n\n```typescript\ninterface ActivateSublayerAction extends VisualAction {\n  readonly type: \"activateSublayer\";\n  readonly sublayerId: string;    // e.g., \"self-attention\", \"add-norm-1\", \"ffn\", \"add-norm-2\"\n  readonly label: string;         // Human-readable label\n}\n```\n\n#### `showResidualConnection`\n\n```typescript\ninterface ShowResidualConnectionAction extends VisualAction {\n  readonly type: \"showResidualConnection\";\n  readonly input: readonly number[];           // Input vector (d_model)\n  readonly sublayerOutput: readonly number[];  // Sublayer output (d_model)\n  readonly result: readonly number[];          // input + sublayerOutput (d_model)\n  // Invariant: Math.abs(result[j] - (input[j] + sublayerOutput[j])) \u003C= 1e-9\n}\n```\n\n#### `showLayerNorm`\n\n```typescript\ninterface ShowLayerNormAction extends VisualAction {\n  readonly type: \"showLayerNorm\";\n  readonly input: readonly number[];    // Before normalization (d_model)\n  readonly output: readonly number[];   // After normalization (d_model)\n  // Invariant (without learnable gamma/beta):\n  //   Math.abs(mean(output)) \u003C= 1e-6\n  //   Math.abs(variance(output) - 1.0) \u003C= 1e-4\n}\n```\n\n---\n\n## Discriminated Union\n\nFor exhaustive switch statements in renderers, the `KnownVisualAction` type is a discriminated union of all known action interfaces:\n\n```typescript\ntype KnownVisualAction =\n  | HighlightElementAction\n  | MovePointerAction\n  | HighlightRangeAction\n  | DimRangeAction\n  | SwapElementsAction\n  | CompareElementsAction\n  | MarkFoundAction\n  | MarkNotFoundAction\n  | ShowMessageAction\n  | VisitNodeAction\n  | HighlightEdgeAction\n  | UpdateNodeValueAction\n  | ActivateNeuronAction\n  | PropagateSignalAction\n  | ShowGradientAction\n  | ShowWeightsAction\n  | ShowWeightGradientsAction\n  | ShowBiasGradientsAction\n  | UpdateWeightsAction\n  | ShowPreActivationAction\n  | ShowActivationFunctionAction\n  | ShowLossAction\n  | HighlightKernelPositionAction\n  | ShowConvolutionProductsAction\n  | WriteOutputCellAction\n  | ShowLandscapePositionAction\n  | ShowDescentStepAction\n  | ShowTrajectoryAction\n  | ShowMomentumAction\n  | ShowAttentionWeightsAction\n  | HighlightTokenAction\n  | UpdateBarChartAction\n  | HighlightTextSpanAction\n  | MergeTokensAction\n  | ShowEmbeddingAction\n  | ShowSimilarityAction\n  | ShowProjectionMatrixAction\n  | ShowAttentionScoresAction\n  | ShowFullAttentionMatrixAction\n  | ShowWeightedValuesAction\n  | ActivateHeadAction\n  | ShowConcatenatedHeadsAction\n  | ActivateSublayerAction\n  | ShowResidualConnectionAction\n  | ShowLayerNormAction;\n```\n\nRenderers must also handle unknown types gracefully (no-op) because the vocabulary is open.\n\n---\n\n## Extensibility\n\nTo add a new visual action type:\n\n1. **Define the interface** in `shared/types/step.ts`, extending `VisualAction`.\n2. **Add it to the `KnownVisualAction` union** for type safety.\n3. **Use it in a generator** -- no changes to the runner are needed (the runner does not inspect action types, except for validated types like `highlightRange` and `showAttentionWeights`).\n4. **Implement renderer support** -- add a case to the renderer's action handler. Until renderer support exists, the action is silently ignored.\n\nNo version bump is required for new action types because the open vocabulary design ensures backward compatibility. Renderers that do not recognize a new action type will simply skip it.\n\n### Validation\n\nThe generator runner validates the following action types at runtime:\n\n| Action Type             | Validation Rule                                              |\n| ----------------------- | ------------------------------------------------------------ |\n| `highlightRange`        | `from \u003C= to`                                                 |\n| `dimRange`              | `from \u003C= to`                                                 |\n| `compareElements`       | `result` is one of `\"less\"`, `\"greater\"`, `\"equal\"`          |\n| `showAttentionWeights`  | Each weight in `[0, 1]`; weights sum to `1.0` within `1e-6` (Kahan summation) |\n| `updateBarChart`        | If `labels` present, `labels.length === values.length`       |\n\nAll other action types are passed through without validation. If you need validation for a custom action type, add it to the `validateVisualActions` function in `GeneratorRunner.ts`.","src/content/docs/api-reference/visual-actions.mdx","6ba917b93f04ee68","architecture/rendering-engine",{"id":331,"data":333,"body":339,"filePath":340,"digest":341,"deferredRender":16},{"title":334,"description":335,"editUrl":16,"head":336,"template":50,"sidebar":337,"pagefind":16,"draft":39},"Rendering Engine","Technical docs for the rendering engine.",[],{"hidden":39,"attrs":338},{},"The rendering engine is the component that transforms step sequences into interactive visualizations. It is responsible for drawing visual primitives on a canvas, managing layouts, animating transitions between steps, and handling canvas sizing and DPI scaling.\n\n---\n\n## Visual Primitives\n\nThe rendering engine provides five categories of visual primitives. Layouts compose these primitives to create algorithm-specific visualizations.\n\n### Element\n\nAn element is a discrete, addressable visual unit -- an array cell, a graph node, a neuron, a token, a grid cell.\n\n| Property      | Type                | Description                                                              |\n| ------------- | ------------------- | ------------------------------------------------------------------------ |\n| `id`          | `string`            | Unique identifier within the layout (e.g., `\"cell-3\"`, `\"node-A\"`).     |\n| `x`           | `number`            | Horizontal position (canvas coordinates).                                |\n| `y`           | `number`            | Vertical position (canvas coordinates).                                  |\n| `width`       | `number`            | Element width in pixels.                                                 |\n| `height`      | `number`            | Element height in pixels.                                                |\n| `value`       | `string \\| number \\| null` | Display value inside the element.                                 |\n| `label`       | `string \\| null`    | Optional label (e.g., index label below an array cell).                  |\n| `color`       | `string`            | Fill color (hex or semantic name resolved by the theme).                 |\n| `borderColor` | `string`            | Border/stroke color.                                                     |\n| `opacity`     | `number`            | Opacity in `[0, 1]`. Used for dimming.                                   |\n| `shape`       | `string`            | Rendering shape: `\"rect\"`, `\"circle\"`, `\"roundedRect\"`.                 |\n| `state`       | `string`            | Semantic state: `\"default\"`, `\"highlighted\"`, `\"active\"`, `\"dimmed\"`, `\"found\"`. |\n\n### Connection\n\nA connection is a visual link between two elements -- a graph edge, a neural network weight line, a data flow arrow.\n\n| Property      | Type                | Description                                                              |\n| ------------- | ------------------- | ------------------------------------------------------------------------ |\n| `id`          | `string`            | Unique identifier (e.g., `\"edge-A-B\"`, `\"weight-0-1-2-0\"`).            |\n| `fromId`      | `string`            | ID of the source element.                                                |\n| `toId`        | `string`            | ID of the target element.                                                |\n| `color`       | `string`            | Line color.                                                              |\n| `width`       | `number`            | Line width in pixels.                                                    |\n| `style`       | `string`            | Line style: `\"solid\"`, `\"dashed\"`, `\"dotted\"`.                          |\n| `directed`    | `boolean`           | Whether to draw an arrowhead at the target end.                          |\n| `label`       | `string \\| null`    | Optional label (e.g., edge weight, connection weight value).             |\n| `opacity`     | `number`            | Opacity in `[0, 1]`.                                                     |\n| `curvature`   | `number`            | Curvature amount for curved connections. `0` is straight.                |\n\n### Container\n\nA container groups elements visually -- a layer column in a neural network, a sublayer block in a transformer, a partition boundary in quicksort.\n\n| Property      | Type                | Description                                                              |\n| ------------- | ------------------- | ------------------------------------------------------------------------ |\n| `id`          | `string`            | Unique identifier (e.g., `\"layer-1\"`, `\"sublayer-ffn\"`).               |\n| `x`           | `number`            | Top-left x position.                                                     |\n| `y`           | `number`            | Top-left y position.                                                     |\n| `width`       | `number`            | Container width.                                                         |\n| `height`      | `number`            | Container height.                                                        |\n| `label`       | `string \\| null`    | Optional label displayed above or inside the container.                  |\n| `color`       | `string`            | Background fill color.                                                   |\n| `borderColor` | `string`            | Border color.                                                            |\n| `borderStyle` | `string`            | Border style: `\"solid\"`, `\"dashed\"`, `\"dotted\"`, `\"none\"`.             |\n| `padding`     | `number`            | Internal padding in pixels.                                              |\n\n### Annotation\n\nAn annotation is a text label, tooltip, or callout that provides additional information -- a pointer label, a value annotation, a formula display.\n\n| Property      | Type                | Description                                                              |\n| ------------- | ------------------- | ------------------------------------------------------------------------ |\n| `id`          | `string`            | Unique identifier.                                                       |\n| `x`           | `number`            | Horizontal position.                                                     |\n| `y`           | `number`            | Vertical position.                                                       |\n| `text`        | `string`            | Display text.                                                            |\n| `fontSize`    | `number`            | Font size in pixels.                                                     |\n| `fontWeight`  | `string`            | Font weight: `\"normal\"`, `\"bold\"`.                                      |\n| `color`       | `string`            | Text color.                                                              |\n| `anchor`      | `string`            | Text anchor: `\"start\"`, `\"middle\"`, `\"end\"`.                            |\n| `baseline`    | `string`            | Text baseline: `\"top\"`, `\"middle\"`, `\"bottom\"`.                         |\n| `targetId`    | `string \\| null`    | If set, the annotation is attached to this element and moves with it.    |\n| `offset`      | `{x: number, y: number}` | Offset from the anchor point or target element.                    |\n\n### Overlay\n\nAn overlay is a full-canvas or region-scoped visual effect -- a message banner, a heatmap background, a gradient arrow field.\n\n| Property      | Type                | Description                                                              |\n| ------------- | ------------------- | ------------------------------------------------------------------------ |\n| `id`          | `string`            | Unique identifier.                                                       |\n| `type`        | `string`            | Overlay type: `\"message\"`, `\"heatmap\"`, `\"arrowField\"`, `\"highlight\"`.  |\n| `region`      | `{x, y, width, height} \\| null` | Scoped region, or `null` for full-canvas.                  |\n| `data`        | `unknown`           | Type-specific data (e.g., message text, heatmap values, arrow vectors).  |\n| `opacity`     | `number`            | Opacity in `[0, 1]`.                                                     |\n| `zIndex`      | `number`            | Stacking order. Higher values render on top.                             |\n\n---\n\n## Layout System\n\nThe layout system maps algorithm types to spatial arrangements of visual primitives. See the [Layout System reference](/docs/api-reference/layout-system) for the complete layout catalog and configuration options.\n\nThe rendering engine interacts with layouts through a simple contract:\n\n1. **Initialize**: The engine creates a layout instance, passing the `LayoutConfig` (components, theme, canvas dimensions).\n2. **Render**: For each step, the engine calls the layout's render function with the step data. The layout reads `step.state` and `step.visualActions`, creates/updates visual primitives, and returns the drawing commands.\n3. **Dispose**: When the visualization is torn down, the engine calls the layout's dispose function to release resources.\n\nThe layout is the only component that understands the semantics of specific visual action types. The rendering engine itself is action-type agnostic -- it draws primitives, not algorithm-specific concepts.\n\n---\n\n## Animation Manager\n\nThe animation manager handles smooth transitions between steps. When the user advances from step N to step N+1, the animation manager interpolates visual primitive properties over a configurable duration.\n\n### Step Transitions\n\nWhen a new step is rendered, the animation manager:\n\n1. **Captures the current state** of all visual primitives (positions, colors, opacities).\n2. **Computes the target state** by running the layout's render function for the new step.\n3. **Generates a transition plan** that maps each primitive from its current state to its target state.\n4. **Executes the transition** using `requestAnimationFrame`, interpolating properties frame by frame.\n\nPrimitives that exist in the current step but not the next are faded out. Primitives that exist in the next step but not the current are faded in. Primitives that exist in both are smoothly transitioned.\n\n### Interpolation\n\nThe animation manager supports the following interpolation strategies:\n\n| Property Type | Interpolation                                                                |\n| ------------- | ---------------------------------------------------------------------------- |\n| Position      | Linear interpolation between `(x, y)` coordinates with easing.              |\n| Color         | Interpolation in HSL color space for perceptually smooth color transitions.  |\n| Opacity       | Linear interpolation in `[0, 1]`.                                            |\n| Size          | Linear interpolation of `width` and `height`.                                |\n| Value/Label   | Discrete swap at the midpoint of the transition (no gradual text morphing).  |\n\nThe default easing function is `ease-in-out` (cubic bezier). The default transition duration is 300ms, configurable per-layout.\n\n### Playback Control\n\nThe animation manager integrates with the playback controls:\n\n| Control        | Behavior                                                                   |\n| -------------- | -------------------------------------------------------------------------- |\n| Step forward   | Animate from current step to next step.                                    |\n| Step backward   | Animate from current step to previous step (reverse transition).          |\n| Play           | Auto-advance through steps at a configurable interval (default: 1000ms).  |\n| Pause          | Halt auto-advance. Current animation completes.                            |\n| Seek           | Jump directly to a target step. No intermediate transitions.               |\n\n---\n\n## Canvas Manager\n\nThe canvas manager handles the physical canvas element: sizing, DPI scaling, resize handling, and the render loop.\n\n### Sizing\n\nThe canvas occupies a designated region of the algorithm page (the \"canvas slot\"). Its dimensions are determined by the parent container and communicated to the layout system.\n\n```typescript\ninterface CanvasDimensions {\n  /** CSS pixel width of the canvas. */\n  readonly width: number;\n  /** CSS pixel height of the canvas. */\n  readonly height: number;\n  /** Device pixel ratio (window.devicePixelRatio). */\n  readonly dpr: number;\n  /** Physical pixel width (width * dpr). */\n  readonly physicalWidth: number;\n  /** Physical pixel height (height * dpr). */\n  readonly physicalHeight: number;\n}\n```\n\n### DPI Scaling\n\nOn high-DPI displays (Retina, 4K), the canvas manager scales the canvas to match the device pixel ratio:\n\n1. The canvas element's `width` and `height` attributes are set to `cssWidth * dpr` and `cssHeight * dpr`.\n2. The canvas element's CSS `width` and `height` are set to the CSS dimensions.\n3. The 2D rendering context is scaled by `dpr` so that all drawing operations use CSS-pixel coordinates.\n\nThis produces crisp rendering on all display densities without requiring layouts to handle DPI manually.\n\n### Resize Handling\n\nWhen the browser window is resized:\n\n1. A `ResizeObserver` detects the change in the canvas slot's dimensions.\n2. The canvas manager debounces resize events (100ms).\n3. The canvas is resized to match the new dimensions.\n4. The layout is notified of the new dimensions and re-renders the current step.\n5. No animation is applied during resize -- the new layout is rendered immediately.\n\n### Render Loop\n\nThe canvas manager does not run a continuous render loop. Rendering is event-driven:\n\n1. **Step change**: When the active step changes (via user interaction or auto-play), the canvas manager triggers a render.\n2. **Animation frame**: During a step transition, the animation manager requests frames via `requestAnimationFrame`. Each frame triggers a render.\n3. **Resize**: Canvas resize triggers a single render.\n\nBetween events, no rendering occurs. This minimizes CPU and GPU usage when the visualization is idle.\n\n---\n\n## Canvas Slot Architecture\n\nThe algorithm page layout designates a rectangular region -- the canvas slot -- for the visualization. The canvas slot is embedded in the page alongside other UI components.\n\n```\n\n  Algorithm Title & Description                    \n\n                                                   \n     \n                                              \n      Canvas Slot             Code Panel      \n      (visualization)         (syntax         \n                               highlighted)    \n                                              \n     \n                                                   \n   \n    Playback Controls                             \n    [|\u003C] [\u003C] [>] [>|] [Play]  3/12   \n   \n                                                   \n   \n    Step Explanation                              \n    \"mid = floor((0 + 9) / 2) = 4.              \n     array[4] = 9. Since 9 \u003C 13, search right.\"  \n   \n                                                   \n   \n    State Inspector                               \n    array: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]  \n    target: 13  left: 5  right: 9  mid: 4        \n   \n                                                   \n  Educational Content, Quiz, Related Algorithms    \n\n```\n\nThe canvas slot is:\n\n- **Responsive**: It adapts to the available width, maintaining a minimum aspect ratio.\n- **Isolated**: The canvas manager owns the canvas element entirely. No other component writes to it.\n- **Lazy-rendered**: The canvas is initialized when the page loads but does not render until the first step is available (either pre-computed or freshly generated).\n\nThe canvas slot communicates with surrounding UI components through a shared step state:\n\n| UI Component       | Reads From Step                              | Writes To                |\n| ------------------- | -------------------------------------------- | ------------------------ |\n| Canvas Slot         | `visualActions`, `state` (via layout)        | --                       |\n| Code Panel          | `codeHighlight.language`, `codeHighlight.lines` | --                    |\n| Step Explanation    | `title`, `explanation`                       | --                       |\n| State Inspector     | `state`                                      | --                       |\n| Playback Controls   | `index`, total step count, `isTerminal`      | Active step index        |\n| Phase Indicator     | `phase`                                      | --                       |\n\nAll components react to the same active step. When the playback controls change the active step index, all components update simultaneously.","src/content/docs/architecture/rendering-engine.mdx","1af83b5dba34d479","contributing/adding-a-layout",{"id":342,"data":344,"body":350,"filePath":351,"digest":352,"deferredRender":16},{"title":345,"description":346,"editUrl":16,"head":347,"template":50,"sidebar":348,"pagefind":16,"draft":39},"Adding a Layout","Guide for creating new layout functions that convert algorithm steps into visual primitives for rendering on the Eigenvue canvas.",[],{"hidden":39,"attrs":349},{},"A layout is a **pure function** that transforms a `Step` object into a\n`PrimitiveScene` -- the set of visual elements rendered on the canvas. If none\nof the existing 10 layouts fit your algorithm's visualization needs, you will\nneed to create a new one.\n\n---\n\n## What Is a Layout?\n\nA layout function reads the `state` and `visualActions` from a `Step` and\nproduces a `PrimitiveScene` containing render primitives (elements, connections,\ncontainers, annotations, and overlays). The rendering engine draws these\nprimitives on the HTML5 Canvas.\n\n```\nStep (from generator)\n  \n  \nLayout Function (pure function)\n  \n  \nPrimitiveScene (array of render primitives)\n  \n  \nCanvas Renderer (draws to screen)\n```\n\nLayouts are **pure functions**: given the same `Step` and `CanvasSize`, they\nalways return the same `PrimitiveScene`. No side effects, no canvas access, no\nexternal state.\n\n---\n\n## When to Create a New Layout\n\nYou need a new layout when:\n\n- No existing layout can represent your algorithm's visualization\n  (e.g., a circular arrangement, a tree structure, a 3D projection).\n- An existing layout is close but would need significant modifications that\n  would break other algorithms using it.\n\nYou do **not** need a new layout when:\n\n- An existing layout already handles your visual pattern. For example, both\n  binary search and linear search use `array-with-pointers`.\n- You only need different colors or pointer names -- those are handled by\n  `meta.json`'s `visual.components` config, not the layout itself.\n\n### Existing Layouts\n\n| Layout                | Used By                                    | Visual Pattern               |\n|-----------------------|--------------------------------------------|------------------------------|\n| `array-with-pointers` | Binary search, linear search               | Horizontal array + pointers  |\n| `array-comparison`    | Bubble sort, quicksort, merge sort         | Array + comparison indicators|\n| `graph-network`       | BFS, DFS, Dijkstra                         | Force-directed graph         |\n| `neuron-diagram`      | Perceptron                                 | Single neuron with inputs    |\n| `layer-network`       | Feedforward network, backpropagation       | Multi-layer network          |\n| `convolution-grid`    | Convolution                                | 2D grid + sliding kernel     |\n| `loss-landscape`      | Gradient descent                           | 3D-like loss surface         |\n| `token-sequence`      | Tokenization (BPE), token embeddings       | Horizontal token chips       |\n| `attention-heatmap`   | Self-attention, multi-head attention       | Token sequence + heatmap     |\n| `layer-diagram`       | Transformer block                          | Vertical block architecture  |\n\n---\n\n## Layout Function Signature\n\nEvery layout function has this signature:\n\n```typescript\ntype LayoutFunction = (\n  step: Step,\n  canvasSize: CanvasSize,\n  config: Record\u003Cstring, unknown>,\n) => PrimitiveScene;\n```\n\n**Parameters:**\n\n| Parameter    | Type                        | Description                                       |\n|--------------|-----------------------------|---------------------------------------------------|\n| `step`       | `Step`                      | The current step (state, visual actions, etc.)     |\n| `canvasSize` | `CanvasSize`                | `{ width, height }` in CSS pixels                 |\n| `config`     | `Record\u003Cstring, unknown>`   | Layout-specific config from `meta.json`'s `visual.components` |\n\n**Returns:** A `PrimitiveScene` object with a `primitives` array.\n\n```typescript\ninterface PrimitiveScene {\n  readonly primitives: readonly RenderPrimitive[];\n}\n```\n\n### Implementation Template\n\nHere is a minimal layout to get you started:\n\n```typescript\n// web/src/engine/layouts/my-layout.ts\n\nimport type {\n  Step,\n  CanvasSize,\n  PrimitiveScene,\n  RenderPrimitive,\n  ElementPrimitive,\n} from \"../types\";\nimport { LAYOUT_PADDING, Z_INDEX } from \"../types\";\nimport { registerLayout } from \"./registry\";\n\nfunction myLayout(\n  step: Step,\n  canvasSize: CanvasSize,\n  _config: Record\u003Cstring, unknown>,\n): PrimitiveScene {\n  const primitives: RenderPrimitive[] = [];\n\n  // 1. Extract data from step.state\n  const data = (step.state as Record\u003Cstring, unknown>).values as number[];\n  if (!data || data.length === 0) {\n    return { primitives: [] };\n  }\n\n  // 2. Compute geometry based on canvas size\n  const usableWidth = canvasSize.width - 2 * LAYOUT_PADDING;\n  const usableHeight = canvasSize.height - 2 * LAYOUT_PADDING;\n\n  // 3. Process visual actions to determine highlighting, colors, etc.\n  for (const action of step.visualActions) {\n    switch (action.type) {\n      case \"highlightElement\": {\n        // Handle highlighting...\n        break;\n      }\n      default:\n        // Unknown actions are silently ignored (open vocabulary contract).\n        break;\n    }\n  }\n\n  // 4. Create primitives\n  for (let i = 0; i \u003C data.length; i++) {\n    const element: ElementPrimitive = {\n      kind: \"element\",\n      id: `item-${i}`,        // Stable ID for animation diffing\n      x: LAYOUT_PADDING + (i + 0.5) * (usableWidth / data.length),\n      y: canvasSize.height / 2,\n      width: 40,\n      height: 40,\n      shape: \"roundedRect\",\n      cornerRadius: 6,\n      fillColor: \"#1e293b\",\n      strokeColor: \"#475569\",\n      strokeWidth: 1,\n      label: String(data[i]),\n      labelFontSize: 16,\n      labelColor: \"#f1f5f9\",\n      subLabel: String(i),\n      subLabelFontSize: 11,\n      subLabelColor: \"#94a3b8\",\n      rotation: 0,\n      opacity: 1,\n      zIndex: Z_INDEX.ELEMENT,\n    };\n    primitives.push(element);\n  }\n\n  return { primitives };\n}\n\n// Self-register\nregisterLayout({\n  name: \"my-layout\",\n  description: \"Description of what this layout visualizes.\",\n  layout: myLayout,\n});\n```\n\n---\n\n## Visual Primitives\n\nThe rendering engine knows how to draw five primitive types. Your layout\nproduces these; the renderer handles everything else (drawing, animation,\nDPI scaling).\n\n### Element\n\nA positioned, shaped item: an array cell, a graph node, a neuron, a token chip.\n\n```typescript\ninterface ElementPrimitive {\n  readonly kind: \"element\";\n  readonly id: string;         // Stable ID for animation diffing\n  x: number;                   // Center x (CSS pixels)\n  y: number;                   // Center y (CSS pixels)\n  width: number;               // Bounding box width\n  height: number;              // Bounding box height\n  shape: ElementShape;         // \"rect\" | \"roundedRect\" | \"circle\" | \"diamond\"\n  cornerRadius: number;        // For \"roundedRect\" only\n  fillColor: string;           // CSS color\n  strokeColor: string;         // CSS color\n  strokeWidth: number;         // CSS pixels\n  label: string;               // Text inside the element\n  labelFontSize: number;\n  labelColor: string;\n  subLabel: string;            // Text below the element\n  subLabelFontSize: number;\n  subLabelColor: string;\n  rotation: number;            // Radians, clockwise from positive x-axis\n  opacity: number;             // [0, 1]\n  zIndex: number;              // Drawing order (lower = behind)\n}\n```\n\n### Connection\n\nA line or curve between two points: graph edges, neural network weights,\nattention arcs.\n\n```typescript\ninterface ConnectionPrimitive {\n  readonly kind: \"connection\";\n  readonly id: string;\n  x1: number; y1: number;     // Start point\n  x2: number; y2: number;     // End point\n  curveOffset: number;         // Bezier control point offset (0 = straight line)\n  color: string;\n  lineWidth: number;\n  dashPattern: number[];       // e.g., [6, 4] for dashed\n  arrowHead: ArrowHead;        // \"none\" | \"end\" | \"start\" | \"both\"\n  arrowSize: number;\n  label: string;               // Label at midpoint\n  labelFontSize: number;\n  labelColor: string;\n  opacity: number;\n  zIndex: number;\n}\n```\n\n### Container\n\nA visual grouping box. Does not \"contain\" children in a tree sense -- it is\nsimply a rectangle drawn at its z-index. You position child elements inside\nits bounds manually.\n\n```typescript\ninterface ContainerPrimitive {\n  readonly kind: \"container\";\n  readonly id: string;\n  x: number; y: number;       // Center of the container\n  width: number; height: number;\n  cornerRadius: number;\n  fillColor: string;\n  strokeColor: string;\n  strokeWidth: number;\n  dashPattern: number[];\n  label: string;               // Displayed at top-center of the container\n  labelFontSize: number;\n  labelColor: string;\n  opacity: number;\n  zIndex: number;\n}\n```\n\n### Annotation\n\nVisual indicators: pointers (triangles), brackets, labels, and badges.\n\n```typescript\ninterface AnnotationPrimitive {\n  readonly kind: \"annotation\";\n  readonly id: string;\n  form: AnnotationForm;        // \"pointer\" | \"bracket\" | \"label\" | \"badge\"\n  x: number; y: number;       // Anchor point (meaning depends on form)\n  text: string;\n  fontSize: number;\n  textColor: string;\n  color: string;               // Shape color\n  // Pointer-specific:\n  pointerHeight: number;       // Triangle height\n  pointerWidth: number;        // Triangle base width\n  // Bracket-specific:\n  bracketWidth: number;\n  bracketTickHeight: number;\n  // Badge-specific:\n  badgePaddingX: number;\n  badgePaddingY: number;\n  opacity: number;\n  zIndex: number;\n}\n```\n\n**Anchor point semantics by form:**\n\n- `\"pointer\"`: `(x, y)` is the tip of the downward-pointing triangle.\n- `\"bracket\"`: `(x, y)` is the left end; bracket extends rightward by `bracketWidth`.\n- `\"label\"`: `(x, y)` is the center of the text.\n- `\"badge\"`: `(x, y)` is the center of the pill shape.\n\n### Overlay\n\nFull-canvas or region-based effects: grids and heatmaps.\n\n```typescript\ninterface OverlayPrimitive {\n  readonly kind: \"overlay\";\n  readonly id: string;\n  readonly overlayType: OverlayType;  // \"grid\" | \"heatmap\"\n  x: number; y: number;              // Top-left corner\n  width: number; height: number;\n  // Grid-specific:\n  gridSpacing: number;\n  gridColor: string;\n  gridLineWidth: number;\n  // Heatmap-specific:\n  heatmapData: number[][];           // Normalized [0, 1] values\n  heatmapColorLow: string;\n  heatmapColorHigh: string;\n  opacity: number;\n  zIndex: number;\n}\n```\n\n---\n\n## Coordinate System\n\nAll coordinates are in **CSS pixels** with the origin at the **top-left corner**\nof the canvas:\n\n- **x** increases to the right.\n- **y** increases downward.\n- Angles are in **radians**, measured clockwise from the positive x-axis.\n\nUse `LAYOUT_PADDING` (40px) as the standard margin around the edges:\n\n```typescript\nimport { LAYOUT_PADDING } from \"../types\";\n\nconst usableWidth = canvasSize.width - 2 * LAYOUT_PADDING;\nconst usableHeight = canvasSize.height - 2 * LAYOUT_PADDING;\n```\n\nElement positions (`x`, `y`) are **center coordinates**, not top-left.\nContainer positions are also center coordinates. Overlay positions are\ntop-left.\n\n---\n\n## Registering a Layout\n\nEvery layout file must self-register by calling `registerLayout()` at module\nload time:\n\n```typescript\nimport { registerLayout } from \"./registry\";\n\nregisterLayout({\n  name: \"my-layout\",           // Must be unique across all layouts\n  description: \"Short human-readable description.\",\n  layout: myLayoutFunction,    // The pure function\n});\n```\n\n**Location:** `web/src/engine/layouts/registry.ts`\n\nThe registry API:\n\n| Function                      | Description                          |\n|-------------------------------|--------------------------------------|\n| `registerLayout(registration)` | Register a layout. Throws if name already exists. |\n| `getLayout(name)`             | Look up a layout by name. Returns `undefined` if not found. |\n| `getRegisteredLayoutNames()`  | List all registered layout names (sorted). |\n| `clearLayoutRegistry()`       | Reset registry (for tests only).     |\n\nAfter creating your layout file, import it in the layouts index to ensure\nit is loaded:\n\n```typescript\n// web/src/engine/layouts/index.ts\n\n// Add your import:\nimport \"./my-layout\";\n```\n\n---\n\n## Testing Layouts\n\n### Unit Tests\n\nTest that your layout produces the expected primitives for known step data:\n\n```typescript\nimport { describe, it, expect, beforeEach } from \"vitest\";\nimport { clearLayoutRegistry, getLayout } from \"../registry\";\nimport \"../my-layout\"; // Triggers self-registration\n\ndescribe(\"my-layout\", () => {\n  beforeEach(() => {\n    // Reset registry between tests to avoid pollution.\n    clearLayoutRegistry();\n    // Re-import to re-register (dynamic import or manual re-call).\n  });\n\n  it(\"produces correct number of elements for a 5-item array\", () => {\n    const layout = getLayout(\"my-layout\")!;\n    const scene = layout(\n      {\n        index: 0,\n        id: \"test\",\n        title: \"Test\",\n        explanation: \"Test step\",\n        state: { values: [1, 2, 3, 4, 5] },\n        visualActions: [],\n        codeHighlight: { language: \"pseudocode\", lines: [1] },\n        isTerminal: false,\n      },\n      { width: 800, height: 600 },\n      {},\n    );\n\n    const elements = scene.primitives.filter((p) => p.kind === \"element\");\n    expect(elements.length).toBe(5);\n  });\n\n  it(\"handles empty state gracefully\", () => {\n    const layout = getLayout(\"my-layout\")!;\n    const scene = layout(\n      {\n        index: 0,\n        id: \"test\",\n        title: \"Test\",\n        explanation: \"Test step\",\n        state: { values: [] },\n        visualActions: [],\n        codeHighlight: { language: \"pseudocode\", lines: [1] },\n        isTerminal: true,\n      },\n      { width: 800, height: 600 },\n      {},\n    );\n\n    expect(scene.primitives.length).toBe(0);\n  });\n\n  it(\"all element IDs are unique\", () => {\n    const layout = getLayout(\"my-layout\")!;\n    const scene = layout(\n      {\n        index: 0,\n        id: \"test\",\n        title: \"Test\",\n        explanation: \"Test step\",\n        state: { values: [10, 20, 30] },\n        visualActions: [],\n        codeHighlight: { language: \"pseudocode\", lines: [1] },\n        isTerminal: false,\n      },\n      { width: 800, height: 600 },\n      {},\n    );\n\n    const ids = scene.primitives.map((p) => p.id);\n    expect(new Set(ids).size).toBe(ids.length);\n  });\n});\n```\n\n### What to Test\n\n- **Correct primitive count** for various input sizes.\n- **Primitive IDs are unique** within a scene (required for animation diffing).\n- **Primitives stay within canvas bounds** (no negative coordinates, no\n  overflow past `canvasSize`).\n- **Visual actions are processed correctly** (highlight colors, pointer\n  positions, dimming).\n- **Empty / edge case inputs** return empty scenes gracefully.\n- **Different canvas sizes** produce proportionally scaled output.\n\n---\n\n## Existing Layouts as Reference\n\nThe best way to learn the layout pattern is to read existing implementations:\n\n| File                          | Complexity | Good For Learning                    |\n|-------------------------------|------------|--------------------------------------|\n| `array-with-pointers.ts`     | Simple     | Basic elements + annotations         |\n| `array-comparison.ts`        | Simple     | Elements + swap animations           |\n| `graph-network.ts`           | Medium     | Elements + connections               |\n| `neuron-diagram.ts`          | Medium     | Custom geometry + connections        |\n| `attention-heatmap.ts`       | Complex    | Overlays + dynamic data              |\n\nAll layout files are in `web/src/engine/layouts/`. Start by reading\n`array-with-pointers.ts` -- it is the simplest and most thoroughly commented.\n\n### Design Tips\n\n1. **Use the Z_INDEX constants** for consistent layering:\n   ```typescript\n   import { Z_INDEX } from \"../types\";\n   // Z_INDEX.OVERLAY_BACKGROUND = 0\n   // Z_INDEX.CONTAINER = 10\n   // Z_INDEX.CONNECTION = 20\n   // Z_INDEX.ELEMENT = 30\n   // Z_INDEX.ANNOTATION = 40\n   // Z_INDEX.OVERLAY_FOREGROUND = 50\n   ```\n\n2. **Keep IDs stable across steps.** The animation engine diffs scenes by\n   primitive ID. If `cell-3` in step N maps to `cell-3` in step N+1, the\n   engine smoothly animates the transition. If you change the ID, the engine\n   treats it as a new element (fade in) and the old one as removed (fade out).\n\n3. **Respond to canvas size.** Layouts should scale gracefully. Use relative\n   positioning based on `canvasSize` and `LAYOUT_PADDING`, not hardcoded\n   pixel values.\n\n4. **Process all visual actions, ignore unknowns.** Use a `switch` with a\n   `default: break;` case. Never throw on unrecognized action types.\n\n5. **Use the theme constants** from your layout file (like `array-with-pointers`\n   does with its `THEME` object) for consistent colors across the dark UI.","src/content/docs/contributing/adding-a-layout.mdx","2738c4b8c2109b8c","contributing/adding-an-algorithm",{"id":353,"data":355,"body":361,"filePath":362,"digest":363,"deferredRender":16},{"title":356,"description":357,"editUrl":16,"head":358,"template":50,"sidebar":359,"pagefind":16,"draft":39},"Adding an Algorithm","End-to-end walkthrough for adding a new algorithm to Eigenvue, from planning through TypeScript and Python generators, tests, registration, and pull request submission.",[],{"hidden":39,"attrs":360},{},"This is the definitive guide for adding a new algorithm to Eigenvue. We will walk\nthrough every step using **Linear Search** as a concrete example, from initial\nplanning to a merge-ready pull request.\n\nBy the end of this guide you will have created:\n\n1. A `meta.json` with full algorithm metadata\n2. A TypeScript generator that yields step-by-step visualization data\n3. Test fixtures and unit tests (Vitest)\n4. A Python generator producing identical output\n5. Cross-language parity validation\n6. Pre-computed step data for zero-latency initial page loads\n\n---\n\n## Step 1: Plan Your Algorithm\n\nBefore writing any code, decide three things:\n\n| Decision     | Value for Our Example       | Where It Matters                           |\n|--------------|-----------------------------|--------------------------------------------|\n| **ID**       | `linear-search`             | Directory name, URL slug, all cross-refs   |\n| **Category** | `classical`                 | Directory path, catalog grouping           |\n| **Layout**   | `array-with-pointers`       | Visual rendering strategy                  |\n\nThe **algorithm ID** must be URL-safe, lowercase, and hyphen-separated. It must\nmatch the regex `^[a-z0-9][a-z0-9-]*$`.\n\nThe **layout** determines which visual rendering function interprets your\ngenerator's step data. Browse the\n[10 existing layouts](/docs/api-reference/layout-system/) to find one that fits, or\n[create a new layout](/docs/contributing/adding-a-layout/) if none does.\n\nAvailable layouts:\n\n| Layout                | Best For                                        |\n|-----------------------|-------------------------------------------------|\n| `array-with-pointers` | Array algorithms with named pointers            |\n| `array-comparison`    | Sorting algorithms with element comparisons     |\n| `graph-network`       | Graph traversal and shortest path               |\n| `neuron-diagram`      | Single neuron / perceptron                       |\n| `layer-network`       | Multi-layer neural networks                     |\n| `convolution-grid`    | 2D convolution operations                       |\n| `loss-landscape`      | Gradient descent and optimization               |\n| `token-sequence`      | Tokenization and text processing                |\n| `attention-heatmap`   | Self-attention and multi-head attention          |\n| `layer-diagram`       | Transformer block architecture                  |\n\n---\n\n## Step 2: Create Directory Structure\n\nCreate the algorithm directory and all required files:\n\n```bash\nmkdir -p algorithms/classical/linear-search/tests\ntouch algorithms/classical/linear-search/meta.json\ntouch algorithms/classical/linear-search/generator.ts\ntouch algorithms/classical/linear-search/tests/generator.test.ts\n```\n\nYour directory should look like this:\n\n```\nalgorithms/classical/linear-search/\n  meta.json              # Algorithm metadata (display info, inputs, education)\n  generator.ts           # TypeScript step generator\n  tests/\n    generator.test.ts    # Vitest unit tests\n    found.fixture.json   # Golden test fixture: target found\n    not-found.fixture.json  # Golden test fixture: target not found\n    single-element.fixture.json  # Golden test fixture: edge case\n```\n\n---\n\n## Step 3: Write `meta.json`\n\nThe `meta.json` file is the single source of truth for everything about your\nalgorithm that is **data** (not logic): display information, input schemas,\neducational content, visual configuration, and SEO metadata.\n\nHere is the complete `meta.json` for Linear Search, annotated with explanations:\n\n```json\n{\n  // URL-safe unique identifier. Must match directory name.\n  \"id\": \"linear-search\",\n\n  // Human-readable display name shown in the catalog and page title.\n  \"name\": \"Linear Search\",\n\n  // Category determines the catalog section and directory path.\n  // Options: \"classical\", \"deep-learning\", \"generative-ai\", \"quantum\"\n  \"category\": \"classical\",\n\n  \"description\": {\n    // Short description for cards and meta tags. Max 80 characters.\n    \"short\": \"Scan an array element by element to find a target value.\",\n    // Long description supports markdown. Shown on the algorithm page.\n    \"long\": \"Linear search (also called sequential search) checks every element in an array one by one until it finds the target or reaches the end. While simple, it works on unsorted arrays  unlike binary search, which requires sorted input. Linear search has O(n) time complexity, making it practical for small datasets or unsorted collections.\"\n  },\n\n  \"complexity\": {\n    \"time\": \"O(n)\",\n    \"space\": \"O(1)\",\n    // Options: \"beginner\", \"intermediate\", \"advanced\", \"expert\"\n    \"level\": \"beginner\"\n  },\n\n  \"visual\": {\n    // Must match a registered layout name in web/src/engine/layouts/registry.ts\n    \"layout\": \"array-with-pointers\",\n    \"theme\": {\n      \"primary\": \"#38bdf8\",\n      \"secondary\": \"#0284c7\"\n    },\n    // Layout-specific configuration. Shape depends on the layout.\n    \"components\": {\n      \"pointers\": [\n        { \"id\": \"i\", \"label\": \"i\", \"color\": \"#f472b6\" }\n      ],\n      \"showIndices\": true,\n      \"showValues\": true,\n      \"highlightColor\": \"#fbbf24\",\n      \"foundColor\": \"#22c55e\",\n      \"dimColor\": \"rgba(160, 168, 192, 0.15)\",\n      \"compareColor\": \"#f472b6\"\n    }\n  },\n\n  \"inputs\": {\n    // JSON Schema for input validation and UI generation.\n    \"schema\": {\n      \"array\": {\n        \"type\": \"array\",\n        \"items\": { \"type\": \"number\", \"minimum\": -999, \"maximum\": 999 },\n        \"minItems\": 1,\n        \"maxItems\": 20,\n        \"description\": \"The array to search through (does not need to be sorted).\"\n      },\n      \"target\": {\n        \"type\": \"number\",\n        \"minimum\": -999,\n        \"maximum\": 999,\n        \"description\": \"The value to search for.\"\n      }\n    },\n    // Default values shown when the page first loads.\n    \"defaults\": {\n      \"array\": [4, 2, 7, 1, 9, 3, 8, 5],\n      \"target\": 3\n    },\n    // Named presets users can select from a dropdown.\n    \"examples\": [\n      {\n        \"name\": \"Default (target found)\",\n        \"values\": { \"array\": [4, 2, 7, 1, 9, 3, 8, 5], \"target\": 3 }\n      },\n      {\n        \"name\": \"Target not found\",\n        \"values\": { \"array\": [4, 2, 7, 1, 9], \"target\": 6 }\n      },\n      {\n        \"name\": \"Single element (found)\",\n        \"values\": { \"array\": [42], \"target\": 42 }\n      },\n      {\n        \"name\": \"Target is first element\",\n        \"values\": { \"array\": [3, 7, 1, 9, 5], \"target\": 3 }\n      },\n      {\n        \"name\": \"Target is last element\",\n        \"values\": { \"array\": [3, 7, 1, 9, 5], \"target\": 5 }\n      }\n    ]\n  },\n\n  \"code\": {\n    \"implementations\": {\n      // At minimum, \"pseudocode\" must be present.\n      \"pseudocode\": \"function linearSearch(array, target):\\n  for i = 0 to length(array) - 1:\\n    if array[i] == target:\\n      return i          // Found!\\n  return -1               // Not found\",\n      \"python\": \"def linear_search(array: list[int], target: int) -> int:\\n    for i in range(len(array)):\\n        if array[i] == target:\\n            return i\\n    return -1\",\n      \"javascript\": \"function linearSearch(array, target) {\\n  for (let i = 0; i \u003C array.length; i++) {\\n    if (array[i] === target) {\\n      return i;\\n    }\\n  }\\n  return -1;\\n}\"\n    },\n    \"defaultLanguage\": \"pseudocode\"\n  },\n\n  \"education\": {\n    \"keyConcepts\": [\n      {\n        \"title\": \"Sequential Scan\",\n        \"description\": \"Linear search examines each element in order from index 0 to n-1. It is the simplest search algorithm and requires no preconditions on the array (unlike binary search, which needs sorted input).\"\n      },\n      {\n        \"title\": \"Linear Time Complexity\",\n        \"description\": \"In the worst case, every element must be checked, giving O(n) time. On average, the target is found after n/2 comparisons. Best case is O(1) if the target is the first element.\"\n      }\n    ],\n    \"pitfalls\": [\n      {\n        \"title\": \"Using linear search on sorted data\",\n        \"description\": \"If the array is sorted, binary search is O(log n)  exponentially faster. Linear search should only be preferred when the data is unsorted or the array is very small.\"\n      }\n    ],\n    \"quiz\": [\n      {\n        \"question\": \"What is the average number of comparisons for linear search on an array of 100 elements?\",\n        \"options\": [\"10\", \"50\", \"100\", \"7\"],\n        \"correctIndex\": 1,\n        \"explanation\": \"On average, the target is equally likely to be at any position, so the expected number of comparisons is (1 + 2 + ... + n) / n = (n + 1) / 2, which is about 50 for n = 100.\"\n      }\n    ],\n    \"resources\": [\n      {\n        \"title\": \"Linear Search  Wikipedia\",\n        \"url\": \"https://en.wikipedia.org/wiki/Linear_search\",\n        \"type\": \"reference\"\n      }\n    ]\n  },\n\n  \"seo\": {\n    \"keywords\": [\n      \"linear search visualization\",\n      \"sequential search algorithm\",\n      \"linear search step by step\",\n      \"linear search animation\"\n    ],\n    \"ogDescription\": \"Interactive step-by-step visualization of linear search. Watch how the algorithm scans an array to find a target value.\"\n  },\n\n  \"prerequisites\": [],\n  \"related\": [\"binary-search\", \"bubble-sort\"],\n  \"author\": \"eigenvue\",\n  \"version\": \"1.0.0\"\n}\n```\n\n:::note\nJSON does not officially support comments. The comments above are for\nillustration. Your actual `meta.json` must be valid JSON (no comments). Use the\nschema at `shared/meta.schema.json` to validate your file.\n:::\n\n---\n\n## Step 4: Write the TypeScript Generator\n\nThe generator is the heart of your algorithm. It is a JavaScript generator\nfunction (`function*`) that yields `Step` objects at each \"interesting moment\"\nof the algorithm's execution.\n\nCreate `algorithms/classical/linear-search/generator.ts`:\n\n```typescript\n/**\n * @fileoverview Linear Search  Step Generator\n *\n * Generates a step-by-step visualization of linear search on an array.\n *\n * ALGORITHM:\n *   Iterate through the array from index 0 to n-1. At each position,\n *   compare array[i] with the target. If equal, return i. If the loop\n *   completes without finding the target, return -1.\n *\n * STEP GENERATION STRATEGY:\n *   - Initialization: show the full array and target\n *   - Each comparison: highlight the current element, show result\n *   - Found / Not found: terminal step with result\n *\n * STATE SNAPSHOT SAFETY:\n *   The array is spread-copied in every state snapshot: [...array].\n *   Primitive values (i, target) are safe by value.\n */\n\nimport { createGenerator } from \"@/engine/generator\";\n\n//  Input Type \n\n/**\n * Input parameters for linear search.\n *\n * CONSTRAINTS (enforced by meta.json schema):\n * - array: 1-20 elements, each value in [-999, 999]\n * - target: a number in [-999, 999]\n */\ninterface LinearSearchInputs extends Record\u003Cstring, unknown> {\n  readonly array: readonly number[];\n  readonly target: number;\n}\n\n//  Generator \n\nexport default createGenerator\u003CLinearSearchInputs>({\n  // Must match the \"id\" field in meta.json exactly.\n  id: \"linear-search\",\n\n  *generate(inputs, step) {\n    const { array, target } = inputs;\n\n    //  Step: Initialize \n    // Show the full array. No pointer yet.\n    yield step({\n      id: \"initialize\",\n      title: \"Initialize Search\",\n      explanation:\n        `Searching for ${target} in an array of ${array.length} ` +\n        `element${array.length === 1 ? \"\" : \"s\"}. ` +\n        `Will check each element from left to right.`,\n      state: {\n        array: [...array],  // IMPORTANT: spread copy, not reference\n        target,\n        i: -1,\n        result: null,\n      },\n      visualActions: [\n        // Highlight the entire array as the search space.\n        { type: \"highlightRange\", from: 0, to: array.length - 1, color: \"highlight\" },\n      ],\n      codeHighlight: { language: \"pseudocode\", lines: [1] },\n      phase: \"initialization\",\n    });\n\n    //  Main loop \n    for (let i = 0; i \u003C array.length; i++) {\n      //  Step: Compare \n      const isMatch = array[i] === target;\n\n      yield step({\n        id: \"compare\",\n        title: `Compare Element at Index ${i}`,\n        explanation:\n          `Checking array[${i}] = ${array[i]}. ` +\n          (isMatch\n            ? `It equals the target ${target}!`\n            : `${array[i]} !== ${target}, moving to the next element.`),\n        state: {\n          array: [...array],\n          target,\n          i,\n          result: null,\n        },\n        visualActions: [\n          // Dim elements we have already checked.\n          ...(i > 0 ? [{ type: \"dimRange\" as const, from: 0, to: i - 1 }] : []),\n          // Highlight the current element being compared.\n          { type: \"highlightElement\", index: i, color: \"compare\" },\n          // Move the pointer to the current position.\n          { type: \"movePointer\", id: \"i\", to: i },\n        ],\n        codeHighlight: { language: \"pseudocode\", lines: [2, 3] },\n        phase: \"search\",\n      });\n\n      if (isMatch) {\n        //  Step: Found \n        yield step({\n          id: \"found\",\n          title: \"Target Found!\",\n          explanation:\n            `array[${i}] = ${array[i]} equals target ${target}. ` +\n            `Found at index ${i} after checking ${i + 1} element${i === 0 ? \"\" : \"s\"}.`,\n          state: {\n            array: [...array],\n            target,\n            i,\n            result: i,\n          },\n          visualActions: [\n            { type: \"markFound\", index: i },\n            { type: \"movePointer\", id: \"i\", to: i },\n            {\n              type: \"showMessage\",\n              text: `Found ${target} at index ${i}!`,\n              messageType: \"success\",\n            },\n          ],\n          codeHighlight: { language: \"pseudocode\", lines: [4] },\n          phase: \"result\",\n          isTerminal: true,  // This is the final step.\n        });\n        return; // Generator ends here.\n      }\n    }\n\n    //  Step: Not Found \n    yield step({\n      id: \"not_found\",\n      title: \"Target Not Found\",\n      explanation:\n        `Checked all ${array.length} elements. ` +\n        `${target} is not in the array. Returning -1.`,\n      state: {\n        array: [...array],\n        target,\n        i: array.length,\n        result: -1,\n      },\n      visualActions: [\n        { type: \"dimRange\", from: 0, to: array.length - 1 },\n        { type: \"markNotFound\" },\n        {\n          type: \"showMessage\",\n          text: `${target} was not found in the array.`,\n          messageType: \"warning\",\n        },\n      ],\n      codeHighlight: { language: \"pseudocode\", lines: [5] },\n      phase: \"result\",\n      isTerminal: true,  // This is the final step.\n    });\n  },\n});\n```\n\n### Key Generator Rules\n\nUnderstanding these rules prevents the most common generator mistakes:\n\n1. **Always yield at least one step.** The runner rejects generators that produce zero steps.\n\n2. **Exactly one step must have `isTerminal: true`, and it must be the last.**\n   The runner validates this invariant after the generator finishes.\n\n3. **State snapshots must be independent.** Use `[...array]` (spread) or\n   `structuredClone()` for mutable data. Never pass a reference to a variable\n   that will be mutated later. If you do, all steps will point to the same\n   mutated object, and stepping backward in the UI will show incorrect state.\n\n4. **Step IDs can repeat.** The `id` field is a template identifier, not a\n   unique key. For example, `\"compare\"` appears once per loop iteration. The\n   `index` field (auto-assigned by the runner) is the unique identifier.\n\n5. **Code highlight lines are 1-indexed.** Line 1 means the first line of the\n   pseudocode in `meta.json`.\n\n6. **The generator must terminate.** The runner enforces a default limit of\n   10,000 steps as a safety net against infinite loops.\n\n---\n\n## Step 5: Write Test Fixtures\n\nTest fixtures are golden JSON files that define the expected step output for\nspecific inputs. They are the ground truth for both TypeScript and Python\ngenerators.\n\nCreate `algorithms/classical/linear-search/tests/found.fixture.json`:\n\n```json\n{\n  \"description\": \"Target 3 found at index 5 in [4, 2, 7, 1, 9, 3, 8, 5].\",\n  \"inputs\": {\n    \"array\": [4, 2, 7, 1, 9, 3, 8, 5],\n    \"target\": 3\n  },\n  \"expected\": {\n    \"stepCount\": 8,\n    \"terminalStepId\": \"found\",\n    \"result\": 5,\n    \"stepIds\": [\n      \"initialize\",\n      \"compare\",\n      \"compare\",\n      \"compare\",\n      \"compare\",\n      \"compare\",\n      \"compare\",\n      \"found\"\n    ],\n    \"keyStates\": [\n      { \"stepIndex\": 0, \"field\": \"i\", \"value\": -1 },\n      { \"stepIndex\": 1, \"field\": \"i\", \"value\": 0 },\n      { \"stepIndex\": 6, \"field\": \"i\", \"value\": 5 },\n      { \"stepIndex\": 7, \"field\": \"result\", \"value\": 5 }\n    ]\n  }\n}\n```\n\nCreate `algorithms/classical/linear-search/tests/not-found.fixture.json`:\n\n```json\n{\n  \"description\": \"Target 6 not found in [4, 2, 7, 1, 9].\",\n  \"inputs\": {\n    \"array\": [4, 2, 7, 1, 9],\n    \"target\": 6\n  },\n  \"expected\": {\n    \"stepCount\": 7,\n    \"terminalStepId\": \"not_found\",\n    \"result\": -1,\n    \"stepIds\": [\n      \"initialize\",\n      \"compare\",\n      \"compare\",\n      \"compare\",\n      \"compare\",\n      \"compare\",\n      \"not_found\"\n    ],\n    \"keyStates\": [\n      { \"stepIndex\": 0, \"field\": \"i\", \"value\": -1 },\n      { \"stepIndex\": 5, \"field\": \"i\", \"value\": 4 },\n      { \"stepIndex\": 6, \"field\": \"result\", \"value\": -1 }\n    ]\n  }\n}\n```\n\nCreate `algorithms/classical/linear-search/tests/single-element.fixture.json`:\n\n```json\n{\n  \"description\": \"Single element array, target found.\",\n  \"inputs\": {\n    \"array\": [42],\n    \"target\": 42\n  },\n  \"expected\": {\n    \"stepCount\": 3,\n    \"terminalStepId\": \"found\",\n    \"result\": 0,\n    \"stepIds\": [\n      \"initialize\",\n      \"compare\",\n      \"found\"\n    ],\n    \"keyStates\": [\n      { \"stepIndex\": 0, \"field\": \"i\", \"value\": -1 },\n      { \"stepIndex\": 1, \"field\": \"i\", \"value\": 0 },\n      { \"stepIndex\": 2, \"field\": \"result\", \"value\": 0 }\n    ]\n  }\n}\n```\n\n### Fixture Design Principles\n\n- **Cover the core paths.** At minimum, create fixtures for: found, not found,\n  and single-element cases.\n\n- **Include edge cases.** Target at the first index, target at the last index,\n  and duplicate values are all worth testing.\n\n- **Keep fixtures small.** Use arrays of 5--10 elements. Smaller arrays make\n  the expected step sequences easy to compute by hand and verify.\n\n- **State checks should be surgical.** Only assert on the fields that matter\n  for the specific test. The `keyStates` array lets you spot-check specific\n  state fields at specific step indices without writing out the entire step.\n\n---\n\n## Step 6: Write Unit Tests (Vitest)\n\nCreate `algorithms/classical/linear-search/tests/generator.test.ts`:\n\n```typescript\n/**\n * @fileoverview Linear Search Generator  Unit Tests\n */\n\nimport { describe, it, expect } from \"vitest\";\nimport { runGenerator } from \"@/engine/generator\";\nimport linearSearchGenerator from \"../generator\";\n\n// Import fixtures.\nimport foundFixture from \"./found.fixture.json\";\nimport notFoundFixture from \"./not-found.fixture.json\";\nimport singleElementFixture from \"./single-element.fixture.json\";\n\n//  Helper \n\nfunction assertFixture(fixture: {\n  description: string;\n  inputs: { array: number[]; target: number };\n  expected: {\n    stepCount: number;\n    terminalStepId: string;\n    result: number;\n    stepIds: string[];\n    keyStates: Array\u003C{ stepIndex: number; field: string; value: unknown }>;\n  };\n}) {\n  const result = runGenerator(linearSearchGenerator, fixture.inputs);\n  const { steps } = result;\n\n  // 1. Step count.\n  expect(steps.length).toBe(fixture.expected.stepCount);\n\n  // 2. Terminal step ID.\n  const terminalStep = steps[steps.length - 1]!;\n  expect(terminalStep.id).toBe(fixture.expected.terminalStepId);\n  expect(terminalStep.isTerminal).toBe(true);\n\n  // 3. Result value in terminal state.\n  expect(terminalStep.state.result).toBe(fixture.expected.result);\n\n  // 4. Step ID sequence.\n  const actualStepIds = steps.map((s) => s.id);\n  expect(actualStepIds).toEqual(fixture.expected.stepIds);\n\n  // 5. Key state values.\n  for (const check of fixture.expected.keyStates) {\n    const stepState = steps[check.stepIndex]!.state;\n    expect(stepState[check.field]).toEqual(check.value);\n  }\n}\n\n//  Fixture Tests \n\ndescribe(\"Linear Search Generator  Fixture Tests\", () => {\n  it(\"found: default example\", () => {\n    assertFixture(foundFixture);\n  });\n\n  it(\"not found: target absent from array\", () => {\n    assertFixture(notFoundFixture);\n  });\n\n  it(\"single element: target found\", () => {\n    assertFixture(singleElementFixture);\n  });\n});\n\n//  Format Compliance Tests \n\ndescribe(\"Linear Search Generator  Format Compliance\", () => {\n  const result = runGenerator(linearSearchGenerator, {\n    array: [4, 2, 7, 1, 9, 3],\n    target: 9,\n  });\n\n  it(\"produces a valid StepSequence\", () => {\n    expect(result.formatVersion).toBe(1);\n    expect(result.algorithmId).toBe(\"linear-search\");\n    expect(result.generatedBy).toBe(\"typescript\");\n    expect(result.steps.length).toBeGreaterThan(0);\n  });\n\n  it(\"all step indices are contiguous and 0-based\", () => {\n    result.steps.forEach((step, i) => {\n      expect(step.index).toBe(i);\n    });\n  });\n\n  it(\"exactly one step is terminal and it is the last\", () => {\n    const terminalSteps = result.steps.filter((s) => s.isTerminal);\n    expect(terminalSteps.length).toBe(1);\n    expect(terminalSteps[0]!.index).toBe(result.steps.length - 1);\n  });\n\n  it(\"all step IDs match the required pattern\", () => {\n    const pattern = /^[a-z0-9][a-z0-9_-]*$/;\n    for (const step of result.steps) {\n      expect(step.id).toMatch(pattern);\n    }\n  });\n\n  it(\"all code highlight lines are positive integers\", () => {\n    for (const step of result.steps) {\n      for (const line of step.codeHighlight.lines) {\n        expect(Number.isInteger(line)).toBe(true);\n        expect(line).toBeGreaterThanOrEqual(1);\n      }\n    }\n  });\n});\n\n//  State Immutability Tests \n\ndescribe(\"Linear Search Generator  State Immutability\", () => {\n  it(\"step states are independent snapshots\", () => {\n    const result = runGenerator(linearSearchGenerator, {\n      array: [10, 20, 30],\n      target: 20,\n    });\n\n    const arrays = result.steps.map((s) => s.state.array as number[]);\n\n    // All arrays should contain the same values.\n    for (const arr of arrays) {\n      expect(arr).toEqual([10, 20, 30]);\n    }\n\n    // But they should NOT be the same reference.\n    for (let i = 0; i \u003C arrays.length - 1; i++) {\n      expect(arrays[i]).not.toBe(arrays[i + 1]);\n    }\n  });\n});\n\n//  Edge Case Tests \n\ndescribe(\"Linear Search Generator  Edge Cases\", () => {\n  it(\"target is the first element (best case)\", () => {\n    const result = runGenerator(linearSearchGenerator, {\n      array: [5, 3, 8, 1],\n      target: 5,\n    });\n    const terminal = result.steps[result.steps.length - 1]!;\n    expect(terminal.id).toBe(\"found\");\n    expect(terminal.state.result).toBe(0);\n  });\n\n  it(\"target is the last element (worst case for found)\", () => {\n    const result = runGenerator(linearSearchGenerator, {\n      array: [5, 3, 8, 1],\n      target: 1,\n    });\n    const terminal = result.steps[result.steps.length - 1]!;\n    expect(terminal.id).toBe(\"found\");\n    expect(terminal.state.result).toBe(3);\n  });\n\n  it(\"all duplicate elements, target found\", () => {\n    const result = runGenerator(linearSearchGenerator, {\n      array: [7, 7, 7, 7],\n      target: 7,\n    });\n    const terminal = result.steps[result.steps.length - 1]!;\n    expect(terminal.id).toBe(\"found\");\n    // Linear search finds the first occurrence.\n    expect(terminal.state.result).toBe(0);\n  });\n\n  it(\"step count is bounded by n + 2\", () => {\n    // n elements => 1 init + n compares + 1 result = n + 2 max\n    const result = runGenerator(linearSearchGenerator, {\n      array: [1, 2, 3, 4, 5, 6, 7, 8],\n      target: 999,\n    });\n    expect(result.steps.length).toBeLessThanOrEqual(10); // 8 + 2\n  });\n});\n```\n\nRun the tests:\n\n```bash\ncd web\nnpx vitest run algorithms/classical/linear-search/tests/generator.test.ts\n```\n\n---\n\n## Step 7: Write the Python Generator\n\nEvery TypeScript generator must have a corresponding Python generator that\nproduces **identical** output for the same inputs. This is required for the\nPyPI package and for JOSS reproducibility.\n\nCreate `python/src/eigenvue/generators/classical/linear_search.py`:\n\n```python\n\"\"\"\nLinear Search  Step Generator (Python)\n\nProduces step-by-step visualization data identical to the TypeScript\ngenerator when given the same inputs. Uses snake_case internally but\noutputs camelCase JSON via the Step dataclass's to_dict() method.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any\n\nfrom eigenvue._step_types import Step, VisualAction, CodeHighlight\n\n\ndef generate(inputs: dict[str, Any]) -> list[Step]:\n    \"\"\"Generate linear search steps.\n\n    Parameters\n    ----------\n    inputs : dict\n        Must contain:\n        - array: list[int]  The array to search.\n        - target: int  The value to search for.\n\n    Returns\n    -------\n    list[Step]\n        Ordered list of Step dataclass objects.\n    \"\"\"\n    array: list[int] = list(inputs[\"array\"])\n    target: int = inputs[\"target\"]\n    steps: list[Step] = []\n    n = len(array)\n\n    #  Step: Initialize \n    steps.append(Step(\n        index=0,\n        id=\"initialize\",\n        title=\"Initialize Search\",\n        explanation=(\n            f\"Searching for {target} in an array of {n} \"\n            f\"element{'s' if n != 1 else ''}. \"\n            f\"Will check each element from left to right.\"\n        ),\n        state={\n            \"array\": list(array),  # Copy!\n            \"target\": target,\n            \"i\": -1,\n            \"result\": None,\n        },\n        visual_actions=[\n            VisualAction(type=\"highlightRange\", from_=0, to=n - 1, color=\"highlight\"),\n        ],\n        code_highlight=CodeHighlight(language=\"pseudocode\", lines=[1]),\n        is_terminal=False,\n        phase=\"initialization\",\n    ))\n\n    #  Main loop \n    step_index = 1\n    for i in range(n):\n        is_match = array[i] == target\n\n        #  Step: Compare \n        dim_actions: list[VisualAction] = []\n        if i > 0:\n            dim_actions.append(\n                VisualAction(type=\"dimRange\", from_=0, to=i - 1)\n            )\n\n        steps.append(Step(\n            index=step_index,\n            id=\"compare\",\n            title=f\"Compare Element at Index {i}\",\n            explanation=(\n                f\"Checking array[{i}] = {array[i]}. \"\n                + (\n                    f\"It equals the target {target}!\"\n                    if is_match\n                    else f\"{array[i]} !== {target}, moving to the next element.\"\n                )\n            ),\n            state={\n                \"array\": list(array),\n                \"target\": target,\n                \"i\": i,\n                \"result\": None,\n            },\n            visual_actions=[\n                *dim_actions,\n                VisualAction(type=\"highlightElement\", index=i, color=\"compare\"),\n                VisualAction(type=\"movePointer\", id=\"i\", to=i),\n            ],\n            code_highlight=CodeHighlight(language=\"pseudocode\", lines=[2, 3]),\n            is_terminal=False,\n            phase=\"search\",\n        ))\n        step_index += 1\n\n        if is_match:\n            #  Step: Found \n            steps.append(Step(\n                index=step_index,\n                id=\"found\",\n                title=\"Target Found!\",\n                explanation=(\n                    f\"array[{i}] = {array[i]} equals target {target}. \"\n                    f\"Found at index {i} after checking \"\n                    f\"{i + 1} element{'s' if i > 0 else ''}.\"\n                ),\n                state={\n                    \"array\": list(array),\n                    \"target\": target,\n                    \"i\": i,\n                    \"result\": i,\n                },\n                visual_actions=[\n                    VisualAction(type=\"markFound\", index=i),\n                    VisualAction(type=\"movePointer\", id=\"i\", to=i),\n                    VisualAction(\n                        type=\"showMessage\",\n                        text=f\"Found {target} at index {i}!\",\n                        message_type=\"success\",\n                    ),\n                ],\n                code_highlight=CodeHighlight(language=\"pseudocode\", lines=[4]),\n                is_terminal=True,\n                phase=\"result\",\n            ))\n            return steps\n\n    #  Step: Not Found \n    steps.append(Step(\n        index=step_index,\n        id=\"not_found\",\n        title=\"Target Not Found\",\n        explanation=(\n            f\"Checked all {n} elements. \"\n            f\"{target} is not in the array. Returning -1.\"\n        ),\n        state={\n            \"array\": list(array),\n            \"target\": target,\n            \"i\": n,\n            \"result\": -1,\n        },\n        visual_actions=[\n            VisualAction(type=\"dimRange\", from_=0, to=n - 1),\n            VisualAction(type=\"markNotFound\"),\n            VisualAction(\n                type=\"showMessage\",\n                text=f\"{target} was not found in the array.\",\n                message_type=\"warning\",\n            ),\n        ],\n        code_highlight=CodeHighlight(language=\"pseudocode\", lines=[5]),\n        is_terminal=True,\n        phase=\"result\",\n    ))\n\n    return steps\n```\n\n### TypeScript vs Python: Key Differences\n\n| Aspect                | TypeScript                           | Python                              |\n|-----------------------|--------------------------------------|-------------------------------------|\n| Pattern               | Generator function (`function*`)     | Regular function returning `list`   |\n| Index tracking        | Automatic (runner assigns indices)   | Manual (you set `index` yourself)   |\n| State copy            | `[...array]`                         | `list(array)`                       |\n| Naming                | camelCase (`isTerminal`)             | snake_case (`is_terminal`)          |\n| JSON output           | camelCase natively                   | `to_dict()` converts to camelCase   |\n\n---\n\n## Step 8: Validate Cross-Language Parity\n\nAfter writing both generators, verify that they produce identical output:\n\n```bash\npython scripts/verify-step-parity.py\n```\n\nThis script loads the golden fixtures, deserializes them into Python dataclasses,\nre-serializes them, and compares the JSON byte-for-byte.\n\nFor a deeper manual check, you can run both generators on the same input and\ndiff the output:\n\n```bash\n# Generate steps from TypeScript\ncd web\nnpx tsx -e \"\n  import gen from '../algorithms/classical/linear-search/generator';\n  import { runGenerator } from './src/engine/generator';\n  const result = runGenerator(gen, { array: [4,2,7,1,9,3,8,5], target: 3 });\n  console.log(JSON.stringify(result.steps, null, 2));\n\" > /tmp/ts-steps.json\n\n# Generate steps from Python\ncd python\npython -c \"\nfrom eigenvue.runner import run_generator\nimport json\nsteps = run_generator('linear-search', {'array': [4,2,7,1,9,3,8,5], 'target': 3})\nprint(json.dumps(steps, indent=2))\n\" > /tmp/py-steps.json\n\n# Diff\ndiff /tmp/ts-steps.json /tmp/py-steps.json\n```\n\nThe diff should be empty. If not, see\n[Cross-Language Parity](/docs/contributing/cross-language-parity/) for debugging\nstrategies.\n\n---\n\n## Step 9: Register the Algorithm\n\n### TypeScript Side\n\nThe TypeScript generator is auto-discovered from its file path. No manual\nregistration is needed -- the build system scans `algorithms/*/generator.ts`.\n\n### Python Side\n\nAdd the generator to the registry in\n`python/src/eigenvue/generators/__init__.py`:\n\n```python\n_REGISTRY_MAP: dict[str, tuple[str, str]] = {\n    # ... existing entries ...\n\n    # Add your new algorithm:\n    \"linear-search\": (\"eigenvue.generators.classical.linear_search\", \"generate\"),\n}\n```\n\n---\n\n## Step 10: Pre-compute Steps\n\nFor zero-latency initial page loads, Eigenvue pre-computes steps for default\ninputs at build time. Create the pre-computed directory and generate the data:\n\n```bash\nmkdir -p algorithms/classical/linear-search/precomputed\n```\n\nThe build script (`scripts/bundle-python-data.py`) handles pre-computation\nautomatically. It runs each algorithm's generator with default inputs from\n`meta.json` and writes the result to the `precomputed/` directory.\n\n---\n\n## Step 11: Test End-to-End\n\nRun the full test suite to make sure nothing is broken:\n\n```bash\n# TypeScript tests\ncd web\nnpm run test\n\n# Python tests\ncd python\npytest\n\n# Cross-language parity\npython scripts/verify-step-parity.py\n\n# TypeScript linting and type checking\ncd web\nnpm run lint\nnpm run typecheck\n\n# Python linting and type checking\ncd python\nruff check .\nmypy src/\n```\n\nStart the dev server and visually inspect your algorithm:\n\n```bash\ncd web\nnpm run dev\n# Open http://localhost:3000/algorithms/linear-search\n```\n\nVerify:\n\n- The visualization renders correctly with the default inputs.\n- Stepping forward and backward works smoothly.\n- All example presets from `meta.json` produce correct visualizations.\n- The code panel highlights the correct lines at each step.\n- The explanation text accurately describes each step.\n\n---\n\n## Step 12: Submit Your Pull Request\n\nOnce all tests pass and the visualization looks correct:\n\n```bash\n# Stage your new files\ngit add algorithms/classical/linear-search/\ngit add python/src/eigenvue/generators/classical/linear_search.py\ngit add python/src/eigenvue/generators/__init__.py\n\n# Commit with Conventional Commits format\ngit commit -m \"feat(algorithms): add linear search visualization\n\nAdds complete linear search implementation with:\n- meta.json with full metadata, education, and SEO content\n- TypeScript generator with annotated step generation\n- Python generator with cross-language parity\n- Test fixtures and unit tests (Vitest)\n- Pre-computed default steps\"\n\n# Push and create a PR\ngit push -u origin feature/add-linear-search\n```\n\n### PR Checklist\n\nBefore requesting review, verify all of the following:\n\n- [ ] `meta.json` validates against `shared/meta.schema.json`\n- [ ] TypeScript generator passes all Vitest tests\n- [ ] Python generator passes all pytest tests\n- [ ] Cross-language parity script passes\n- [ ] Linting passes (`npm run lint` and `ruff check .`)\n- [ ] Type checking passes (`npm run typecheck` and `mypy src/`)\n- [ ] Algorithm renders correctly in the browser\n- [ ] All example presets work\n- [ ] Code highlights match pseudocode lines\n- [ ] Step explanations are accurate and educational\n\n---\n\n## Summary\n\nThe complete file tree for a new algorithm looks like this:\n\n```\nalgorithms/classical/linear-search/\n  meta.json\n  generator.ts\n  precomputed/\n    default.json          # Auto-generated by build script\n  tests/\n    generator.test.ts\n    found.fixture.json\n    not-found.fixture.json\n    single-element.fixture.json\n\npython/src/eigenvue/generators/classical/\n  linear_search.py        # Python generator (parity with TypeScript)\n\npython/src/eigenvue/generators/__init__.py   # Updated registry\n```\n\nThat is 4 new files you write by hand, 1 file you edit (the registry), and 1\nfile that is auto-generated (pre-computed steps). The entire process, once you\nare familiar with it, takes about 2--4 hours for a simple algorithm and up to a\nday for something complex like multi-head attention.","src/content/docs/contributing/adding-an-algorithm.mdx","0b13447bf29a8642","contributing/code-style",{"id":364,"data":366,"body":372,"filePath":373,"digest":374,"deferredRender":16},{"title":367,"description":368,"editUrl":16,"head":369,"template":50,"sidebar":370,"pagefind":16,"draft":39},"Code Style Guide","Coding standards for the Eigenvue project, covering TypeScript/JavaScript conventions, Python conventions, file naming, import ordering, comments philosophy, and documentation style.",[],{"hidden":39,"attrs":371},{},"Consistent code style makes the codebase easier to navigate, review, and\nmaintain. This guide covers the standards enforced by our tooling and the\nconventions followed by the team.\n\n---\n\n## TypeScript / JavaScript\n\n### Tooling\n\n| Tool      | Config File          | Purpose               |\n|-----------|----------------------|-----------------------|\n| ESLint    | `web/.eslintrc.cjs`  | Linting and rules     |\n| Prettier  | `web/.prettierrc`    | Formatting            |\n| TypeScript| `web/tsconfig.json`  | Type checking         |\n\n```bash\n# Run linting\ncd web\nnpm run lint\n\n# Run formatting\nnpm run format\n\n# Run type checking\nnpm run typecheck\n```\n\n### Strict Mode\n\nTypeScript is configured with `strict: true`. This enables all strict checks:\n\n- `strictNullChecks` -- no implicit `null` or `undefined`\n- `noImplicitAny` -- every variable must have a type\n- `strictFunctionTypes` -- function parameter types are contravariant\n- `strictPropertyInitialization` -- class properties must be initialized\n\nDo not add `@ts-ignore` or `@ts-expect-error` unless absolutely necessary,\nand always include a comment explaining why.\n\n### Naming Conventions\n\n| Kind               | Convention   | Example                          |\n|--------------------|-------------|----------------------------------|\n| Variables          | camelCase    | `stepCount`, `isTerminal`        |\n| Functions          | camelCase    | `runGenerator`, `createStep`     |\n| Interfaces         | PascalCase   | `StepInput`, `VisualAction`      |\n| Type aliases       | PascalCase   | `GeneratorFunction`, `ArrowHead` |\n| Constants          | UPPER_SNAKE  | `STEP_FORMAT_VERSION`, `Z_INDEX` |\n| Enum members       | UPPER_SNAKE  | `ElementShape.ROUNDED_RECT`      |\n| File names         | kebab-case   | `generator-runner.ts`            |\n| React components   | PascalCase   | `VisualizationPanel.tsx`         |\n\n### Type Annotations\n\nAlways annotate function parameters and return types. Let TypeScript infer\nlocal variables when the type is obvious.\n\n```typescript\n// GOOD: Annotated parameters and return type\nfunction createStep(input: StepInput): Step {\n  const index = steps.length;  // Inferred as number  no annotation needed\n  // ...\n}\n\n// BAD: Missing parameter types\nfunction createStep(input) {\n  // ...\n}\n\n// BAD: Unnecessary annotation on obvious local\nconst index: number = steps.length;\n```\n\n### Readonly by Default\n\nUse `readonly` for interface properties unless mutation is required. This\ncommunicates intent and catches accidental mutations at compile time.\n\n```typescript\n// GOOD: Immutable by default\ninterface StepInput {\n  readonly id: string;\n  readonly title: string;\n  readonly state: Record\u003Cstring, unknown>;\n}\n\n// BAD: Mutable when it should not be\ninterface StepInput {\n  id: string;\n  title: string;\n}\n```\n\n### Avoid `any`\n\nUse `unknown` instead of `any` when the type is genuinely unknown. Then\nnarrow with type guards.\n\n```typescript\n// GOOD: unknown + type guard\nfunction processValue(value: unknown): string {\n  if (typeof value === \"string\") {\n    return value;\n  }\n  return String(value);\n}\n\n// BAD: any bypasses all type safety\nfunction processValue(value: any): string {\n  return value;\n}\n```\n\n### String Formatting\n\nUse template literals for string interpolation. Prefer them over\nconcatenation for readability.\n\n```typescript\n// GOOD\nconst message = `Found ${target} at index ${index}!`;\n\n// ACCEPTABLE for complex multi-line strings\nconst explanation =\n  `array[${mid}] = ${array[mid]} \u003C target ${target}. ` +\n  `Target must be in the right half.`;\n\n// BAD: String concatenation\nconst message = \"Found \" + target + \" at index \" + index + \"!\";\n```\n\n---\n\n## Python\n\n### Tooling\n\n| Tool   | Config File           | Purpose                   |\n|--------|-----------------------|---------------------------|\n| Ruff   | `python/pyproject.toml`| Linting and formatting   |\n| mypy   | `python/pyproject.toml`| Static type checking     |\n\n```bash\ncd python\n\n# Lint\nruff check .\n\n# Auto-fix lint issues\nruff check --fix .\n\n# Format\nruff format .\n\n# Type check\nmypy src/\n```\n\n### mypy Strict Mode\n\nLike TypeScript, Python uses strict type checking. The mypy configuration\nincludes:\n\n- `strict = true`\n- `disallow_untyped_defs = true`\n- `disallow_any_generics = true`\n\nAll function signatures must have type annotations.\n\n### Naming Conventions\n\n| Kind               | Convention     | Example                        |\n|--------------------|---------------|--------------------------------|\n| Variables          | snake_case     | `step_count`, `is_terminal`    |\n| Functions          | snake_case     | `run_generator`, `create_step` |\n| Classes            | PascalCase     | `StepSequence`, `VisualAction` |\n| Constants          | UPPER_SNAKE    | `STEP_FORMAT_VERSION`          |\n| Module files       | snake_case     | `binary_search.py`             |\n| Package dirs       | snake_case     | `eigenvue/generators/`         |\n| Private members    | `_prefix`      | `_validate_steps`              |\n\n### Type Annotations\n\nAnnotate all function parameters and return types. Use `from __future__ import\nannotations` at the top of every file for forward reference support.\n\n```python\nfrom __future__ import annotations\n\nfrom typing import Any\n\n\ndef generate(inputs: dict[str, Any]) -> list[Step]:\n    \"\"\"Generate algorithm steps.\"\"\"\n    array: list[int] = list(inputs[\"array\"])\n    target: int = inputs[\"target\"]\n    # ...\n```\n\n### Docstrings\n\nUse NumPy-style docstrings for all public functions and classes:\n\n```python\ndef run_generator(\n    algorithm_id: str,\n    inputs: dict[str, Any] | None = None,\n) -> list[dict[str, Any]]:\n    \"\"\"Run an algorithm generator and return validated step dicts.\n\n    Parameters\n    ----------\n    algorithm_id : str\n        The algorithm identifier.\n    inputs : dict or None\n        Custom input parameters. If None, uses algorithm defaults.\n\n    Returns\n    -------\n    list[dict[str, Any]]\n        Validated step dicts in camelCase wire format.\n\n    Raises\n    ------\n    ValueError\n        If no generator is registered for this algorithm.\n    \"\"\"\n```\n\n---\n\n## File Naming Conventions\n\n### TypeScript Files\n\n| Kind                    | Pattern                  | Example                        |\n|-------------------------|--------------------------|--------------------------------|\n| Generator               | `generator.ts`           | `binary-search/generator.ts`   |\n| Tests                   | `*.test.ts`              | `generator.test.ts`            |\n| Fixtures                | `*.fixture.json`         | `found.fixture.json`           |\n| Metadata                | `meta.json`              | `binary-search/meta.json`      |\n| Layout                  | `kebab-case.ts`          | `array-with-pointers.ts`       |\n| React component         | `PascalCase.tsx`         | `VisualizationPanel.tsx`       |\n| Engine module           | `PascalCase.ts`          | `GeneratorRunner.ts`           |\n| Shared types            | `kebab-case.ts`          | `step.ts`                      |\n\n### Python Files\n\n| Kind                    | Pattern                  | Example                        |\n|-------------------------|--------------------------|--------------------------------|\n| Generator               | `snake_case.py`          | `binary_search.py`             |\n| Tests                   | `test_*.py`              | `test_binary_search.py`        |\n| Package init            | `__init__.py`            | `generators/__init__.py`       |\n| Utility module          | `snake_case.py`          | `math_utils.py`                |\n\n### Algorithm Directories\n\nAlgorithm directory names use **kebab-case** and must match the algorithm ID:\n\n```\nalgorithms/classical/binary-search/    # ID: \"binary-search\"\nalgorithms/deep-learning/perceptron/   # ID: \"perceptron\"\nalgorithms/generative-ai/self-attention/  # ID: \"self-attention\"\n```\n\n---\n\n## Import Ordering\n\n### TypeScript\n\nImports should be ordered in three groups, separated by blank lines:\n\n1. External dependencies (npm packages)\n2. Internal aliases (`@/...`)\n3. Relative imports (`./...`, `../...`)\n\nWithin each group, sort alphabetically.\n\n```typescript\n// 1. External\nimport { describe, it, expect } from \"vitest\";\n\n// 2. Internal aliases\nimport { runGenerator } from \"@/engine/generator\";\nimport type { Step, VisualAction } from \"@/shared/types/step\";\n\n// 3. Relative\nimport binarySearchGenerator from \"../generator\";\nimport foundFixture from \"./found.fixture.json\";\n```\n\nSeparate `import type` from value imports when both exist:\n\n```typescript\nimport { createGenerator } from \"@/engine/generator\";\nimport type { VisualAction } from \"@/engine/generator\";\n```\n\n### Python\n\nFollow the standard Python import ordering (enforced by Ruff's `isort` rules):\n\n1. Standard library\n2. Third-party packages\n3. Local imports\n\n```python\n# 1. Standard library\nfrom __future__ import annotations\n\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import Any\n\n# 2. Third-party\nimport pytest\n\n# 3. Local\nfrom eigenvue._step_types import Step, VisualAction, CodeHighlight\nfrom eigenvue.runner import run_generator\n```\n\n---\n\n## Comments Philosophy\n\n### When to Comment\n\nComment the **why**, not the **what**. If the code is clear, it does not need\na comment explaining what it does. It does need a comment explaining why it\ndoes it that way.\n\n```typescript\n// GOOD: Explains WHY\n// Kahan summation maintains a running compensation term that tracks\n// the low-order bits lost during each addition, achieving O(1) error\n// independent of the number of terms.\nlet sum = 0;\nlet compensation = 0;\nfor (const w of weights) {\n  const y = w - compensation;\n  const t = sum + y;\n  compensation = (t - sum) - y;\n  sum = t;\n}\n\n// BAD: Explains WHAT (obvious from the code)\n// Add w to sum\nsum += w;\n```\n\n### When Comments Are Required\n\n1. **Mathematical derivations.** When the code implements a formula, cite the\n   formula and explain the mapping from math to code.\n\n2. **Non-obvious invariants.** When a loop or algorithm maintains an invariant\n   that is not immediately apparent.\n\n3. **Performance decisions.** When you chose a specific approach for performance\n   reasons (e.g., Kahan summation instead of naive reduce).\n\n4. **Cross-language parity notes.** When TypeScript and Python implementations\n   differ in their approach to achieve identical output.\n\n5. **Safety constraints.** When a copy is necessary to avoid shared mutable\n   state (the `[...array]` pattern in state snapshots).\n\n### File Headers\n\nEvery source file should have a `@fileoverview` JSDoc comment (TypeScript) or\na module docstring (Python) explaining its purpose:\n\n```typescript\n/**\n * @fileoverview Binary Search  Step Generator\n *\n * Generates a step-by-step visualization of iterative binary search on a\n * sorted array of numbers.\n *\n * ALGORITHM:\n *   Given a sorted array and a target value, binary search maintains two\n *   pointers (left, right) defining the search space...\n */\n```\n\n```python\n\"\"\"\nBinary Search  Step Generator (Python)\n\nProduces step-by-step visualization data identical to the TypeScript\ngenerator when given the same inputs.\n\"\"\"\n```\n\n---\n\n## Documentation Style\n\n### JSDoc (TypeScript)\n\nUse JSDoc for all exported functions, interfaces, and type aliases:\n\n```typescript\n/**\n * Runs a generator with the given inputs and returns a validated StepSequence.\n *\n * @typeParam TInputs - The algorithm's input parameter shape.\n * @param definition - The generator definition (from `createGenerator()`).\n * @param inputs     - The algorithm's input parameters.\n * @param options    - Optional configuration (max steps, etc.).\n * @returns A complete, validated StepSequence.\n * @throws {GeneratorError} If the generator fails or produces invalid output.\n *\n * @example\n * ```typescript\n * const result = runGenerator(binarySearchGenerator, {\n *   array: [1, 3, 5, 7, 9],\n *   target: 5,\n * });\n * ```\n */\n```\n\n### NumPy-Style Docstrings (Python)\n\nUse the NumPy docstring format with sections for Parameters, Returns, Raises,\nand Examples:\n\n```python\ndef verify_fixture(fixture_path: Path) -> bool:\n    \"\"\"Verify that a fixture round-trips through Python dataclasses.\n\n    Args:\n        fixture_path: Path to a .fixture.json file.\n\n    Returns:\n        True if the round-trip produces identical JSON, False otherwise.\n\n    Examples\n    --------\n    >>> verify_fixture(Path(\"fixtures/binary-search-found.fixture.json\"))\n    True\n    \"\"\"\n```\n\n### Key Principles\n\n1. **Document contracts, not implementations.** Describe what a function\n   promises to do, not how it does it internally.\n\n2. **Include examples.** A short usage example is worth more than a paragraph\n   of description.\n\n3. **Document mathematical invariants.** When a function enforces or relies on\n   mathematical properties, state them explicitly with the notation.\n\n4. **Keep docs close to code.** Inline documentation (JSDoc, docstrings)\n   stays up to date better than separate documentation files.","src/content/docs/contributing/code-style.mdx","1415ee7e4a4b33c2","contributing/cross-language-parity",{"id":375,"data":377,"body":383,"filePath":384,"digest":385,"deferredRender":16},{"title":378,"description":379,"editUrl":16,"head":380,"template":50,"sidebar":381,"pagefind":16,"draft":39},"Cross-Language Parity","Ensuring TypeScript and Python generators produce identical step output, including validation scripts, common pitfalls like PRNG mismatches and float ordering, and debugging strategies.",[],{"hidden":39,"attrs":382},{},"Eigenvue's TypeScript and Python generators must produce **identical** step\noutput for the same inputs. This is not a \"nice to have\" -- it is a hard\nrequirement. The JOSS (Journal of Open Source Software) review process demands\nreproducible results, and users of the Python package must see the exact same\nvisualization as users of the web application.\n\n---\n\n## Why Parity Matters\n\n1. **JOSS Requirement.** The paper submission requires that the software produce\n   reproducible, verifiable results. A reviewer running the Python package must\n   get the same step sequence as someone using the web app.\n\n2. **User Trust.** If a student sees different behavior between the web app and\n   their Jupyter notebook, they lose trust in the platform.\n\n3. **Pre-computed Steps.** Build-time pre-computation uses the Python generator.\n   Runtime uses the TypeScript generator. If they disagree, the initial page\n   load shows different steps than stepping through manually.\n\n4. **Test Fixtures.** Golden test fixtures are shared between both languages. If\n   either language drifts, the fixture tests catch it.\n\n---\n\n## What \"Identical\" Means\n\nTwo step sequences are identical if, when serialized to JSON with\ndeterministic settings (sorted keys, consistent indentation), they produce\n**byte-identical strings**.\n\nFor floating-point values, \"identical\" means the JSON string representation\nis the same. In practice, this requires that both languages:\n\n- Perform floating-point operations in the **same order**.\n- Use the **same rounding** strategy.\n- Produce the **same string** when serializing a float (e.g., both produce\n  `\"0.7071067811865476\"`, not one producing `\"0.7071067811865475\"`).\n\nFor values where exact agreement is impractical (e.g., long chains of\nfloating-point operations), the tolerance is **+/-1e-9**. However, the\nparity verification script uses byte-identical comparison, so in practice\nyou must achieve exact agreement or round both sides to the same precision.\n\n---\n\n## Parity Validation Script\n\n### Location\n\n```\nscripts/verify-step-parity.py\n```\n\n### What It Does\n\n1. Loads golden fixture files from `shared/fixtures/`.\n2. Deserializes each fixture into Python `Step` dataclasses via `from_dict()`.\n3. Re-serializes back to camelCase dicts via `to_dict()`.\n4. Compares the re-serialized JSON to the original fixture JSON.\n\nIf the round-trip produces identical JSON, the Python types are proven\nwire-compatible with the TypeScript types.\n\n### Running It\n\n```bash\npython scripts/verify-step-parity.py\n```\n\n### Output\n\n```\n============================================================\nCross-Language Step Parity Verification\n============================================================\n\nVerifying: binary-search-found.fixture.json\n  PASS\nVerifying: binary-search-not-found.fixture.json\n  PASS\nVerifying: single-element.fixture.json\n  PASS\nVerifying: minimal-valid.fixture.json\n  PASS\n\n============================================================\nALL FIXTURES PASSED\n============================================================\n```\n\nExit code 0 means all fixtures passed. Exit code 1 means at least one failed.\n\n### Adding Your Algorithm's Fixtures\n\nWhen you add a new algorithm, add its fixtures to the parity test by placing\nthem in `shared/fixtures/` with the naming convention\n`\u003Calgorithm-id>-\u003Ccase>.fixture.json` and adding them to the `valid_fixtures`\nlist in the script.\n\n---\n\n## Common Pitfalls\n\nThese are the most common causes of parity failures, ordered by frequency.\nEach includes the symptom, root cause, and fix.\n\n### 1. PRNG Mismatch (LCG Parameters)\n\n**Symptom:** Steps involving random initialization (neural network weights,\nshuffle operations) produce different values.\n\n**Root cause:** TypeScript and Python use different default random number\ngenerators. Even if you seed both with the same value, the sequences differ.\n\n**Fix:** Use the same **Linear Congruential Generator (LCG)** implementation\nin both languages. The Eigenvue LCG uses these parameters:\n\n```typescript\n// TypeScript\nfunction lcg(seed: number): () => number {\n  let state = seed;\n  return () => {\n    // Multiplier and increment from Numerical Recipes\n    state = (state * 1664525 + 1013904223) >>> 0;  // Force unsigned 32-bit\n    return state / 0x100000000;  // Normalize to [0, 1)\n  };\n}\n```\n\n```python\n# Python\ndef lcg(seed: int) -> Callable[[], float]:\n    state = seed\n\n    def next_value() -> float:\n        nonlocal state\n        state = (state * 1664525 + 1013904223) & 0xFFFFFFFF  # Force unsigned 32-bit\n        return state / 0x100000000  # Normalize to [0, 1)\n\n    return next_value\n```\n\n:::caution\nThe critical detail is unsigned 32-bit truncation:\n- TypeScript: `>>> 0` (unsigned right shift by 0 forces 32-bit unsigned)\n- Python: `& 0xFFFFFFFF` (bitwise AND with 32-bit mask)\n\nThese are **not** interchangeable with `>> 0` in TypeScript (which is signed)\nor `% 2**32` in Python (which handles negatives differently).\n:::\n\n### 2. Integer Overflow and Unsigned 32-bit Arithmetic\n\n**Symptom:** Hash values, LCG states, or bitwise operations produce different\nresults for large numbers.\n\n**Root cause:** JavaScript numbers are 64-bit floats. Python integers are\narbitrary precision. When JavaScript does `>>> 0`, it truncates to unsigned\n32-bit. Python has no equivalent automatic truncation.\n\n**Fix:** In Python, explicitly mask every intermediate result that should be\n32-bit unsigned:\n\n```python\n# WRONG: Python integers grow without bound\nresult = a * b + c  # Could be 50+ bits\n\n# CORRECT: Mask to 32-bit unsigned after every operation\nresult = ((a * b) + c) & 0xFFFFFFFF\n```\n\nIn TypeScript, use `>>> 0` to force unsigned 32-bit:\n\n```typescript\n// WRONG: May produce negative number (signed 32-bit)\nconst result = (a * b + c) | 0;\n\n// CORRECT: Unsigned 32-bit\nconst result = (a * b + c) >>> 0;\n```\n\n### 3. Float Order of Operations\n\n**Symptom:** Floating-point values differ by tiny amounts (e.g.,\n`0.7071067811865476` vs `0.7071067811865475`).\n\n**Root cause:** Floating-point addition is not associative.\n`(a + b) + c !== a + (b + c)` in general. If the TypeScript generator\ncomputes a sum left-to-right and the Python generator uses `sum()` (which\nmay use a different accumulation order), the results can differ.\n\n**Fix:** Ensure both generators compute floating-point operations in the\n**exact same order**:\n\n```typescript\n// TypeScript: explicit left-to-right accumulation\nlet total = 0;\nfor (let i = 0; i \u003C values.length; i++) {\n  total += values[i];\n}\n```\n\n```python\n# Python: explicit left-to-right accumulation (NOT sum())\ntotal = 0.0\nfor v in values:\n    total += v\n```\n\nDo **not** use Python's `sum()`, `math.fsum()`, or `numpy.sum()` unless the\nTypeScript side uses the exact same algorithm. Python's `sum()` is a simple\nleft-to-right accumulation (same as the explicit loop), but `math.fsum()` uses\nShewchuk's algorithm for perfect precision, which will produce different\nresults.\n\n### 4. Sorting Stability\n\n**Symptom:** When sorting elements with equal keys, the order of tied elements\ndiffers between languages.\n\n**Root cause:** Both JavaScript's `Array.prototype.sort()` and Python's\n`sorted()` / `list.sort()` are stable sorts (they preserve relative order of\nequal elements). However, if you use a comparison function that does not\nfully specify the order, different implementations may break ties differently.\n\n**Fix:** Always use a **total ordering** in sort comparisons. If two elements\nhave the same primary key, sort by a secondary key (e.g., original index):\n\n```typescript\n// WRONG: Ties are broken arbitrarily\nnodes.sort((a, b) => a.distance - b.distance);\n\n// CORRECT: Break ties by node ID for deterministic order\nnodes.sort((a, b) => a.distance - b.distance || a.id.localeCompare(b.id));\n```\n\n```python\n# CORRECT: Break ties by node ID\nnodes.sort(key=lambda n: (n.distance, n.id))\n```\n\n### 5. Object Key Ordering in JSON\n\n**Symptom:** JSON serialization produces different output because keys appear\nin a different order.\n\n**Root cause:** JavaScript objects iterate keys in insertion order (for string\nkeys). Python dicts (3.7+) also maintain insertion order. But if the two\ngenerators build state objects in a different order, the JSON output differs.\n\n**Fix:** Build state objects with keys in the **same order** in both\ngenerators. The parity script uses `sort_keys=True` for comparison, which\nnormalizes key order. But for step-by-step comparison, matching insertion\norder makes debugging much easier.\n\n```typescript\n// TypeScript\nstate: {\n  array: [...array],\n  target,\n  left,\n  right,\n  mid,\n  result: null,\n}\n```\n\n```python\n# Python: same key order\nstate={\n    \"array\": list(array),\n    \"target\": target,\n    \"left\": left,\n    \"right\": right,\n    \"mid\": mid,\n    \"result\": None,\n}\n```\n\n### 6. String Representation of Numbers\n\n**Symptom:** JSON output differs in how numbers are formatted (e.g., `1.0` vs\n`1`, or `1e-10` vs `0.0000000001`).\n\n**Root cause:** JavaScript's `JSON.stringify` and Python's `json.dumps` have\ndifferent default number formatting. JavaScript omits trailing zeros\n(`1` not `1.0`), while Python may include them depending on the value.\n\n**Fix:** Both languages represent integers the same way (`1`, `42`, `-5`).\nFor floating-point values, ensure you use the same type in both languages.\nIf a value is always an integer (like an array index), use integer types in\nboth languages.\n\n```typescript\n// TypeScript: result is a number (integer or -1)\nstate: { result: foundIndex }  // Will serialize as 6 or -1\n```\n\n```python\n# Python: result is an int\nstate={\"result\": found_index}  # Will serialize as 6 or -1\n```\n\nIf you must store a float, ensure both languages produce the same string.\nJavaScript's `JSON.stringify(0.1)` produces `\"0.1\"`, and Python's\n`json.dumps(0.1)` also produces `\"0.1\"`. But for computed values, the exact\nfloat bits may differ, producing different strings.\n\n### 7. None/null vs Missing Keys\n\n**Symptom:** One language includes a key with value `null`/`None`, the other\nomits the key entirely.\n\n**Root cause:** TypeScript's `undefined` is omitted by `JSON.stringify`, but\n`null` is included. Python's `None` maps to `null` in JSON.\n\n**Fix:** Use `null` (TypeScript) / `None` (Python) for \"no value\" instead of\n`undefined`. Always include the key with a `null` value rather than omitting it.\n\n```typescript\n// CORRECT: explicit null\nstate: { result: null }\n\n// WRONG: undefined is omitted from JSON\nstate: { result: undefined }\n```\n\n---\n\n## Debugging Parity Failures\n\nWhen the parity test fails, follow this systematic approach:\n\n### Step 1: Generate Both Outputs\n\n```bash\n# TypeScript output\ncd web\nnpx tsx -e \"\n  import gen from '../algorithms/classical/my-algo/generator';\n  import { runGenerator } from './src/engine/generator';\n  const result = runGenerator(gen, { array: [1,2,3], target: 2 });\n  console.log(JSON.stringify(result.steps, null, 2));\n\" > /tmp/ts-steps.json\n\n# Python output\ncd python\npython -c \"\nfrom eigenvue.runner import run_generator\nimport json\nsteps = run_generator('my-algo', {'array': [1,2,3], 'target': 2})\nprint(json.dumps(steps, indent=2, sort_keys=True))\n\" > /tmp/py-steps.json\n```\n\n### Step 2: Diff\n\n```bash\ndiff /tmp/ts-steps.json /tmp/py-steps.json\n```\n\n### Step 3: Identify the First Divergent Step\n\nLook at the diff output. The first difference tells you which step diverged.\nNote the step index and which field differs.\n\n### Step 4: Trace the Computation\n\nAdd logging to both generators at the divergent step. Print the intermediate\nvalues that feed into the differing field.\n\n### Step 5: Check the Common Pitfalls\n\nMost parity failures fall into one of the categories above. Check:\n\n1. Is there unsigned 32-bit arithmetic involved? Check `>>> 0` vs `& 0xFFFFFFFF`.\n2. Is there floating-point accumulation? Check operation order.\n3. Is there sorting? Check tie-breaking.\n4. Is there randomness? Check LCG parameters.\n5. Is there `null` vs `undefined`? Check key presence.\n\n### Step 6: Fix and Verify\n\nAfter fixing, run the parity test again:\n\n```bash\npython scripts/verify-step-parity.py\n```\n\nAnd run both test suites:\n\n```bash\ncd web && npm run test\ncd python && pytest\n```\n\n---\n\n## Parity Test Integration\n\nThe parity test runs as part of CI on every pull request. It is a blocking\ncheck -- your PR cannot merge if parity fails.\n\nTo run it locally before pushing:\n\n```bash\npython scripts/verify-step-parity.py\n```\n\nThis is fast (under 2 seconds) and catches most issues before CI.\n\n---\n\n## Summary\n\n| Rule                                     | TypeScript               | Python                     |\n|------------------------------------------|--------------------------|----------------------------|\n| Unsigned 32-bit truncation               | `>>> 0`                  | `& 0xFFFFFFFF`             |\n| Float accumulation                       | Explicit `for` loop      | Explicit `for` loop        |\n| Random numbers                           | Custom LCG with seed     | Same LCG with same seed    |\n| Sort tie-breaking                        | Total ordering in comparator | Total ordering in key  |\n| Missing values                           | `null` (not `undefined`) | `None`                     |\n| State key order                          | Match Python's order     | Match TypeScript's order   |\n| Integer vs float                         | Use integer types        | Use `int` types            |\n| String formatting                        | Template literals        | f-strings with same format |\n| Tolerance for computed floats            | +/-1e-9                  | +/-1e-9                    |","src/content/docs/contributing/cross-language-parity.mdx","6246480338fd8b89","contributing/development-setup",{"id":386,"data":388,"body":394,"filePath":395,"digest":396,"deferredRender":16},{"title":389,"description":390,"editUrl":16,"head":391,"template":50,"sidebar":392,"pagefind":16,"draft":39},"Development Setup","Set up your local development environment for contributing to Eigenvue, including prerequisites, workspace installation, running services, and CI requirements.",[],{"hidden":39,"attrs":393},{},"This guide walks you through everything you need to start contributing to Eigenvue.\nBy the end, you will have the web app, documentation site, and Python package\nrunning locally with all tests passing.\n\n## Prerequisites\n\nBefore you begin, make sure the following tools are installed and available on\nyour `PATH`:\n\n| Tool       | Minimum Version | Check Command        |\n|------------|-----------------|----------------------|\n| Node.js    | 20.0            | `node --version`     |\n| npm        | 10.0            | `npm --version`      |\n| Python     | 3.10            | `python --version`   |\n| pip        | 22.0            | `pip --version`      |\n| Git        | 2.30            | `git --version`      |\n\n:::tip\nWe recommend using [nvm](https://github.com/nvm-sh/nvm) (or\n[nvm-windows](https://github.com/coreybutler/nvm-windows)) to manage Node.js\nversions and [pyenv](https://github.com/pyenv/pyenv) to manage Python versions.\n:::\n\n## Clone and Install\n\nEigenvue is a monorepo with three installable workspaces: `web/`, `python/`, and\n`docs/`. Clone the repository and install each workspace:\n\n```bash\n# Clone the repository\ngit clone https://github.com/ashutoshm1771/eigenvue.git\ncd eigenvue\n```\n\n### Web Application (Next.js)\n\n```bash\ncd web\nnpm install\n```\n\n### Documentation Site (Starlight)\n\n```bash\ncd docs\nnpm install\n```\n\n### Python Package\n\n```bash\ncd python\npip install -e \".[dev]\"\n```\n\nThe `-e` flag installs the package in **editable mode** so your local changes\nare reflected immediately. The `[dev]` extra pulls in Ruff, mypy, pytest, and\nother development dependencies.\n\n## Running Locally\n\n### Web Dev Server\n\n```bash\ncd web\nnpm run dev\n```\n\nThis starts the Next.js development server at **http://localhost:3000** with hot\nmodule reloading. Changes to files in `web/`, `shared/`, and `algorithms/` are\npicked up automatically.\n\n### Docs Dev Server\n\n```bash\ncd docs\nnpm run dev\n```\n\nThe Astro Starlight docs site starts at **http://localhost:4321**. MDX changes\nappear instantly in the browser.\n\n### Running Tests\n\n```bash\n# TypeScript tests (from web/)\ncd web\nnpm run test           # Run all Vitest tests\nnpm run test -- --watch  # Watch mode\n\n# Python tests (from python/)\ncd python\npytest                 # Run all pytest tests\npytest -x              # Stop on first failure\n```\n\n## Project Structure Overview\n\nUnderstanding the monorepo layout helps you navigate the codebase:\n\n```\neigenvue/\n  algorithms/           # Algorithm definitions (meta.json + generators)\n    classical/          #   Binary search, sorting, graph traversal, etc.\n    deep-learning/      #   Perceptron, backpropagation, convolution, etc.\n    generative-ai/      #   Tokenization, attention, transformer block, etc.\n  web/                  # Next.js web application\n    src/\n      engine/           #   Rendering engine (layouts, animation, canvas)\n        generator/      #     Generator runner and types\n        layouts/        #     Layout functions and registry\n        animation/      #     Animation and interpolation\n        canvas/         #     Canvas manager and DPI handling\n      components/       #   React components\n  python/               # PyPI package (eigenvue)\n    src/eigenvue/\n      generators/       #   Python generator implementations\n      runner.py         #   Python generator runner\n  docs/                 # Astro Starlight documentation site\n    src/content/docs/   #   MDX documentation pages\n  shared/               # Cross-language types and schemas\n    types/              #   Step, VisualAction, CodeHighlight definitions\n    fixtures/           #   Golden test fixtures\n    validation/         #   JSON Schema validators\n  scripts/              # Build and validation scripts\n    verify-step-parity.py  # Cross-language parity checker\n  tests/                # Integration and end-to-end tests\n```\n\n### Key Relationships\n\n- **`algorithms/`** contains `meta.json` (metadata) and `generator.ts`\n  (TypeScript generator) for each algorithm. Python generators live under\n  `python/src/eigenvue/generators/`.\n\n- **`shared/types/step.ts`** is the single source of truth for the `Step`\n  interface. Both TypeScript and Python must produce identical JSON output\n  for the same inputs.\n\n- **`web/src/engine/layouts/`** contains layout functions that convert `Step`\n  objects into `PrimitiveScene` objects for rendering. Each layout self-registers\n  via `registerLayout()` in `registry.ts`.\n\n## Branch Strategy\n\n| Branch         | Purpose                                             |\n|----------------|-----------------------------------------------------|\n| `main`         | Stable, releasable state. Protected by CI.          |\n| `feature/*`    | New features (e.g., `feature/add-linear-search`).   |\n| `fix/*`        | Bug fixes (e.g., `fix/canvas-dpi-scaling`).         |\n| `docs/*`       | Documentation changes (e.g., `docs/update-api-ref`).|\n\nAll work happens on feature branches created from `main`. When your branch is\nready, open a pull request targeting `main`. At least one approval and all CI\nchecks must pass before merging.\n\n```bash\n# Create a feature branch\ngit checkout main\ngit pull origin main\ngit checkout -b feature/add-linear-search\n```\n\n## Commit Message Format\n\nWe follow the [Conventional Commits](https://www.conventionalcommits.org/)\nspecification. Every commit message must follow this structure:\n\n```\n\u003Ctype>(\u003Cscope>): \u003Cdescription>\n\n[optional body]\n\n[optional footer(s)]\n```\n\n### Types\n\n| Type       | When to Use                                        |\n|------------|----------------------------------------------------|\n| `feat`     | A new feature (algorithm, layout, UI component)    |\n| `fix`      | A bug fix                                          |\n| `docs`     | Documentation-only changes                         |\n| `style`    | Formatting, whitespace, no logic change            |\n| `refactor` | Code restructuring without behavior change         |\n| `test`     | Adding or updating tests                           |\n| `chore`    | Build system, CI, dependencies, tooling            |\n\n### Scopes\n\nUse the workspace name as the scope: `web`, `python`, `docs`, `shared`,\n`algorithms`, or `scripts`.\n\n### Examples\n\n```\nfeat(algorithms): add linear search generator and meta.json\nfix(python): correct integer overflow in LCG PRNG implementation\ntest(web): add layout snapshot tests for array-with-pointers\ndocs: add contributing guide for cross-language parity\nchore(web): upgrade Next.js to 15.1\n```\n\n## CI Checks That Must Pass\n\nEvery pull request runs the following CI jobs. All must pass before merge:\n\n### Web CI\n\n| Check       | Command                 | What It Verifies                  |\n|-------------|-------------------------|-----------------------------------|\n| Lint        | `npm run lint`          | ESLint rules, no warnings         |\n| Typecheck   | `npm run typecheck`     | TypeScript strict mode, no errors |\n| Test        | `npm run test`          | All Vitest tests pass             |\n| Build       | `npm run build`         | Next.js production build succeeds |\n\n### Python CI\n\n| Check       | Command                    | What It Verifies                  |\n|-------------|----------------------------|-----------------------------------|\n| Lint        | `ruff check .`             | Ruff linting, no violations       |\n| Typecheck   | `mypy src/`                | mypy strict mode, no errors       |\n| Test        | `pytest`                   | All pytest tests pass             |\n\n### Cross-Language Parity\n\n```bash\npython scripts/verify-step-parity.py\n```\n\nThis script verifies that Python dataclasses round-trip through JSON identically\nto the TypeScript interfaces. It is the final gate ensuring both languages\nproduce byte-identical step output.\n\n:::caution\nRun all CI checks locally before pushing. A failing CI check blocks the entire\nPR from merging, even if your specific change is unrelated to the failure.\n\n```bash\n# Quick local CI check\ncd web && npm run lint && npm run typecheck && npm run test && cd ..\ncd python && ruff check . && mypy src/ && pytest && cd ..\npython scripts/verify-step-parity.py\n```\n:::\n\n## Next Steps\n\nOnce your development environment is ready:\n\n- **Adding an algorithm?** Read [Adding an Algorithm](/docs/contributing/adding-an-algorithm/).\n- **Working on layouts?** Read [Adding a Layout](/docs/contributing/adding-a-layout/).\n- **Writing tests?** Read [Writing Tests](/docs/contributing/writing-tests/).\n- **Need API details?** Read [The Generator API](/docs/contributing/generator-api/).","src/content/docs/contributing/development-setup.mdx","e96ec766cc191c59","contributing/generator-api",{"id":397,"data":399,"body":405,"filePath":406,"digest":407,"deferredRender":16},{"title":400,"description":401,"editUrl":16,"head":402,"template":50,"sidebar":403,"pagefind":16,"draft":39},"The Generator API","Complete reference for the Eigenvue generator API, covering TypeScript and Python generator patterns, step building, visual actions, code highlighting, and input contracts.",[],{"hidden":39,"attrs":404},{},"Generators are the core abstraction of Eigenvue. A generator takes an\nalgorithm's inputs and produces an ordered sequence of `Step` objects, each\nrepresenting one \"interesting moment\" in the algorithm's execution.\n\nThis page is the API reference. For a hands-on tutorial, see\n[Adding an Algorithm](/docs/contributing/adding-an-algorithm/).\n\n---\n\n## TypeScript Generator API\n\n### `createGenerator\u003CTInputs>(config)`\n\nFactory function that produces a type-safe `GeneratorDefinition`. This is the\nentry point for every TypeScript generator.\n\n**Location:** `web/src/engine/generator/types.ts`\n\n```typescript\nimport { createGenerator } from \"@/engine/generator\";\n\ninterface MyInputs extends Record\u003Cstring, unknown> {\n  array: readonly number[];\n  target: number;\n}\n\nexport default createGenerator\u003CMyInputs>({\n  id: \"my-algorithm\",  // Must match meta.json \"id\"\n  *generate(inputs, step) {\n    // Yield steps here...\n  },\n});\n```\n\n**Parameters:**\n\n| Field      | Type                                    | Description                                    |\n|------------|-----------------------------------------|------------------------------------------------|\n| `id`       | `string`                                | Algorithm ID. Must match `^[a-z0-9][a-z0-9-]*$`. |\n| `generate` | `GeneratorFunction\u003CTInputs>`            | A generator function (`function*`).            |\n\n**Returns:** `GeneratorDefinition\u003CTInputs>`\n\n**Validation:** The `id` is validated at definition time. An invalid ID throws\nimmediately, so you catch mistakes before any test runs.\n\n---\n\n### `GeneratorDefinition\u003CTInputs>`\n\nThe object returned by `createGenerator()`. It bundles the algorithm ID with the\ngenerator function.\n\n```typescript\ninterface GeneratorDefinition\u003CTInputs extends Record\u003Cstring, unknown>> {\n  readonly id: string;\n  readonly generate: GeneratorFunction\u003CTInputs>;\n}\n```\n\nYou never construct this directly. Always use `createGenerator()`.\n\n---\n\n### `GeneratorFunction\u003CTInputs>`\n\nThe signature of the generator function itself:\n\n```typescript\ntype GeneratorFunction\u003CTInputs extends Record\u003Cstring, unknown>> =\n  (inputs: TInputs, step: StepBuilderFn) => Generator\u003CStep, void, undefined>;\n```\n\n**Parameters passed to your generator:**\n\n| Parameter | Type            | Description                                           |\n|-----------|-----------------|-------------------------------------------------------|\n| `inputs`  | `TInputs`       | The algorithm's validated input parameters.           |\n| `step`    | `StepBuilderFn` | Function to create steps. Call this for each step.    |\n\n**Contract:**\n\n1. Must yield at least one step.\n2. The last yielded step must have `isTerminal: true`.\n3. No step other than the last may have `isTerminal: true`.\n4. Must terminate (no infinite loops). Default safety limit: 10,000 steps.\n5. State objects must be independent snapshots (no shared references).\n\n---\n\n### `StepBuilderFn` (the `step` parameter)\n\nThe `step` parameter passed to your generator is a function that converts a\n`StepInput` into a complete `Step` object:\n\n```typescript\ntype StepBuilderFn = (input: StepInput) => Step;\n```\n\nThe step builder:\n\n1. Assigns the correct `index` (auto-incrementing from 0).\n2. Defaults `isTerminal` to `false` if not provided.\n3. Validates the step ID format and title length.\n4. Validates code highlight line numbers (must be positive integers).\n5. Returns the complete `Step` object.\n\n**You never construct `Step` objects directly.** Always use the `step()` builder.\n\n---\n\n### `StepInput`\n\nThe fields you provide when yielding a step:\n\n```typescript\ninterface StepInput {\n  readonly id: string;            // Template identifier (e.g., \"compare_mid\")\n  readonly title: string;         // Short heading (max 200 chars)\n  readonly explanation: string;   // Plain-language narration\n  readonly state: Record\u003Cstring, unknown>;  // Variable snapshot\n  readonly visualActions: readonly VisualAction[];\n  readonly codeHighlight: CodeHighlight;\n  readonly isTerminal?: boolean;  // Only true on the last step\n  readonly phase?: string;        // Optional grouping label\n}\n```\n\n#### Field Details\n\n**`id`** -- Template identifier matching `^[a-z0-9][a-z0-9_-]*$`. The same\nID can appear multiple times in a sequence (e.g., `\"compare\"` appears once\nper loop iteration). This is intentional: the ID identifies the _type_ of step,\nnot the specific instance.\n\n**`title`** -- Short heading shown in the UI. Must be non-empty and at most\n200 characters. Make it descriptive: `\"Compare Middle Element (Iteration 3)\"`\nis better than `\"Step 7\"`.\n\n**`explanation`** -- Educational narration shown below the visualization.\nReference concrete values: `\"mid = floor((2 + 8) / 2) = 5. Checking array[5] = 11.\"`\nis far better than `\"Calculate the middle index.\"`.\n\n**`state`** -- A snapshot of ALL algorithm variables at this point. Must be a\nJSON-serializable object. **Critical rule:** this must be an independent copy.\n\n```typescript\n// CORRECT: spread copy of the array\nstate: { array: [...array], target, left, right, mid }\n\n// CORRECT: deep clone for nested mutable structures\nstate: structuredClone({ matrix, weights, biases })\n\n// WRONG: reference to a mutable variable\nstate: { array, target, left, right }\n//        ^^^^^ if mutated later, ALL steps see the mutation\n```\n\n**`visualActions`** -- Rendering instructions for the layout. See the\n[Visual Actions](#visual-actions) section below.\n\n**`codeHighlight`** -- Maps this step to source code lines. See\n[CodeHighlight](#codehighlight) below.\n\n**`isTerminal`** -- Set to `true` **only** on the final step. The runner\nvalidates that exactly one step is terminal and that it is the last one.\n\n**`phase`** -- Optional grouping label. Consecutive steps with the same phase\nare visually grouped in the UI. Common values: `\"initialization\"`, `\"search\"`,\n`\"comparison\"`, `\"result\"`.\n\n---\n\n### `runGenerator(definition, inputs, options?)`\n\nExecutes a generator and returns a validated `StepSequence`.\n\n**Location:** `web/src/engine/generator/GeneratorRunner.ts`\n\n```typescript\nimport { runGenerator } from \"@/engine/generator\";\nimport binarySearchGenerator from \"@/algorithms/classical/binary-search/generator\";\n\nconst result = runGenerator(binarySearchGenerator, {\n  array: [1, 3, 5, 7, 9, 11, 13],\n  target: 7,\n});\n\nconsole.log(result.steps.length);  // Number of steps\nconsole.log(result.algorithmId);   // \"binary-search\"\nconsole.log(result.generatedBy);   // \"typescript\"\n```\n\n**Parameters:**\n\n| Parameter    | Type                          | Description                        |\n|--------------|-------------------------------|------------------------------------|\n| `definition` | `GeneratorDefinition\u003CTInputs>` | From `createGenerator()`         |\n| `inputs`     | `TInputs`                    | Algorithm input parameters         |\n| `options`    | `RunOptions` (optional)       | Configuration (e.g., `maxSteps`)  |\n\n**Returns:** `StepSequence`\n\n**Throws:** `GeneratorError` with the algorithm ID and step index for easy\ndebugging.\n\n**`RunOptions`:**\n\n```typescript\ninterface RunOptions {\n  readonly maxSteps?: number;  // Default: 10,000\n}\n```\n\n---\n\n### `StepSequence`\n\nThe complete output of a generator run:\n\n```typescript\ninterface StepSequence {\n  readonly formatVersion: number;      // Currently 1\n  readonly algorithmId: string;        // e.g., \"binary-search\"\n  readonly inputs: Record\u003Cstring, unknown>;\n  readonly steps: readonly Step[];\n  readonly generatedAt: string;        // ISO 8601 timestamp\n  readonly generatedBy: \"typescript\" | \"python\" | \"precomputed\";\n}\n```\n\n---\n\n### `Step`\n\nA single step in the execution trace:\n\n```typescript\ninterface Step {\n  readonly index: number;              // 0-based, auto-assigned\n  readonly id: string;                 // Template identifier\n  readonly title: string;              // Short heading\n  readonly explanation: string;        // Educational narration\n  readonly state: Record\u003Cstring, unknown>;  // Variable snapshot\n  readonly visualActions: readonly VisualAction[];\n  readonly codeHighlight: CodeHighlight;\n  readonly isTerminal: boolean;        // true only on last step\n  readonly phase?: string;             // Optional grouping label\n}\n```\n\n**Invariants enforced by the runner:**\n\n1. `steps[i].index === i` for all `i` (index contiguity).\n2. Exactly one step has `isTerminal === true`, and it is the last.\n3. At least one step exists.\n\n---\n\n### Visual Actions\n\nVisual actions are an **open vocabulary**. The `type` field is a plain string,\nnot a closed enum. Renderers silently ignore action types they do not recognize.\nThis means you can define new action types in a generator before any renderer\nsupports them.\n\n#### Base Interface\n\n```typescript\ninterface VisualAction {\n  readonly type: string;         // Action type identifier (camelCase)\n  readonly [key: string]: unknown;  // Action-specific parameters\n}\n```\n\n#### Common Action Types\n\n**Array actions:**\n\n| Type               | Parameters                         | Description                          |\n|--------------------|------------------------------------|--------------------------------------|\n| `highlightElement` | `index`, `color?`                  | Highlight a single element           |\n| `highlightRange`   | `from`, `to`, `color?`            | Highlight a contiguous range         |\n| `dimRange`         | `from`, `to`                       | Visually de-emphasize a range        |\n| `movePointer`      | `id`, `to`                         | Move a named pointer to an index     |\n| `swapElements`     | `i`, `j`                           | Swap two elements (sorting)          |\n| `compareElements`  | `i`, `j`, `result`                 | Compare two elements                 |\n| `markFound`        | `index`                            | Mark an element as found             |\n| `markNotFound`     | _(none)_                           | Indicate target was not found        |\n| `showMessage`      | `text`, `messageType`              | Display a transient text message     |\n\n**Graph actions:**\n\n| Type               | Parameters                         | Description                          |\n|--------------------|------------------------------------|--------------------------------------|\n| `visitNode`        | `nodeId`, `color?`                 | Mark a graph node as visited         |\n| `highlightEdge`    | `from`, `to`, `color?`            | Highlight a graph edge               |\n| `updateNodeValue`  | `nodeId`, `value`                  | Update a node's displayed value      |\n\n**Neural network actions:**\n\n| Type                 | Parameters                       | Description                          |\n|----------------------|----------------------------------|--------------------------------------|\n| `activateNeuron`     | `layer`, `index`, `value`        | Activate a neuron                    |\n| `propagateSignal`    | `fromLayer`, `toLayer`           | Signal propagation                   |\n| `showGradient`       | `layer`, `values`                | Display gradient values              |\n| `showWeights`        | `fromLayer`, `toLayer`, `weights`| Display weight matrix                |\n| `showLoss`           | `loss`, `lossFunction`, ...      | Show computed loss value             |\n\n**Attention actions:**\n\n| Type                     | Parameters                    | Description                      |\n|--------------------------|-------------------------------|----------------------------------|\n| `showAttentionWeights`   | `queryIdx`, `weights`         | Attention weights (must sum to 1)|\n| `highlightToken`         | `index`, `color?`             | Highlight a token                |\n| `showProjectionMatrix`   | `projectionType`, `matrix`    | Show Q/K/V projection           |\n| `showAttentionScores`    | `scores`                      | Raw attention scores             |\n| `showFullAttentionMatrix`| `weights`                     | Full attention matrix            |\n\n**Validated constraints:**\n\n- `highlightRange` / `dimRange`: `from \u003C= to` (enforced by runner)\n- `compareElements`: `result` must be `\"less\"`, `\"greater\"`, or `\"equal\"`\n- `showAttentionWeights`: weights must sum to 1.0 within `+/-1e-6`\n  (validated using Kahan summation for numerical stability)\n- `updateBarChart`: if `labels` is present, `labels.length === values.length`\n\n---\n\n### `CodeHighlight`\n\nMaps a step to source code lines:\n\n```typescript\ninterface CodeHighlight {\n  readonly language: string;       // Key in meta.json code.implementations\n  readonly lines: readonly number[];  // 1-indexed line numbers to highlight\n}\n```\n\n**Example:**\n\n```typescript\ncodeHighlight: { language: \"pseudocode\", lines: [5, 6] }\n```\n\nLines are 1-indexed to match editor conventions (line 1 is the first line).\nOrder does not matter; the renderer highlights all listed lines.\n\n---\n\n## Python Generator API\n\nPython generators follow a different pattern than TypeScript. Instead of\n`function*` / `yield`, they are regular functions that return a `list[Step]`.\n\n### Function Signature\n\n```python\ndef generate(inputs: dict[str, Any]) -> list[Step]:\n    \"\"\"Generate algorithm steps.\n\n    Parameters\n    ----------\n    inputs : dict\n        Algorithm input parameters.\n\n    Returns\n    -------\n    list[Step]\n        Ordered list of Step dataclass objects.\n    \"\"\"\n```\n\n### Step Dataclass\n\nThe Python `Step` class mirrors the TypeScript `Step` interface but uses\nsnake_case field names. The `to_dict()` method converts to camelCase for\nJSON wire format.\n\n```python\nfrom eigenvue._step_types import Step, VisualAction, CodeHighlight\n\nstep = Step(\n    index=0,\n    id=\"initialize\",\n    title=\"Initialize Search\",\n    explanation=\"Setting up the algorithm...\",\n    state={\"array\": list(array), \"target\": target},\n    visual_actions=[\n        VisualAction(type=\"highlightRange\", from_=0, to=n - 1, color=\"highlight\"),\n    ],\n    code_highlight=CodeHighlight(language=\"pseudocode\", lines=[1]),\n    is_terminal=False,\n    phase=\"initialization\",\n)\n```\n\n### snake_case to camelCase Conversion\n\nThe Python `Step` and `VisualAction` dataclasses use snake_case internally but\nconvert to camelCase when serialized to JSON via `to_dict()`:\n\n| Python (snake_case)  | JSON (camelCase)    |\n|----------------------|---------------------|\n| `is_terminal`        | `isTerminal`        |\n| `visual_actions`     | `visualActions`     |\n| `code_highlight`     | `codeHighlight`     |\n| `step_index`         | `stepIndex`         |\n| `message_type`       | `messageType`       |\n\nFor `VisualAction` fields, `from_` (with trailing underscore to avoid the\nPython keyword) maps to `from` in JSON.\n\n### Running a Python Generator\n\n```python\nfrom eigenvue.runner import run_generator\n\n# With custom inputs\nsteps = run_generator(\"binary-search\", {\n    \"array\": [1, 3, 5, 7, 9],\n    \"target\": 5,\n})\n\n# With default inputs from meta.json\nsteps = run_generator(\"binary-search\")\n```\n\nThe runner:\n1. Looks up the generator in the registry.\n2. Resolves inputs (custom or defaults from `meta.json`).\n3. Calls the generator function.\n4. Serializes `Step` objects to camelCase dicts via `to_dict()`.\n5. Validates step sequence invariants (index contiguity, single terminal).\n\n### Key Differences from TypeScript\n\n| Concern                | TypeScript                                | Python                                  |\n|------------------------|-------------------------------------------|-----------------------------------------|\n| Function type          | Generator function (`function*`)          | Regular function returning `list`       |\n| Step creation          | `yield step({ ... })`                     | `steps.append(Step(...))`               |\n| Index assignment       | Automatic (runner assigns)                | Manual (you set `index`)                |\n| State snapshots        | `[...array]` or `structuredClone()`       | `list(array)` or `copy.deepcopy()`      |\n| Field naming           | camelCase                                 | snake_case (auto-converted on serialize)|\n| Validation timing      | Runner validates after all yields         | Runner validates after function returns |\n| Registry               | Auto-discovered from file path            | Manual entry in `__init__.py`           |\n\n---\n\n## Inputs Contract\n\nBoth TypeScript and Python generators receive the same logical inputs. The\ninputs are validated against the JSON Schema defined in `meta.json` before\nreaching the generator.\n\n### Input Schema Example\n\nFrom `meta.json`:\n\n```json\n{\n  \"inputs\": {\n    \"schema\": {\n      \"array\": {\n        \"type\": \"array\",\n        \"items\": { \"type\": \"number\", \"minimum\": -999, \"maximum\": 999 },\n        \"minItems\": 1,\n        \"maxItems\": 20\n      },\n      \"target\": {\n        \"type\": \"number\",\n        \"minimum\": -999,\n        \"maximum\": 999\n      }\n    },\n    \"defaults\": {\n      \"array\": [1, 3, 5, 7, 9],\n      \"target\": 5\n    }\n  }\n}\n```\n\n### Type Safety\n\nIn TypeScript, define an interface that matches your input schema:\n\n```typescript\ninterface BinarySearchInputs extends Record\u003Cstring, unknown> {\n  readonly array: readonly number[];\n  readonly target: number;\n}\n```\n\nThe `extends Record\u003Cstring, unknown>` is required by `createGenerator`'s\ngeneric constraint. It ensures inputs are always a plain object.\n\nIn Python, inputs arrive as a plain `dict[str, Any]`. Type-narrow them at the\ntop of your generator:\n\n```python\ndef generate(inputs: dict[str, Any]) -> list[Step]:\n    array: list[int] = list(inputs[\"array\"])\n    target: int = inputs[\"target\"]\n```\n\n---\n\n## Step Ordering Rules\n\nSteps must follow these ordering rules, enforced by the runner:\n\n1. **Index contiguity.** `steps[i].index === i` for all `i`. No gaps, no\n   duplicates, no reordering.\n\n2. **Single terminal.** Exactly one step has `isTerminal: true`. It must be\n   the last step in the sequence.\n\n3. **Non-empty.** At least one step must be yielded.\n\n4. **Phase consistency.** While not enforced at the type level, phases should\n   flow logically: `\"initialization\"` -> `\"search\"` / `\"computation\"` ->\n   `\"result\"`. Consecutive steps with the same phase are visually grouped.\n\n5. **Determinism.** Given the same inputs, a generator must produce the exact\n   same steps every time. No random behavior, no time-dependent logic, no\n   external state. This is essential for cross-language parity and\n   reproducibility.","src/content/docs/contributing/generator-api.mdx","339f6d2ebc9a6955","contributing/writing-tests",{"id":408,"data":410,"body":416,"filePath":417,"digest":418,"deferredRender":16},{"title":411,"description":412,"editUrl":16,"head":413,"template":50,"sidebar":414,"pagefind":16,"draft":39},"Writing Tests","Complete testing strategy for Eigenvue, covering test fixtures, TypeScript generator tests (Vitest), Python generator tests (pytest), cross-language parity, layout tests, and E2E tests.",[],{"hidden":39,"attrs":415},{},"Testing is central to Eigenvue's reliability. Every algorithm has both\nTypeScript and Python generators, and both must produce **identical** output.\nThis page explains every layer of the testing strategy.\n\n---\n\n## Test Fixture Format\n\nTest fixtures are golden JSON files that define the expected step output for a\nspecific set of inputs. They are the single source of truth that both TypeScript\nand Python tests validate against.\n\n### Fixture Location\n\nFixtures live alongside the TypeScript generator they test:\n\n```\nalgorithms/classical/binary-search/tests/\n  found.fixture.json\n  not-found.fixture.json\n  single-element.fixture.json\n  edge-left.fixture.json\n  edge-right.fixture.json\n  generator.test.ts\n```\n\n### Fixture Schema\n\n```json\n{\n  \"description\": \"Human-readable description of this test case.\",\n  \"inputs\": {\n    \"array\": [1, 3, 5, 7, 9, 11, 13, 15, 17, 19],\n    \"target\": 13\n  },\n  \"expected\": {\n    \"stepCount\": 9,\n    \"terminalStepId\": \"found\",\n    \"result\": 6,\n    \"stepIds\": [\n      \"initialize\",\n      \"calculate_mid\",\n      \"search_right\",\n      \"calculate_mid\",\n      \"search_left\",\n      \"calculate_mid\",\n      \"search_right\",\n      \"calculate_mid\",\n      \"found\"\n    ],\n    \"keyStates\": [\n      { \"stepIndex\": 0, \"field\": \"left\", \"value\": 0 },\n      { \"stepIndex\": 0, \"field\": \"right\", \"value\": 9 },\n      { \"stepIndex\": 1, \"field\": \"mid\", \"value\": 4 },\n      { \"stepIndex\": 8, \"field\": \"result\", \"value\": 6 }\n    ]\n  }\n}\n```\n\n### Field Reference\n\n| Field                       | Type       | Description                                    |\n|-----------------------------|------------|------------------------------------------------|\n| `description`               | `string`   | What this test case verifies                   |\n| `inputs`                    | `object`   | Algorithm input parameters                     |\n| `expected.stepCount`        | `number`   | Total number of steps                          |\n| `expected.terminalStepId`   | `string`   | The `id` of the final (terminal) step          |\n| `expected.result`           | `any`      | The `result` field in the terminal step's state|\n| `expected.stepIds`          | `string[]` | Ordered list of step IDs                       |\n| `expected.keyStates`        | `array`    | Spot-checks on specific state fields           |\n\n### Fixture Design Guidelines\n\n1. **Cover the core paths.** At minimum: success case, failure case, edge cases.\n2. **Keep arrays small.** 5--10 elements. Easy to compute expected values by hand.\n3. **Use surgical state checks.** Only assert the fields that matter for the test.\n   Writing out the entire expected state of every step is brittle and hard to\n   maintain.\n4. **Name files descriptively.** `found.fixture.json`, `not-found.fixture.json`,\n   `single-element.fixture.json`, `edge-left.fixture.json`.\n\n---\n\n## TypeScript Generator Tests (Vitest)\n\n### Test File Location\n\n```\nalgorithms/\u003Ccategory>/\u003Calgorithm>/tests/generator.test.ts\n```\n\n### Test Structure\n\nEvery generator test file follows the same pattern:\n\n```typescript\nimport { describe, it, expect } from \"vitest\";\nimport { runGenerator } from \"@/engine/generator\";\nimport myGenerator from \"../generator\";\n\n// Import fixtures\nimport foundFixture from \"./found.fixture.json\";\nimport notFoundFixture from \"./not-found.fixture.json\";\n\n//  Helper \n\nfunction assertFixture(fixture: {\n  description: string;\n  inputs: Record\u003Cstring, unknown>;\n  expected: {\n    stepCount: number;\n    terminalStepId: string;\n    result: unknown;\n    stepIds: string[];\n    keyStates: Array\u003C{ stepIndex: number; field: string; value: unknown }>;\n  };\n}) {\n  const result = runGenerator(myGenerator, fixture.inputs);\n  const { steps } = result;\n\n  expect(steps.length).toBe(fixture.expected.stepCount);\n\n  const terminal = steps[steps.length - 1]!;\n  expect(terminal.id).toBe(fixture.expected.terminalStepId);\n  expect(terminal.isTerminal).toBe(true);\n  expect(terminal.state.result).toBe(fixture.expected.result);\n\n  expect(steps.map((s) => s.id)).toEqual(fixture.expected.stepIds);\n\n  for (const check of fixture.expected.keyStates) {\n    expect(steps[check.stepIndex]!.state[check.field]).toEqual(check.value);\n  }\n}\n```\n\n### Test Categories\n\nEvery generator test file should include these four categories:\n\n#### 1. Fixture Tests\n\nValidate against golden fixtures:\n\n```typescript\ndescribe(\"Fixture Tests\", () => {\n  it(\"found: default example\", () => assertFixture(foundFixture));\n  it(\"not found: target absent\", () => assertFixture(notFoundFixture));\n});\n```\n\n#### 2. Format Compliance Tests\n\nVerify structural invariants of the step format:\n\n```typescript\ndescribe(\"Format Compliance\", () => {\n  const result = runGenerator(myGenerator, defaultInputs);\n\n  it(\"produces a valid StepSequence\", () => {\n    expect(result.formatVersion).toBe(1);\n    expect(result.algorithmId).toBe(\"my-algorithm\");\n    expect(result.generatedBy).toBe(\"typescript\");\n  });\n\n  it(\"all step indices are contiguous\", () => {\n    result.steps.forEach((step, i) => expect(step.index).toBe(i));\n  });\n\n  it(\"exactly one terminal step, it is the last\", () => {\n    const terminals = result.steps.filter((s) => s.isTerminal);\n    expect(terminals.length).toBe(1);\n    expect(terminals[0]!.index).toBe(result.steps.length - 1);\n  });\n\n  it(\"all step IDs match pattern\", () => {\n    for (const step of result.steps) {\n      expect(step.id).toMatch(/^[a-z0-9][a-z0-9_-]*$/);\n    }\n  });\n\n  it(\"all code highlight lines are positive integers\", () => {\n    for (const step of result.steps) {\n      for (const line of step.codeHighlight.lines) {\n        expect(Number.isInteger(line)).toBe(true);\n        expect(line).toBeGreaterThanOrEqual(1);\n      }\n    }\n  });\n});\n```\n\n#### 3. State Immutability Tests\n\nVerify that state snapshots are independent:\n\n```typescript\ndescribe(\"State Immutability\", () => {\n  it(\"step states are independent snapshots\", () => {\n    const result = runGenerator(myGenerator, someInputs);\n    const arrays = result.steps.map((s) => s.state.array as number[]);\n\n    // All arrays should contain the same values.\n    for (const arr of arrays) {\n      expect(arr).toEqual(someInputs.array);\n    }\n\n    // But they should NOT be the same reference.\n    for (let i = 0; i \u003C arrays.length - 1; i++) {\n      expect(arrays[i]).not.toBe(arrays[i + 1]);\n    }\n  });\n\n  it(\"mutating one step's state does not affect others\", () => {\n    const result = runGenerator(myGenerator, someInputs);\n    const step0Array = result.steps[0]!.state.array as number[];\n    step0Array[0] = 9999;\n    expect((result.steps[1]!.state.array as number[])[0]).not.toBe(9999);\n  });\n});\n```\n\n#### 4. Edge Case Tests\n\nCover boundary conditions specific to your algorithm:\n\n```typescript\ndescribe(\"Edge Cases\", () => {\n  it(\"single element, target found\", () => {\n    const result = runGenerator(myGenerator, { array: [42], target: 42 });\n    const terminal = result.steps[result.steps.length - 1]!;\n    expect(terminal.state.result).toBe(0);\n  });\n\n  it(\"single element, target not found\", () => {\n    const result = runGenerator(myGenerator, { array: [42], target: 99 });\n    const terminal = result.steps[result.steps.length - 1]!;\n    expect(terminal.state.result).toBe(-1);\n  });\n});\n```\n\n### Running TypeScript Tests\n\n```bash\ncd web\n\n# Run all tests\nnpm run test\n\n# Run tests for a specific algorithm\nnpx vitest run algorithms/classical/binary-search/tests/\n\n# Watch mode (re-runs on file changes)\nnpx vitest watch algorithms/classical/binary-search/tests/\n\n# With coverage report\nnpx vitest run --coverage\n```\n\n---\n\n## Python Generator Tests (pytest)\n\n### Test Location\n\n```\npython/tests/generators/classical/test_binary_search.py\n```\n\n### Test Structure\n\n```python\n\"\"\"Binary Search Generator  Python Tests.\"\"\"\n\nimport pytest\nfrom eigenvue.runner import run_generator\n\n\nclass TestBinarySearchFound:\n    \"\"\"Test cases where the target is found.\"\"\"\n\n    def test_default_example(self) -> None:\n        steps = run_generator(\"binary-search\", {\n            \"array\": [1, 3, 5, 7, 9, 11, 13, 15, 17, 19],\n            \"target\": 13,\n        })\n        assert len(steps) == 9\n        assert steps[-1][\"id\"] == \"found\"\n        assert steps[-1][\"isTerminal\"] is True\n        assert steps[-1][\"state\"][\"result\"] == 6\n\n    def test_single_element(self) -> None:\n        steps = run_generator(\"binary-search\", {\n            \"array\": [42],\n            \"target\": 42,\n        })\n        assert steps[-1][\"id\"] == \"found\"\n        assert steps[-1][\"state\"][\"result\"] == 0\n\n\nclass TestBinarySearchNotFound:\n    \"\"\"Test cases where the target is not found.\"\"\"\n\n    def test_target_absent(self) -> None:\n        steps = run_generator(\"binary-search\", {\n            \"array\": [2, 4, 6, 8, 10],\n            \"target\": 5,\n        })\n        assert steps[-1][\"id\"] == \"not_found\"\n        assert steps[-1][\"state\"][\"result\"] == -1\n\n\nclass TestBinarySearchInvariants:\n    \"\"\"Test structural invariants of the step sequence.\"\"\"\n\n    def test_index_contiguity(self) -> None:\n        steps = run_generator(\"binary-search\", {\n            \"array\": [1, 3, 5, 7, 9],\n            \"target\": 5,\n        })\n        for i, step in enumerate(steps):\n            assert step[\"index\"] == i\n\n    def test_single_terminal(self) -> None:\n        steps = run_generator(\"binary-search\")\n        terminals = [s for s in steps if s.get(\"isTerminal\", False)]\n        assert len(terminals) == 1\n        assert terminals[0] is steps[-1]\n```\n\n### Running Python Tests\n\n```bash\ncd python\n\n# Run all tests\npytest\n\n# Run tests for a specific module\npytest tests/generators/classical/test_binary_search.py\n\n# Verbose output\npytest -v\n\n# Stop on first failure\npytest -x\n\n# With coverage\npytest --cov=eigenvue\n```\n\n---\n\n## Cross-Language Parity Tests\n\nThe parity test verifies that Python and TypeScript produce **identical** JSON\noutput. This is the most important test in the project.\n\n### How It Works\n\nThe script `scripts/verify-step-parity.py`:\n\n1. Loads each golden fixture (already in camelCase JSON format).\n2. Deserializes it into Python `Step` dataclasses via `from_dict()`.\n3. Re-serializes it back to camelCase dicts via `to_dict()`.\n4. Compares the re-serialized output to the original JSON byte-for-byte.\n\nIf the round-trip is clean, the Python dataclasses are proven wire-compatible\nwith the TypeScript interfaces.\n\n### Running the Parity Test\n\n```bash\npython scripts/verify-step-parity.py\n```\n\nExpected output:\n\n```\n============================================================\nCross-Language Step Parity Verification\n============================================================\n\nVerifying: binary-search-found.fixture.json\n  PASS\nVerifying: binary-search-not-found.fixture.json\n  PASS\nVerifying: single-element.fixture.json\n  PASS\nVerifying: minimal-valid.fixture.json\n  PASS\n\n============================================================\nALL FIXTURES PASSED\n============================================================\n```\n\n### When Parity Fails\n\nSee [Cross-Language Parity](/docs/contributing/cross-language-parity/) for debugging\nstrategies for common parity failures.\n\n---\n\n## Layout Tests\n\nLayout tests verify that layout functions produce correct primitives for\nknown step data.\n\n### Test Location\n\n```\nweb/src/engine/layouts/__tests__/\n```\n\n### What to Test\n\n```typescript\nimport { describe, it, expect } from \"vitest\";\n\ndescribe(\"array-with-pointers layout\", () => {\n  it(\"produces N elements for an N-element array\", () => {\n    // Create a mock step with a known array\n    // Call the layout function\n    // Assert the number of element primitives\n  });\n\n  it(\"all primitive IDs are unique\", () => {\n    // IDs must be unique for animation diffing\n    const ids = scene.primitives.map((p) => p.id);\n    expect(new Set(ids).size).toBe(ids.length);\n  });\n\n  it(\"elements fit within canvas bounds\", () => {\n    for (const p of scene.primitives) {\n      if (p.kind === \"element\") {\n        expect(p.x).toBeGreaterThanOrEqual(0);\n        expect(p.x).toBeLessThanOrEqual(canvasSize.width);\n        expect(p.y).toBeGreaterThanOrEqual(0);\n        expect(p.y).toBeLessThanOrEqual(canvasSize.height);\n      }\n    }\n  });\n\n  it(\"handles empty array gracefully\", () => {\n    // Should return an empty primitives array, not throw\n  });\n\n  it(\"processes highlightElement visual action\", () => {\n    // Create a step with a highlightElement action\n    // Verify the corresponding element has the correct fill color\n  });\n});\n```\n\n---\n\n## E2E Tests (Playwright)\n\nEnd-to-end tests verify that algorithms render correctly in the browser and\nthat user interactions (stepping forward/backward, changing inputs) work.\n\n### Test Location\n\n```\ntests/e2e/\n```\n\n### Example E2E Test\n\n```typescript\nimport { test, expect } from \"@playwright/test\";\n\ntest(\"binary search visualization loads and steps forward\", async ({ page }) => {\n  await page.goto(\"/algorithms/binary-search\");\n\n  // Wait for the visualization to load\n  await expect(page.locator(\"canvas\")).toBeVisible();\n\n  // Verify the step counter starts at 1\n  await expect(page.locator(\"[data-testid='step-counter']\")).toContainText(\"1\");\n\n  // Click the \"Next\" button\n  await page.click(\"[data-testid='next-step']\");\n\n  // Verify the step counter advances\n  await expect(page.locator(\"[data-testid='step-counter']\")).toContainText(\"2\");\n});\n\ntest(\"binary search handles custom inputs\", async ({ page }) => {\n  await page.goto(\"/algorithms/binary-search\");\n\n  // Open the input editor\n  await page.click(\"[data-testid='edit-inputs']\");\n\n  // Modify the target value\n  await page.fill(\"[data-testid='input-target']\", \"99\");\n\n  // Run the algorithm\n  await page.click(\"[data-testid='run-algorithm']\");\n\n  // Should reach \"not found\" terminal state\n  // Navigate to the last step\n  await page.click(\"[data-testid='last-step']\");\n  await expect(page.locator(\"[data-testid='step-title']\")).toContainText(\"Not Found\");\n});\n```\n\n### Running E2E Tests\n\n```bash\n# Install Playwright browsers (first time only)\nnpx playwright install\n\n# Run E2E tests\nnpx playwright test\n\n# Run with UI mode (interactive)\nnpx playwright test --ui\n\n# Run a specific test file\nnpx playwright test tests/e2e/binary-search.spec.ts\n```\n\n---\n\n## Floating-Point Tolerance\n\nMany algorithms involve floating-point arithmetic (attention weights, gradient\nvalues, loss computations). When comparing floating-point values across\nTypeScript and Python, use a tolerance of **+/-1e-9**.\n\n### In Vitest (TypeScript)\n\n```typescript\n// For individual values\nexpect(actual).toBeCloseTo(expected, 9);  // 9 decimal digits\n\n// For arrays of floats\nfor (let i = 0; i \u003C actual.length; i++) {\n  expect(actual[i]).toBeCloseTo(expected[i], 9);\n}\n```\n\n### In pytest (Python)\n\n```python\nimport pytest\n\n# For individual values\nassert actual == pytest.approx(expected, abs=1e-9)\n\n# For lists of floats\nassert actual_list == pytest.approx(expected_list, abs=1e-9)\n```\n\n### Why 1e-9?\n\nJavaScript uses IEEE 754 double-precision (64-bit) floats, and Python's `float`\nis also IEEE 754 double-precision. Both have about 15--17 significant decimal\ndigits of precision. A tolerance of 1e-9 allows for small accumulation errors\nfrom different ordering of operations while still catching meaningful numerical\nbugs.\n\n:::caution\nThe parity validation script compares JSON strings byte-for-byte. For\nfloating-point values that appear in step state, both generators must produce\nthe **exact same** string representation. This means the order of floating-point\noperations matters. See [Cross-Language Parity](/docs/contributing/cross-language-parity/)\nfor common pitfalls and solutions.\n:::\n\n---\n\n## Test Checklist\n\nBefore submitting a PR, verify:\n\n- [ ] All Vitest tests pass: `cd web && npm run test`\n- [ ] All pytest tests pass: `cd python && pytest`\n- [ ] Cross-language parity passes: `python scripts/verify-step-parity.py`\n- [ ] New algorithm has fixtures for: found, not found, single element\n- [ ] Format compliance tests are included\n- [ ] State immutability tests are included\n- [ ] Edge case tests cover algorithm-specific boundaries\n- [ ] No test uses random data (all tests are deterministic)","src/content/docs/contributing/writing-tests.mdx","780fe9ccc6b9a9fa","getting-started/jupyter",{"id":419,"data":421,"body":427,"filePath":428,"digest":429,"deferredRender":16},{"title":422,"description":423,"editUrl":16,"head":424,"template":50,"sidebar":425,"pagefind":16,"draft":39},"Getting Started  Jupyter Integration","Embed interactive Eigenvue visualizations directly inside Jupyter notebooks and Google Colab using the eigenvue.jupyter() function.",[],{"hidden":39,"attrs":426},{},"Eigenvue integrates with Jupyter notebooks so you can explore algorithm\nvisualizations right next to your own code and analysis.\n\n## Displaying a Visualization\n\nUse `eigenvue.jupyter()` to render an interactive widget in any notebook cell:\n\n```python\nimport eigenvue\n\neigenvue.jupyter(\"quicksort\", data=[10, 3, 7, 1, 9, 5, 2, 8, 4, 6])\n```\n\nWhen you run the cell, an inline widget appears containing the full Eigenvue\nvisualizer -- canvas, code panel, explanation panel, and playback controls.\n\n## Cell Output\n\nThe widget is rendered as an **IFrame** that embeds a local Flask server. You\nget the same playback controls available on the web app:\n\n- **Play / Pause** -- start or stop continuous playback.\n- **Step Forward / Step Back** -- advance or rewind one step at a time.\n- **Speed selector** -- choose 0.5x, 1x, 2x, or 4x.\n- **Progress bar** -- drag to jump to any step.\n- **Reset** -- return to the first step.\n\nThe IFrame resizes to fit the notebook output area. Scroll within the widget if\nyour screen is narrow.\n\n## Multiple Visualizations\n\nYou can display several visualizations in the same notebook. Each call to\n`eigenvue.jupyter()` in a separate cell produces its own independent widget:\n\n```python\n# Cell 1\nimport eigenvue\n\neigenvue.jupyter(\"bubble-sort\", data=[5, 1, 4, 2, 8])\n```\n\n```python\n# Cell 2\neigenvue.jupyter(\"transformer-attention\", sequence=[\"the\", \"cat\", \"sat\"])\n```\n\nEach widget maintains its own playback state, so you can compare algorithms\nside by side without interference.\n\n## Google Colab Compatibility\n\nEigenvue works in **Google Colab** with no extra setup. Install the package in\na code cell and call `eigenvue.jupyter()` as usual:\n\n```python\n!pip install eigenvue\n\nimport eigenvue\n\neigenvue.jupyter(\"dijkstra\", graph={\n    \"A\": {\"B\": 1, \"C\": 4},\n    \"B\": {\"C\": 2, \"D\": 6},\n    \"C\": {\"D\": 3},\n    \"D\": {}\n}, start=\"A\")\n```\n\nColab routes the local server through its built-in proxy, so the IFrame loads\ncorrectly without any manual port configuration.\n\n## Combining with Analysis\n\nA common workflow is to collect step data with `eigenvue.steps()`, run your own\nanalysis, and then display the visualization with `eigenvue.jupyter()` in the\nsame notebook:\n\n```python\nimport eigenvue\n\n# 1. Collect step data programmatically\nframes = eigenvue.steps(\"a-star\", grid=[\n    [0, 0, 0, 1, 0],\n    [0, 1, 0, 1, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 0, 0, 0, 0],\n], start=(0, 0), goal=(4, 4))\n\n# 2. Analyse the results\ntotal_steps = len(frames)\nnodes_visited = sum(\n    1 for f in frames if \"visit\" in f[\"description\"].lower()\n)\n\nprint(f\"Total steps:   {total_steps}\")\nprint(f\"Nodes visited: {nodes_visited}\")\n\n# 3. Display the interactive visualization\neigenvue.jupyter(\"a-star\", grid=[\n    [0, 0, 0, 1, 0],\n    [0, 1, 0, 1, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 0, 0, 0, 0],\n], start=(0, 0), goal=(4, 4))\n```\n\nThis pattern lets you quantify algorithm behaviour (step counts, comparisons,\nmemory usage) and visually verify the results -- all within a single notebook.","src/content/docs/getting-started/jupyter.mdx","065b6c84e760f9bc","getting-started/python-package",{"id":430,"data":432,"body":438,"filePath":439,"digest":440,"deferredRender":16},{"title":433,"description":434,"editUrl":16,"head":435,"template":50,"sidebar":436,"pagefind":16,"draft":39},"Getting Started  Python Package","Install the eigenvue Python package and use its four public functions to list algorithms, launch visualizations, and access step data programmatically.",[],{"hidden":39,"attrs":437},{},"The `eigenvue` Python package lets you launch the same interactive\nvisualizations you see on [eigenvue.web.app](https://eigenvue.web.app) -- directly from\nyour terminal, scripts, or notebooks.\n\n## Installation\n\nInstall from PyPI:\n\n```bash\npip install eigenvue\n```\n\n**Requirements:**\n\n- Python 3.10 or later\n- The only runtime dependency is **Flask**, which is installed automatically\n\nVerify the installation:\n\n```python\nimport eigenvue\n\nprint(eigenvue.__version__)  # \"0.1.0\"\n```\n\n## Quick Start -- Listing Algorithms\n\nUse `eigenvue.list()` to see every algorithm the package ships with:\n\n```python\nimport eigenvue\n\nalgorithms = eigenvue.list()\n\nfor algo in algorithms:\n    print(f\"{algo['id']:30s} {algo['category']:20s} {algo['name']}\")\n```\n\n`list()` returns a list of dictionaries. Each dictionary contains at minimum\nthe keys `id`, `name`, and `category` (`\"classical\"`, `\"deep_learning\"`, or\n`\"generative_ai\"`).\n\n## Quick Start -- Opening a Visualization\n\nCall `eigenvue.show()` with an algorithm ID to launch the visualizer in your\ndefault browser:\n\n```python\nimport eigenvue\n\neigenvue.show(\"bubble-sort\", data=[5, 3, 8, 1, 2])\n```\n\nA local Flask server starts in the background and serves the interactive\nthree-panel visualizer. The browser tab opens automatically. Press `Ctrl+C` in\nthe terminal to stop the server when you are done.\n\nYou can pass algorithm-specific keyword arguments (like `data` above) to\ncustomise the input. If you omit them, sensible defaults are used.\n\n## Programmatic Step Access\n\nWhen you need raw step data -- for analysis, testing, or integration with other\ntools -- use `eigenvue.steps()`:\n\n```python\nimport eigenvue\n\nframes = eigenvue.steps(\"binary-search\", data=[1, 2, 3, 4, 5, 6, 7], target=4)\n\nprint(f\"Total steps: {len(frames)}\")\n\nfor i, frame in enumerate(frames):\n    print(f\"Step {i}: {frame['description']}\")\n    print(f\"  State: {frame['state']}\")\n```\n\n`steps()` returns a list of frame dictionaries. Each frame includes:\n\n| Key | Type | Description |\n| --- | --- | --- |\n| `description` | `str` | Plain-language explanation of the step |\n| `state` | `dict` | A snapshot of all relevant variables at this point |\n| `code_line` | `int` | The highlighted line number in the pseudocode |\n\nThis makes it straightforward to write assertions in tests, collect metrics, or\nfeed step data into your own visualisation pipeline.\n\n## Type Safety\n\nThe package ships with a `py.typed` marker and full type hints on every public\nfunction. If you use a type checker such as **mypy** or **Pyright**, you get\nautocompletion and static analysis out of the box:\n\n```python\n# Your editor will infer the return type automatically.\nframes: list[dict[str, object]] = eigenvue.steps(\"merge-sort\", data=[9, 4, 7])\n```\n\nType stubs are bundled inside the package -- no extra install is required.","src/content/docs/getting-started/python-package.mdx","502e8a5aadacc73f","getting-started/web-app",{"id":441,"data":443,"body":449,"filePath":450,"digest":451,"deferredRender":16},{"title":444,"description":445,"editUrl":16,"head":446,"template":50,"sidebar":447,"pagefind":16,"draft":39},"Getting Started  Web Application","Learn how to use Eigenvue directly in your browser to explore interactive visualizations of algorithms, AI architectures, and quantum computing concepts.",[],{"hidden":39,"attrs":448},{},"Eigenvue runs entirely in your browser -- no installation, no signup, no friction.\nOpen [eigenvue.web.app](https://eigenvue.web.app) and start exploring.\n\n## Opening the App\n\nNavigate to [eigenvue.web.app](https://eigenvue.web.app). The landing page gives you a\nbrief overview of the platform and a direct link to the algorithm catalog. There\nis no account creation step; every visualization is available immediately.\n\n## Browsing Algorithms\n\nThe catalog lives at [`/algorithms`](https://eigenvue.web.app/algorithms). From\nhere you can discover all 17 algorithms organised into three categories:\n\n| Category | Count | Examples |\n| --- | --- | --- |\n| **Classical** | 7 | Sorting, graph traversal, dynamic programming |\n| **Deep Learning** | 5 | Transformers, CNNs, RNNs |\n| **Generative AI** | 5 | Diffusion, VAEs, GANs |\n\nUse the **category tabs** at the top of the catalog to filter by category. You\ncan further narrow results with:\n\n- **Difficulty filters** -- toggle beginner, intermediate, or advanced tags.\n- **Text search** -- type a keyword in the search bar to match algorithm names\n  and descriptions.\n\nClick any algorithm card to open its visualizer page.\n\n## The Visualizer Page\n\nEvery algorithm page uses a **three-panel layout**:\n\n1. **Canvas** (center) -- the main animation area where the algorithm plays out\n   step by step. Data structures, model layers, and state transitions are\n   rendered here.\n2. **Code Panel** (left) -- displays the algorithm source with three switchable\n   tabs: **Pseudocode**, **Python**, and **JavaScript**. The currently executing\n   line is highlighted as you step through the visualization.\n3. **Explanation Panel** (right) -- contains the algorithm **title**,\n   a **plain-language explanation** of the current step, and a **state\n   inspector** that shows live variable values and data-structure contents.\n\nYou can resize any panel by dragging the dividers between them.\n\n## Playback Controls\n\nThe playback bar sits below the canvas. It contains, from left to right:\n\n| Control | Icon | Action |\n| --- | --- | --- |\n| Reset |  | Jump to the very first step |\n| Step Back |  | Move one step backward |\n| Play / Pause |  /  | Start or pause continuous playback |\n| Step Forward | \\| | Move one step forward |\n| Speed | 0.5x / 1x / 2x / 4x | Choose the playback speed |\n| Step counter | -- | Displays the current step number out of the total |\n| Progress bar | -- | A scrubber you can drag to jump to any step |\n\nPlayback wraps around: when the animation reaches the last step and you press\n**Step Forward**, the visualizer returns to step 1.\n\n## Modifying Inputs\n\nBelow the code panel you will find the **input editor**. Each algorithm exposes\na set of configurable parameters (for example, the array to sort, the graph\nadjacency list, or the learning rate).\n\n1. Edit the parameter values directly in the input fields.\n2. Alternatively, open the **Examples** dropdown to load a pre-built input set.\n3. Press the **Run** button to regenerate the visualization with your new\n   inputs.\n\nThe canvas, code highlights, and explanation panel all update to reflect the new\nrun.\n\n## Keyboard Shortcuts\n\nYou can control playback without touching the mouse:\n\n| Key | Action |\n| --- | --- |\n| `Space` | Play / Pause |\n| `` (Right Arrow) | Step forward |\n| `` (Left Arrow) | Step back |\n| `Home` | Jump to first step |\n| `End` | Jump to last step |\n| `+` | Increase speed |\n| `-` | Decrease speed |\n\n## Educational Content\n\nEach algorithm page includes a **Learn** section accessible via a tab or\nscroll target below the visualizer. Inside you will find:\n\n- **Key Concepts** -- a concise summary of the theory behind the algorithm,\n  including time/space complexity and common use-cases.\n- **Common Pitfalls** -- mistakes learners frequently make and how to avoid\n  them.\n- **Quiz** -- a short set of questions to test your understanding. Answers are\n  revealed inline so you can self-check at your own pace.\n\nUse the Learn section alongside the visualizer to build both intuition and\nformal understanding of every algorithm Eigenvue covers.","src/content/docs/getting-started/web-app.mdx","558c5245364854de","research/comparison",{"id":452,"data":454,"body":460,"filePath":461,"digest":462,"deferredRender":16},{"title":455,"description":456,"editUrl":16,"head":457,"template":50,"sidebar":458,"pagefind":16,"draft":39},"Comparison with Existing Tools","Fair comparison of Eigenvue with existing algorithm visualization tools.",[],{"hidden":39,"attrs":459},{},"## Landscape Overview\n\nThe algorithm visualization space includes several well-established tools, each with distinct strengths. This page offers a fair comparison to help readers understand where Eigenvue fits and where existing tools may better serve specific needs.\n\n## Comparison Table\n\n| Tool | Scope | Platform | Interaction | Shareability | Open Source |\n|------|-------|----------|-------------|--------------|-------------|\n| [VisuAlgo](https://visualgo.net) | Classical algorithms | Web | Step-through, custom inputs | URL params | Partial |\n| [Algorithm Visualizer](https://algorithm-visualizer.org) | Classical algorithms | Web | Step-through | Limited | Yes |\n| [BertViz](https://github.com/jessevig/bertviz) | Attention visualization | Python / Jupyter | Static / Interactive | No | Yes |\n| [TensorFlow Playground](https://playground.tensorflow.org) | Neural networks | Web | Interactive | URL | Yes (Google) |\n| [Quirk](https://algassert.com/quirk) | Quantum circuits | Web | Circuit builder | URL | Yes |\n| [3Blue1Brown](https://www.3blue1brown.com) | Math / ML concepts | Video | Passive viewing | Video links | N/A |\n| **Eigenvue** | Classical + DL + GenAI + Quantum | Web + Python | Step-through, custom inputs | Deep links | Yes (MIT) |\n\n## Where Competitors Excel\n\nIt is important to acknowledge the strengths of existing tools:\n\n- **VisuAlgo** has the most comprehensive coverage of classical data structures and algorithms, with detailed pseudocode annotations and support for many languages. For a student focused exclusively on classical algorithms, VisuAlgo remains an excellent resource.\n- **Algorithm Visualizer** provides an integrated code editor that lets users write and visualize their own algorithm implementations, offering a uniquely hands-on learning experience.\n- **BertViz** offers deep, research-grade attention visualization with support for multiple attention head views (model view, head view, neuron view) that are specifically tailored for NLP research workflows.\n- **TensorFlow Playground** delivers an exceptionally intuitive interface for understanding neural network training dynamics, with real-time feedback on decision boundaries that is difficult to replicate in a step-based format.\n- **Quirk** provides a polished, drag-and-drop circuit builder with real-time state vector display that is purpose-built for quantum computing education.\n- **3Blue1Brown** sets the standard for mathematical storytelling through animation, with production quality and narrative depth that interactive tools do not attempt to match.\n\n## Eigenvue Differentiators\n\nEigenvue's value lies in the combination of features that no single existing tool provides:\n\n### Multi-Domain Coverage\n\nEigenvue is the only platform that covers classical algorithms, deep learning architectures, generative AI components, and quantum computing within a single interface. A student can progress from Bubble Sort to Self-Attention without switching tools, accounts, or mental models.\n\n### Dual Distribution\n\nWhile most visualization tools are either web-only or Python-only, Eigenvue ships as both. The web application at [eigenvue.web.app](https://eigenvue.web.app) serves interactive learners, while `pip install eigenvue` integrates into Jupyter notebooks, research scripts, and CI pipelines.\n\n### Code-Based Generators\n\nUnlike tools that define visualizations through YAML, JSON configuration, or GUI builders, Eigenvue generators are written in TypeScript and Python. This preserves full programmatic expressiveness -- loops, conditionals, data structures -- while producing a standardized step output.\n\n### Cross-Language Parity\n\nThe CI pipeline validates that TypeScript and Python generators produce identical step sequences for the same inputs. Users of the Python package see the exact same visualization data as web users, eliminating the \"second-class SDK\" problem common in multi-platform projects.\n\n### Shareable Deep Links\n\nEvery visualization state -- algorithm, input data, current step, playback speed -- is encoded in the URL. Users can share, bookmark, or embed links that restore exact states without authentication or server-side storage. This is particularly valuable for educators creating course materials and researchers sharing reproducible examples.","src/content/docs/research/comparison.mdx","40922494cb275883","research/citation",{"id":463,"data":465,"body":471,"filePath":472,"digest":473,"deferredRender":16},{"title":466,"description":467,"editUrl":16,"head":468,"template":50,"sidebar":469,"pagefind":16,"draft":39},"Citation","How to cite Eigenvue in academic work.",[],{"hidden":39,"attrs":470},{},"## How to Cite\n\nIf you use Eigenvue in your research, teaching, or academic publications, please cite it using the following BibTeX entry:\n\n```bibtex\n@article{eigenvue2026,\n  title = {Eigenvue: A Visual Learning Platform for Algorithms, AI Architectures, and Quantum Computing},\n  author = {Mishra, Ashutosh},\n  journal = {Journal of Open Source Software},\n  year = {2026},\n  url = {https://eigenvue.web.app},\n  doi = {\u003Cpending>}\n}\n```\n\nThe DOI will be updated once the JOSS review process is complete. In the meantime, the URL `https://eigenvue.web.app` serves as the canonical reference.\n\n## CITATION.cff\n\nA machine-readable citation file is maintained at the repository root as [`CITATION.cff`](https://github.com/ashutoshm1771/eigenvue/blob/main/CITATION.cff). This file follows the [Citation File Format](https://citation-file-format.github.io/) standard and is automatically rendered by GitHub on the repository landing page. When visitors click the \"Cite this repository\" button on GitHub, they receive a pre-formatted citation derived from this file.\n\nThe `CITATION.cff` file includes all author metadata, the software version, license information, and links to the project homepage. If you need to cite a specific version of Eigenvue, refer to the version field in the `CITATION.cff` file or use the DOI associated with that release.\n\n## Academic Use\n\nEigenvue is released under the **MIT License**, which permits free use, modification, and redistribution in both academic and commercial settings. There are no restrictions on using Eigenvue in coursework, research papers, presentations, or derivative educational tools.\n\nWe encourage citation not as a legal requirement but as a way to:\n\n- **Support open-source sustainability** -- citations help demonstrate impact, which supports continued development and funding.\n- **Enable reproducibility** -- citing the specific version used in a study allows others to reproduce results.\n- **Build the community** -- citations help other researchers and educators discover the tool.\n\nIf you use Eigenvue in a publication, we would appreciate it if you let us know. We maintain a list of publications and courses that use Eigenvue, and we are happy to feature your work.","src/content/docs/research/citation.mdx","57802329f032d40f","research/project-overview",{"id":474,"data":476,"body":482,"filePath":483,"digest":484,"deferredRender":16},{"title":477,"description":478,"editUrl":16,"head":479,"template":50,"sidebar":480,"pagefind":16,"draft":39},"Project Overview","Eigenvue project overview for JOSS reviewers and academic audiences.",[],{"hidden":39,"attrs":481},{},"## Statement of Need\n\nThe landscape of algorithm and AI visualization tools is fragmented. Classical algorithm visualizers such as VisuAlgo and Algorithm Visualizer cover sorting and graph traversal but stop short of deep learning. Tools like TensorFlow Playground and BertViz address neural networks or attention mechanisms in isolation. Quantum computing has its own separate ecosystem of circuit builders. No single platform today covers **classical algorithms, deep learning, generative AI, and quantum computing** with a consistent interaction model, a unified step format, and a shareable URL scheme that lets users bookmark and share exact visualization states.\n\nEigenvue fills this gap by providing a single, open-source visual learning platform that spans all four domains. Its architecture is built around a **universal step format** that decouples algorithm logic from rendering, enabling contributors to add new algorithms without touching the visualization engine. The result is a coherent experience where a student can move from Binary Search to Self-Attention to a Quantum Gate and encounter the same playback controls, the same step-through interaction, and the same deep-link sharing mechanism.\n\n## Target Audience\n\nEigenvue is designed for four primary audiences:\n\n- **Students learning algorithms and AI** -- from introductory sorting algorithms to transformer architectures, Eigenvue provides step-by-step visual explanations that build intuition before students engage with code or mathematical notation.\n- **Educators preparing course materials** -- shareable deep links allow instructors to embed specific algorithm states in slides, assignments, or learning management systems. The quiz system supports formative assessment.\n- **Researchers debugging model architectures** -- interactive visualizations of attention patterns, gradient flow, and transformer blocks help researchers verify their mental models and communicate ideas to collaborators.\n- **ML engineers building intuition for transformers** -- stepping through tokenization, embeddings, self-attention, and multi-head attention reveals the mechanics that production code abstracts away.\n\n## Technical Contributions\n\nEigenvue introduces several technical contributions that distinguish it from existing tools:\n\n1. **Step Format as Universal Contract** -- Every algorithm, regardless of domain, produces an array of `Step` objects conforming to a shared schema. Each step declares its layout, highlighted elements, annotation text, and state snapshot. This contract means generators and renderers evolve independently.\n\n2. **Code-Based Generators (Not YAML)** -- Algorithm logic is expressed in TypeScript (web) and Python (package) rather than declarative configuration files. This preserves the full expressiveness of the host language -- loops, conditionals, data structures -- while still producing the standardized step output.\n\n3. **Dual Distribution** -- Eigenvue ships as both a web application (https://eigenvue.web.app) and a Python package (`pip install eigenvue`). The web app serves interactive learners and educators; the Python package integrates into Jupyter notebooks, CI pipelines, and research workflows.\n\n4. **Cross-Language Parity via CI** -- A continuous integration pipeline validates that the TypeScript and Python generators produce identical step sequences for the same inputs. This ensures that the Python package is never a second-class citizen.\n\n5. **Shareable Deep Links** -- Every visualization state is encoded in the URL. Users can share a link that restores the exact algorithm, input data, and current step -- no account required, no server-side state.\n\n## Scope and Current State\n\nAs of the current release, Eigenvue includes:\n\n- **17 algorithms** spanning three categories: Classical Algorithms, Deep Learning, and Generative AI.\n- **3 algorithm categories** with a fourth (Quantum Computing) in active development.\n- **10 layout types** optimized for different data structures and visualization needs (arrays, trees, graphs, matrices, attention heatmaps, and more).\n- **Full playback controls** including play, pause, step forward, step backward, speed adjustment, and jump-to-step.\n- **Educational content** with explanatory text per step, concept introductions, and integrated quizzes for self-assessment.\n- **SEO optimization** with structured data, Open Graph metadata, and pre-rendered pages for discoverability.\n- **Offline support** via service worker caching, enabling use in environments with limited connectivity.\n\n## Quality Assurance\n\nEigenvue maintains software quality through a multi-layered CI/CD pipeline:\n\n- **Unit and integration tests** cover generator logic, step schema compliance, and rendering behavior.\n- **Cross-language validation** ensures TypeScript and Python generators produce identical outputs for shared test fixtures.\n- **Schema validation** checks every generated step against the canonical Step Format JSON Schema, catching contract violations before they reach users.\n- **Bundle size gates** prevent regressions in application size, ensuring the web app remains fast to load.\n- **Lighthouse audits** run on every pull request, enforcing thresholds for performance, accessibility, best practices, and SEO scores.\n- **Automated deployments** publish the web app and Python package from the same pipeline, guaranteeing that releases are synchronized.","src/content/docs/research/project-overview.mdx","0e041282a3a8eee4","user-guide/faq",{"id":485,"data":487,"body":493,"filePath":494,"digest":495,"deferredRender":16},{"title":488,"description":489,"editUrl":16,"head":490,"template":50,"sidebar":491,"pagefind":16,"draft":39},"Frequently Asked Questions","Answers to common questions about Eigenvue, covering pricing, offline use, classroom integration, browser support, embedding, contributing, and troubleshooting.",[],{"hidden":39,"attrs":492},{},"## Is Eigenvue free?\n\nYes. Eigenvue is **free and open-source** under the MIT licence. Both the web\napplication at [eigenvue.web.app](https://eigenvue.web.app) and the Python package\n(`pip install eigenvue`) are available at no cost. You can inspect, modify, and\nredistribute the source code on\n[GitHub](https://github.com/AmarAbbwormo/eigenvue).\n\n## Can I use Eigenvue offline?\n\nThe web application requires an internet connection on the first visit. After\nthe initial load, most assets are cached by the service worker, so basic\nbrowsing may work offline. For a fully offline experience, use the **Python\npackage**: once installed, `eigenvue.show()` and `eigenvue.jupyter()` run a\nlocal server and do not need network access.\n\n## Can I use Eigenvue in the classroom?\n\nAbsolutely. Educators are encouraged to use Eigenvue for lectures, assignments,\nand labs. You can:\n\n- Project the web app and step through an algorithm live during a lecture.\n- Share a URL with a specific step and input set so students can explore on\n  their own (see [Sharing & URLs](/docs/user-guide/sharing-and-urls/)).\n- Assign notebook-based exercises that use `eigenvue.jupyter()` to visualize\n  solutions.\n\nNo licence fees apply for educational or commercial use.\n\n## Which browsers are supported?\n\nEigenvue supports the **latest two major versions** of:\n\n- Google Chrome\n- Mozilla Firefox\n- Microsoft Edge\n- Apple Safari (macOS and iOS)\n\nJavaScript must be enabled. The visualizer relies on the Canvas API and modern\nES modules, so very old browsers (Internet Explorer, legacy Edge) are not\nsupported.\n\n## Can I embed a visualization in my own website?\n\nNot yet as a first-class feature, but you can embed the visualizer using an\n`\u003Ciframe>`:\n\n```html\n\u003Ciframe\n  src=\"https://eigenvue.web.app/algo/merge-sort?data=[9,4,7,2,1]&step=0\"\n  width=\"100%\"\n  height=\"600\"\n  frameborder=\"0\"\n  allow=\"fullscreen\"\n>\u003C/iframe>\n```\n\nThe embedded page includes the full three-panel layout and playback controls. A\ndedicated embed mode with a slimmer UI is on the roadmap.\n\n## How do I report a bug?\n\nOpen an issue on the\n[GitHub issue tracker](https://github.com/AmarAbbwormo/eigenvue/issues). Please\ninclude:\n\n1. A description of the unexpected behaviour.\n2. The algorithm name and the inputs you used.\n3. Your browser (or Python version) and operating system.\n4. Screenshots or console output if available.\n\nThe maintainers triage new issues regularly and aim to respond within a few\ndays.\n\n## How do I request a new algorithm?\n\nFile a **feature request** on the\n[GitHub issue tracker](https://github.com/AmarAbbwormo/eigenvue/issues) with\nthe label `enhancement`. Describe the algorithm, link to a reference (paper,\ntextbook chapter, Wikipedia article), and explain why it would be a good fit for\nthe platform. Community upvotes help prioritise which algorithms are built next.\n\nIf you want to implement it yourself, see the contributing guide for\ninstructions on adding a new algorithm generator.\n\n## I get an error when I run `pip install eigenvue`. What should I do?\n\nTry the following steps in order:\n\n1. **Check your Python version.** Eigenvue requires Python 3.10 or later.\n   Run `python --version` to confirm.\n2. **Upgrade pip.**\n   ```bash\n   python -m pip install --upgrade pip\n   ```\n3. **Use a virtual environment** to avoid conflicts with system packages:\n   ```bash\n   python -m venv .venv\n   source .venv/bin/activate   # macOS / Linux\n   .venv\\Scripts\\activate      # Windows\n   pip install eigenvue\n   ```\n4. **Check your network.** The package is hosted on PyPI, so you need internet\n   access for the initial install.\n\nIf the issue persists, open a GitHub issue with the full error output.\n\n## Can I upload my own model for visualization?\n\nNot in the current release. The visualizations are based on built-in algorithm\ngenerators that ship with the package. Support for user-supplied model weights\nand custom architectures is planned for a future version. In the meantime, you\ncan contribute a new generator if your algorithm fits the existing generator\ninterface.\n\n## How does Eigenvue handle floating-point numbers?\n\nAll numeric computations in the web app use JavaScript's IEEE 754\ndouble-precision floating point. In the Python package, standard Python `float`\n(also IEEE 754 double precision) is used. This means:\n\n- Very small rounding differences may appear in deep-learning and\n  generative-AI visualizations (for example, softmax outputs that sum to\n  0.9999999 instead of 1.0).\n- The state inspector displays values rounded to a sensible number of decimal\n  places for readability, but the underlying computation uses full precision.\n- If you need exact comparisons in your own code, use a tolerance (for example,\n  `math.isclose(a, b, rel_tol=1e-9)`) rather than strict equality.\n\nFloating-point behaviour is consistent across browsers and matches the Python\npackage output for identical inputs.","src/content/docs/user-guide/faq.mdx","986b1602e9b07815","user-guide/sharing-and-urls",{"id":496,"data":498,"body":504,"filePath":505,"digest":506,"deferredRender":16},{"title":499,"description":500,"editUrl":16,"head":501,"template":50,"sidebar":502,"pagefind":16,"draft":39},"Sharing & URLs","Understand how Eigenvue encodes algorithm state in URLs and how to share specific visualization steps with others.",[],{"hidden":39,"attrs":503},{},"Every visualization state in Eigenvue is captured by a URL. This means you can\nbookmark, share, or link to the exact step and input configuration you are\nlooking at.\n\n## URL Structure\n\nVisualization URLs follow this pattern:\n\n```\nhttps://eigenvue.web.app/algo/{algorithm-id}?{input-params}&step={N}\n```\n\n| Segment | Description | Example |\n| --- | --- | --- |\n| `{algorithm-id}` | The unique slug for the algorithm | `bubble-sort` |\n| `{input-params}` | URL-encoded input parameters | `data=[5,3,8,1]` |\n| `step={N}` | The step number to jump to (0-indexed) | `step=12` |\n\nA full example:\n\n```\nhttps://eigenvue.web.app/algo/bubble-sort?data=[5,3,8,1,2]&step=7\n```\n\nThis opens the Bubble Sort visualizer, seeds it with the array `[5, 3, 8, 1, 2]`,\nand jumps directly to step 7.\n\n## Sharing a Specific State\n\nThe visualizer toolbar includes a **Share** button (link icon). Clicking it:\n\n1. Constructs a URL that includes the current algorithm ID, all input parameters,\n   and the current step number.\n2. Copies the URL to your clipboard.\n3. Displays a brief \"Link copied\" confirmation toast.\n\nYou can then paste the link into a chat message, email, forum post, or\nlearning-management system.\n\n## Loading from a URL\n\nWhen someone opens a shared URL, Eigenvue performs the following sequence:\n\n1. **Parse the algorithm ID** from the path to determine which visualizer to\n   load.\n2. **Read query parameters** to extract input values (for example, `data`,\n   `target`, `graph`, `start`).\n3. **Run the step generator** with those inputs to produce the full list of\n   animation frames.\n4. **Jump to the step** specified by the `step` parameter. If the parameter is\n   missing, playback starts from step 0.\n\nThe entire process takes only a moment. The recipient sees the exact same\nvisualization state the sender captured.\n\n## Pre-computed vs Live Generation\n\nEigenvue generates step data **on the fly** in the browser. There is no\nserver-side pre-computation or caching of frames. This has two implications:\n\n- **Deterministic algorithms** (most classical and deep-learning visualizations)\n  always produce identical step sequences for the same inputs, so shared links\n  are perfectly reproducible.\n- **Stochastic algorithms** (some generative-AI visualizations that involve\n  random sampling) may produce slightly different intermediate states on each\n  run. The shared link preserves the inputs and the step number, but the\n  animation may not be frame-identical. A seed parameter is available for\n  algorithms where reproducibility matters.\n\n## Social Sharing Previews\n\nWhen you paste an Eigenvue URL into a platform that supports link previews\n(Slack, Discord, Twitter/X, LinkedIn, iMessage, etc.), the page serves\n**Open Graph (OG) meta tags** that produce a rich preview card:\n\n- **Title** -- the algorithm name (for example, \"Bubble Sort -- Eigenvue\").\n- **Description** -- a one-line summary of the algorithm.\n- **Image** -- a generated preview image showing the canvas at the linked step.\n\nThese OG images are generated automatically so that every shared link looks\ninformative in social feeds and messaging apps, even before the recipient clicks\nthrough.","src/content/docs/user-guide/sharing-and-urls.mdx","69494ce9efba0c82","user-guide/ui-controls",{"id":507,"data":509,"body":515,"filePath":516,"digest":517,"deferredRender":16},{"title":510,"description":511,"editUrl":16,"head":512,"template":50,"sidebar":513,"pagefind":16,"draft":39},"UI Controls & Playback","Comprehensive reference for every interactive element on the Eigenvue visualizer page, including the canvas, code panel, explanation panel, playback bar, input editor, and keyboard shortcuts.",[],{"hidden":39,"attrs":514},{},"This page is a complete reference for the visualizer interface. Use it whenever\nyou need to know exactly what a button, panel, or shortcut does.\n\n## Canvas Panel\n\nThe **canvas** occupies the centre of the visualizer page. It renders the\nalgorithm's data structures, model layers, or quantum circuit as an animated\ngraphic.\n\n- **Pan** -- click and drag on empty space to move the viewport.\n- **Zoom** -- scroll or pinch to zoom in and out.\n- **Element tooltips** -- hover over any visual element (array cell, graph node,\n  neural-network layer) to see its current value or label.\n- **Highlight** -- the element being operated on in the current step is\n  highlighted with an accent colour.\n\nThe canvas redraws on every step transition, whether driven by auto-playback or\nmanual stepping.\n\n## Code Panel\n\nThe **code panel** sits to the left of the canvas. It shows the algorithm's\nsource code and highlights the line that corresponds to the current step.\n\n### Tabs\n\nThree tabs are available at the top of the panel:\n\n| Tab | Description |\n| --- | --- |\n| **Pseudocode** | Language-agnostic notation that focuses on logic |\n| **Python** | Idiomatic Python implementation |\n| **JavaScript** | Idiomatic JavaScript implementation |\n\nSwitching tabs preserves the current step and line highlight. The highlighted\nline maps to the equivalent operation in whichever language you select.\n\n### Line Highlight\n\nThe active line is marked with a coloured background. As you step through the\nalgorithm, the highlight moves to follow execution. If the current step maps to\nmultiple lines (for example, a swap), all relevant lines are highlighted.\n\n### Copy Button\n\nA **Copy** icon in the top-right corner of the code panel copies the full source\nlisting for the active tab to your clipboard.\n\n## Explanation Panel\n\nThe **explanation panel** sits to the right of the canvas and contains three\nsections:\n\n### Title\n\nThe algorithm name and a one-line summary (for example,\n\"Bubble Sort -- repeatedly swap adjacent elements that are out of order\").\n\n### Step Explanation\n\nA plain-language description of what is happening at the current step. This\nupdates automatically as you advance or rewind. Example:\n\n> Comparing elements at index 2 (value 7) and index 3 (value 1). Since 7 > 1,\n> they will be swapped.\n\n### State Inspector\n\nA structured view of the algorithm's live variables. For a sorting algorithm\nthis might look like:\n\n| Variable | Value |\n| --- | --- |\n| `array` | `[1, 3, 5, 7, 8]` |\n| `i` | `2` |\n| `j` | `3` |\n| `swapped` | `true` |\n\nFor a neural-network visualization, the state inspector might show weight\nmatrices, activation values, or loss at the current training step.\n\n## Playback Bar\n\nThe playback bar is a horizontal toolbar anchored below the canvas. Its controls\nare laid out from left to right as described below.\n\n### Reset Button ()\n\nReturns the visualization to **step 0** -- the initial state before any\noperations have been performed.\n\n### Step Back Button ()\n\nMoves **one step backward**. If you are already at step 0, the button is\ndisabled. Holding the button does not auto-repeat; use the Play button for\ncontinuous reverse is not supported -- step back manually.\n\n### Play / Pause Button ( / )\n\nToggles continuous playback.\n\n- **Play ()** -- the visualization advances one step at a time at the\n  currently selected speed.\n- **Pause ()** -- playback stops and you remain on the current step.\n\nWhen playback reaches the last step, it automatically pauses.\n\n### Step Forward Button (|)\n\nMoves **one step forward**. If you are already at the last step, the\nvisualization wraps around to step 0.\n\n### Speed Selector\n\nA dropdown or segmented control offering four presets:\n\n| Speed | Delay Between Steps |\n| --- | --- |\n| **0.5x** | 2000 ms |\n| **1x** | 1000 ms (default) |\n| **2x** | 500 ms |\n| **4x** | 250 ms |\n\nThe speed change takes effect immediately, even during active playback.\n\n### Step Counter\n\nDisplays the current position in the format **Step N / Total**. For example,\n`Step 12 / 45` tells you that you are on step 12 of 45 total steps.\n\n### Progress Bar / Scrubber\n\nA horizontal bar that fills proportionally as you move through the steps. You\ncan **click** anywhere on the bar or **drag** the thumb to jump directly to a\nspecific step. The canvas, code highlight, and explanation all update instantly\nwhen you scrub.\n\n## Input Editor\n\nThe input editor sits below the code panel. It exposes every configurable\nparameter for the active algorithm.\n\n- **Text fields / number inputs** -- type a value directly.\n- **JSON editor** -- for complex inputs (arrays, objects, graphs) a multi-line\n  editor with syntax highlighting is provided.\n- **Examples dropdown** -- a pre-populated list of interesting inputs. Selecting\n  one fills all fields automatically.\n- **Run button** -- press to regenerate the visualization with the current\n  inputs. The visualizer resets to step 0 with the new data.\n\nInvalid inputs are highlighted in red with an inline error message.\n\n## Keyboard Shortcuts\n\nAll shortcuts work when the visualizer page is focused. They mirror the\nplayback-bar buttons for faster navigation.\n\n| Key | Action |\n| --- | --- |\n| `Space` | Play / Pause |\n| `` (Right Arrow) | Step forward |\n| `` (Left Arrow) | Step back |\n| `Home` | Jump to first step (reset) |\n| `End` | Jump to last step |\n| `+` (or `=`) | Increase playback speed to the next preset |\n| `-` | Decrease playback speed to the previous preset |\n\nKeyboard shortcuts are disabled while the input editor text fields have focus,\nso you can type freely without triggering playback actions.","src/content/docs/user-guide/ui-controls.mdx","1c2ebecbdaaa5b5f"]